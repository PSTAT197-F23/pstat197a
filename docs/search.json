[
  {
    "objectID": "about/links.html",
    "href": "about/links.html",
    "title": "Links",
    "section": "",
    "text": "Lecture Attendance reporting form; fill out once per class meeting.\nSection Attendance reporting form\nCapstone project intake form; fill out by October 6."
  },
  {
    "objectID": "about/links.html#resources",
    "href": "about/links.html#resources",
    "title": "Links",
    "section": "Resources",
    "text": "Resources\nTextbooks:\n\nModern Data Science with R by Baumer, Kaplan, and Horton.\nIntroduction to Statistical Learning with Applications in R by James et al.\nFundamentals of Data Visualization by Claus Wilke.\nR for Data Science by Wickham and Grolemund.\nDeep Learning by Goodfellow, Bengio, and Courville.\n\nDocumentation:\n\nTidyverse and tidymodels packages\nGitHub Docs"
  },
  {
    "objectID": "about/outcomes.html",
    "href": "about/outcomes.html",
    "title": "Learning outcomes",
    "section": "",
    "text": "using modern technology and version control to collaborate efficiently on programming for data science projects;\nrecognizing and articulating problem patterns based on data semantics and one or more research questions;\nidentifying and accessing resources to aid in learning independently about methodology and/or application domains pertinent to a problem of interest;\ncommunicating data analysis and/or research findings in a project team setting and to a small audience of peers.\n\nCourse staff are committed to creating an inclusive learning environment. Data science involves a combination of computing, statistics and probability, and domain expertise, as well as use of technology and narrative communication and storytelling, and no one person should expect to be an expert in all of these areas. Course staff recognize this fact that core competencies vary considerably, acknowledge that each student has particular strengths and weaknesses and interests, and make their best effort to avoid promoting one skill set over others in the practice of data science."
  },
  {
    "objectID": "about/schedule.html",
    "href": "about/schedule.html",
    "title": "Course schedule",
    "section": "",
    "text": "This schedule is tentative and may be adjusted at the discretion of the instructor. Check back for updates.\n\n\n\n\n\n\n\n\n\n\nWeek\nTheme\nMonday meeting\nWednesday meeting\nSection meeting\n\n\n\n\n0\nModule 0: Introductions\nNO CLASS\nCourse orientation\nNO LAB\n\n\n1\nModule 0: Introductions\n0.1 Lecture:\n\non research projects in(volving) data science\n\n0.2 Activity:\n\ncollaboration using GitHub\n\nSoftware and technology overview\n\n\n2\nModule 0: Introductions\n0.3 Lecture/discussion:\n\nintroducing class survey data\n\n0.4 Activity:\n\nexploratory and descriptive analysis\n\ntidyverse\n\n\n3\nModule 1: biomarkers\n1.1 Discussion/lecture:\n\nsharing results of survey data analysis;\nintroducing biomarker data\n\n1.2 Lecture:\n\non prediction\n\ntidymodels\n\n\n4\nModule 1: biomarkers\n1.3 Lecture:\n\non classification\n\n1.4 Lecture/discussion:\n\non variable selection;\nreview published analysis of biomarker data\n\nclassification\n\n\n5\nModule 2: web fraud\n2.1 Lecture/discussion:\n\nsharing analysis of soil temperature data;\nintroducing web fraud data\n\n2.2 Lecture:\n\non text as data\n\ntext processing\n\n\n6\nModule 2: web fraud\n2.3 Lecture:\n\non multiclass classification\n\n2.4 Activity:\n\nmeasuring classification accuracy\n\nkeras\n\n\n7\nModule 3: soil temperature\n3.1 Discussion/lecture:\n\nsharing results of biomarker analysis;\nintroducing soil temperature data\n\n3.2 Lecture:\n\non time\n\ntime series analysis\n\n\n8\nModule 3: soil temperature\n3.3 Lecture:\n\non space\n\n3.4 Discussion: results\nspatial analysis\n\n\n9\nModule 4: vignettes\n4.1 Activity:\n\nworkshopping vignettes\n\nNO CLASS\nNO LAB\n\n\n10\nModule 4: vignettes\n4.2 Activity:\n\nteaching exchange\n\n4.3 Activity/discussion:\n\nteaching exchange;\nclosing\n\nNO LAB"
  },
  {
    "objectID": "about/syllabus.html",
    "href": "about/syllabus.html",
    "title": "Course syllabus",
    "section": "",
    "text": "Concurrent course listing: PSTAT197A and CMPSC190DD are held concurrently; enrollment is by instructor consent and admitted students may enroll under either listing. The course content, expectations, assessments, and course policies are identical for students enrolled in either course.\nCatalog description: Introduction to research skills. Discussion of current research trends, writing literature reviews, etc. Students will be required to present materials reflecting their interests, which will be critically appraised for both content and presentation. Emphasis will be placed on aiding students to acquire a high-level of professionalism. Prerequisite: PSTAT126."
  },
  {
    "objectID": "about/syllabus.html#meetings",
    "href": "about/syllabus.html#meetings",
    "title": "Course syllabus",
    "section": "Meetings",
    "text": "Meetings\nClass meetings are held 2pm – 3:15pm Mondays and Wednesdays in Webb Hall 1100.\nSection meetings are held on Tuesdays:\n\n2pm – 2:50pm in Phelps Hall 1448;\n10am – 10:50am in Girvetz Hall 2115;\n4pm – 4:50pm in Phelps Hall 2524."
  },
  {
    "objectID": "about/syllabus.html#staff",
    "href": "about/syllabus.html#staff",
    "title": "Course syllabus",
    "section": "Staff",
    "text": "Staff\nInstructor:\n\nLaura Baracaldo. Visiting assistant professor and co-instructor for 2023-2024 capstone projects.\n\nTeaching assistants:\n\nErika McPhillips. MS/PhD student and capstone project mentor in 2023-2024."
  },
  {
    "objectID": "about/syllabus.html#expectations-and-assessments",
    "href": "about/syllabus.html#expectations-and-assessments",
    "title": "Course syllabus",
    "section": "Expectations and assessments",
    "text": "Expectations and assessments\nMuch of the course is designed around group activity and discussion. Students are therefore expected to:\n\nprepare for class meetings in advance by completing any assigned reading or activity;\nattend and actively participate in class meetings and section meetings;\nprovide meaningful, timely, and concrete contributions to group activities.\n\nStudents having any difficulty in meeting these expectations should raise the issue(s) promptly with the instructor.\nQualitative feedback is emphasized over numerical scores. Students are assessed on:\n\nattendance, preparation, and participation;\nquality of submitted work;\nindividual contributions to group assignments;\noral interview."
  },
  {
    "objectID": "about/syllabus.html#policies",
    "href": "about/syllabus.html#policies",
    "title": "Course syllabus",
    "section": "Policies",
    "text": "Policies\nAttendance. Regular attendance is expected. Each student can miss two sessions without notice; further absences may impact course grades. Students are responsible for material discussed in their absence and should review posted session notes and consult a classmate.\nDeadlines. Students are expected to meet assignment deadlines in a timely manner. All deadlines have a 24-hour grace period. Late or amended work may not be accepted.\nEmail. Course staff will make their best effort to reply to email within 48 weekday hours. However, due to high volume, staff cannot guarantee that all messages will receive replies.\nIllness. Students who are ill are required to stay home. Students ill with COVID-19 must comply with university policy regarding reporting and quarantine. Accommodations will be made to ensure that students absent due to illness do not fall behind.\nAccommodations. Reasonable accommodations will be made for any student with a qualifying disability. Such requests should be made through the Disabled Students Program (DSP). More information, instructions on how to access accommodations, and information on related resources can be found on the DSP website. Note: in this class there are no timed assessments.\nLetter grades. Letter grades are assigned based only on the assessments identified above and according to university guidelines, with the relative weighting of assessments determined at the discretion of the instructor. While grade calculations will not be disclosed, students are entitled to an explanation of the criteria used to determine their grades if desired. Grades will not be changed except in the case of clerical errors. If students feel their grade has been unfairly assigned, they are entitled to contest it following UCSB procedure for contesting grades.\nConduct. All course participants are expected to maintain respectful and honorable conduct consistent with UCSB ethical standards. Students uncomfortable with the behavior of another course participant for any reason should notify the instructor, course staff, or, if the complaint relates to course staff conduct, an administrative or departmental officer. Evidence of academic dishonesty will be reported to the Office of Student Conduct (OSC); evidence of problematic behavior will be addressed on a case-by-case basis in accord with university policies."
  },
  {
    "objectID": "about/technology.html",
    "href": "about/technology.html",
    "title": "Technology",
    "section": "",
    "text": "Computing in PSTAT197A will be shown in R, and codes and other materials will be shared via GitHub. The following software will be required to access course materials:\n\nR\nRStudio\nGit\nGitHub Desktop (or another visual GitHub client)\n\nInstallations and basic functionality will be covered in the first section meeting.\nWhile PSTAT197A is not language-agnostic and some instruction in R is provided, it is also not a course especially emphasizing programming technique in R. Students are free to use or experiment with other software at their discretion provided it does not interfere with their participation in the class, but are expected to submit work and collaborate using RStudio-supported files."
  },
  {
    "objectID": "about/technology.html#sec-github",
    "href": "about/technology.html#sec-github",
    "title": "Technology",
    "section": "GitHub",
    "text": "GitHub\nStudents will learn and practice basic functionality of Git and GitHub for version control and collaboration by accessing course materials via GitHub repositories and submitting work via repository contributions.\nWe have a GitHub classroom for the data science capstone. Materials will be deployed via direct links. Students will be asked to submit work by contributing to team repositories; any such contributions will remain visible to course staff and team contributors, and so are not strictly private.\nTo access GitHub Classroom materials students will need to create a GitHub account if they do not already have one. Here is some advice on choosing a username."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data science capstone preparation",
    "section": "",
    "text": "PSTAT197A/CMPSC190DD is the first course in UC Santa Barbara’s yearlong data science capstone sequence. The course aims to provide preparation and training to undergraduate students of any discipline with a basic background in data science for an independent research or project experience.\nMost students take this course to prepare for their work on sponsored team projects during the remainder of the capstone sequence (PSTAT197B-C/CMPSC190DE-DF). However, some may elect to take the course for other reasons, such as an upcoming faculty-supervised research project or internship.\nThis site hosts course information and resources for currently enrolled students."
  },
  {
    "objectID": "materials/activities/github-basics.html",
    "href": "materials/activities/github-basics.html",
    "title": "GitHub basics",
    "section": "",
    "text": "This activity introduces GitHub repositories and basic Git actions; students will be expected to use these skills to access materials and complete assignments.\nObjectives:\nPrerequisites: completion of lab 1, particularly cloning the group sandbox repository."
  },
  {
    "objectID": "materials/activities/github-basics.html#why-are-we-using-git-and-github",
    "href": "materials/activities/github-basics.html#why-are-we-using-git-and-github",
    "title": "GitHub basics",
    "section": "Why are we using Git and GitHub?",
    "text": "Why are we using Git and GitHub?\nVersion control has many benefits, including the ability to track changes and contributions precisely, work in parallel with other contributors, revert to prior versions of files, keep track of issues, quickly share and disseminate work, and solicit user contributions from the coding public. Arguably, for all of these reasons and because of its widespread use, Git/GitHub is a must for data scientists.\nIn this class you’ll learn and practice some basics that will allow you to easily access course files, collaborate with each other, and efficiently submit your coursework. This should equip you to utilize a repository for efficient collaboration with your peers on your capstone project."
  },
  {
    "objectID": "materials/activities/github-basics.html#basic-workflow",
    "href": "materials/activities/github-basics.html#basic-workflow",
    "title": "GitHub basics",
    "section": "Basic workflow",
    "text": "Basic workflow\nIf I am working out of a repository and want to alter a file and make those changes available to anyone else accessing my repository, most of the time I need to:\n\ncreate/update local copies of repository files on my laptop;\nmake the desired change(s) locally;\nsend the changes back to the remote repository.\n\nTypically these steps are performed iteratively as work progresses – they are a basic workflow.\nWorkflow can be understood as a sequence of Git actions: actions that modify the repository files and/or metadata. The most basic sequence that accomplishes the above steps is:\n\ngit pull update the local repository (technically, fetch changes + merge changes from the remote repository);\ngit add stage file changes to be committed to the local repository;\ngit commit commit staged changes to the local repository;\ngit push send committed changes back to the remote repository.\n\nSometimes contributors take different or additional actions; the complexity of the Git actions required to make a change depends largely on repository settings, permissions, and agreements among collaborators about how workflow should be structured."
  },
  {
    "objectID": "materials/activities/github-basics.html#basic-git-actions",
    "href": "materials/activities/github-basics.html#basic-git-actions",
    "title": "GitHub basics",
    "section": "Basic Git actions",
    "text": "Basic Git actions\nHere you’ll make a local change and then push that change to the remote repository.\n\nPull\nThe first step to making a change is ensuring you have the most up-to-date version of the repository files.\n\n\n\n\n\n\nAction (individual)\n\n\n\nPull changes from the remote repository.\n\nIn your GitHub client, open the group sandbox repository and then look for a ‘Pull’ menu item.\nIf you are using GitHub desktop, you can alternatively ‘fetch origin’ first via a toolbar button. This will retrieve changes but without modifying local files, and if changes are detected, a button will appear in the main screen of the client to pull changes.\nIn the terminal: navigate to the root directory of the repository and git pull\n\n\n\nNow check the repository history to see what changes you just pulled. In GitHub Desktop, there is a history tab on the left-hand side that lists commits chronologically. Select a commit to view line-by-line differences for every file that was altered.\nYou should see two changes: that there is now a class-activity folder containing a copy of this activity; and the README file has been updated. Look at the differences on the readme file.\n\n\n\n\n\n\nRemark\n\n\n\nFetching vs. pulling\nFetching allows you to retrieve changes from the remote repository without merging them into your local repository. If there are commits that you haven’t merged, you can examine them before doing so in one of two ways:\n\nin the terminal, git diff main origin/main\nopen the remote repository on github.com and check the commit history (look for a clock icon with the number of commits in the upper right corner of the file navigator in the code menu); open any commit to see a line-by-line comparison of differences.\n\n\n\n\n\nMake changes\nNow that you have the most up-to-date version of all files, create a new markdown file in the class activity folder with a fun fact about you (or anything else if you’d rather) that you’ll upload to the repository.\n\n\n\n\n\n\nAction (individual)\n\n\n\nCreate a markdown file:\n\nIn RStudio, select File &gt; New File &gt; Markdown File\nAdd an ‘About Me’ or similar header (use one or more hashes # before the header text)\nWrite a fun fact about yourself\nSave the file as YOURGITHUBUSERNAME-about.md in the class activity folder\n\n\n\n\n\nStage and commit changes\nNow that your new file is ready to go you can stage the changes to be committed to the repository and create a commit.\nA commit is a bundle of changes that will be submitted to the repository along with a message briefly explaining the changes made. Your GitHub client will often fill in a default message such as ‘update FILENAME.EXT’.\n\n\n\n\n\n\nAction (individual)\n\n\n\nStage and commit:\n\nIn your client, look for a menu item to add or stage changes. By default any changes made to any file will be included. In GitHub Desktop, look for the ‘Changes’ menu next to ‘History’; you can stage changes by simply selecting or unselecting the checkbox next to each file that was altered.\n\nOr in the terminal: git add FILENAME\n\nOnce you have staged changes, look for a menu item to commit changes. Add a message and commit the changes. In GitHub Desktop, this appears at the bottom of the ‘Changes’ menu.\n\nOr in the terminal: git commit -m \"your message here\"\n\n\n\n\nOften these actions are performed together. However, in some workflows it may make sense to stage changes incrementally and create commits that bundle several changes at once. For example, if you need to make an update that requires modifying files A, B, and C, it may make sense to edit and stage changes to A first, followed by B, followed by C, and create the commit only once the full update has been implemented.\n\n\nPush\nThe last step is to push your commit to the remote repository. However, as you will see in a moment, too many people trying to push changes at once can create some problems.\n\n\n\n\n\n\nAction (group)\n\n\n\n\nChoose one person in your team to push their changes. The very first person to do this will have no problems, since their local repository is up to date with the remote.\nThen choose someone else to try. Since the first person modified the remote repository, the next person to push changes will no longer be up to date. Git will detect this and the push won’t go through.\nHave the second person update their local repository by pulling changes, and then try the push again. It should go through once their local is up to date with the remote.\nHave everyone in your team pull changes but do not push any additional commits.\n\n\n\nSo far everyone is working on independent files and there’s no overlap between changes, so although it would be a bit of a hassle to have everyone check for changes every time they push, in principle it could be done. However, there is a more efficient way to work in parallel: by creating branches."
  },
  {
    "objectID": "materials/activities/github-basics.html#branching",
    "href": "materials/activities/github-basics.html#branching",
    "title": "GitHub basics",
    "section": "Branching",
    "text": "Branching\nInspect your GitHub client closely, and note that you are currently on the ‘main’ branch of the repository. Think of this as the primary version of the repository. Branches allow contributors to create parallel versions of the repository that they can modify for development purposes while leaving the primary version unaffected.\n\nCreate a branch\nHere you’ll use branches to avoid stepping on each others’ toes while pushing your team’s remaining commits. The strategy will be to create a personal branch, push your commit to that branch, and then merge the branch back into the main branch of the repository.\n\n\n\n\n\n\nAction (individual)\n\n\n\nCreate a branch and push your previous commit:\n\nIn your GitHub client, look for a menu item to create and switch to a new branch.\nName your branch your GitHub username.\nCheck to see that you are currently on your personal branch.\nPush your previous commit. You shouldn’t have to repeat any of the previous steps, but you can if need be.\n\nIf you were one of the two who pushed their commit to main, make some small change to your file to push to your personal branch.\n\n\n\n\nAccess your neighbor’s branch\nWhile often the main purpose of branching is to create a version of the repository that only you will modify, contributors can inspect any branch of the repository. This can be useful for sharing ideas or getting input or help.\n\n\n\n\n\n\nAction (in pairs)\n\n\n\nMake a commit to your neighbor’s branch\n\nFind out your neighbor’s (to your right) username and switch to their branch in your GitHub client.\nIn RStudio, verify that you are on their branch by executing git status in the terminal.\nOpen their markdown file, ask them a simple question about themselves (nothing too personal, please), and add the information to their markdown file.\nStage, commit, and push the change.\nWhen your neighbor (to your left) has done the same with you, switch back to your own branch in your GitHub client and pull changes."
  },
  {
    "objectID": "materials/activities/github-basics.html#pull-requests",
    "href": "materials/activities/github-basics.html#pull-requests",
    "title": "GitHub basics",
    "section": "Pull requests",
    "text": "Pull requests\nOnce you are ready to integrate changes you’ve developed on a branch you can open a pull request to merge the development branch with the main branch. (Technically, pull requests can be opened between any two branches, so could also be used, for example, to update your branch if the main branch has new commits.)\n‘Pull request’ is a bit of an odd term; think of it as you making a request that your collaborators pull your changes for review.\n\n\n\n\n\n\nAction (individual)\n\n\n\nOpen a pull request:\n\nIn your GitHub client, find a menu item for opening a pull request. GitHub Desktop will simply redirect you to github.com to open the request.\nSpecify the pull request from your branch to the main branch and submit.\n\n\n\nOnce a pull request is opened, usually a collaborator with maintain privileges must be the one to merge changes and close the request. However, the rules for this depend on repository settings. For this repository, all contributors can merge and close pull requests.\n\n\n\n\n\n\nAction (pairs)\n\n\n\n\nOpen the repository in the browser. Navigate to pull requests.\nFind your neighbor’s (to your right) pull request; merge their changes and close the request. Then delete the branch.\n\n\n\nOnce everyone in your team is finished, examine the repository and verify that everyone’s markdown file is present on the main branch. Then have each contributor pull changes and check that they see the same."
  },
  {
    "objectID": "materials/activities/github-basics.html#merge-conflicts",
    "href": "materials/activities/github-basics.html#merge-conflicts",
    "title": "GitHub basics",
    "section": "Merge conflicts",
    "text": "Merge conflicts\nGit is pretty clever at merging changes when you pull, push, or merge branches via pull request. However, occasionally commits will conflict in such a way that can’t be resolved automatically. These are known as merge conflicts.\nMerge conflicts happen when:\n\ntwo commits differ on the same line of the same file;\nfiles are moved or deleted in conflicting ways.\n\nHere you’ll create an artificial merge conflict to see what this looks like and how to fix it.\n\n\n\n\n\n\nAction (group)\n\n\n\nCreate a merge conflict\n\nHave someone in your team open the README file and add the group members’ names in a list on one line, e.g.,\ngroup: Laura Baracaldo, Erika MchPillips\nCommit and push changes\nThen have someone, without pulling new changes, create a commit with the names shown differently somehow, such as last, first, or initials, or spanning multiple lines with one name per line.\nAttempt to push the commit. Your client will detect ‘upstream’ changes on the remote repository and prompt you to pull changes.\nAttempt to pull the changes. The client will then report a merge conflict and prompt you to resolve the conflict and commit changes. GitHub Desktop in particular will prompt you to open RStudio to resolve the conflict. Go ahead and follow the prompt.\n\nResolve a merge conflict\nYou will see a version of the file with the conflict that shows &lt;&lt;&lt;&lt;HEAD … &gt;&gt;&gt;&gt; followed by a long alphanumeric string. Within the angle brackets the two conflicting versions of the file will be shown, separated by ===== .\n\nAgree with your team on one version of the README file (or another representation of your names).\nCommit and push the change.\n\n\n\nWhen detected, merge conflicts must be resolved with a commit that takes precedence over the conflicting commits. You can read more about resolving merge conflicts here."
  },
  {
    "objectID": "materials/activities/github-basics.html#checklist",
    "href": "materials/activities/github-basics.html#checklist",
    "title": "GitHub basics",
    "section": "Checklist",
    "text": "Checklist\n\nOn github.com, your group-sandbox repository has a directory called class-activity containing a copy of this activity and one markdown file for each group member with two fun facts about them.\nThe repository has only one open branch.\nThe README file lists each group member’s name.\nEach group member has an up-to-date local copy of the repository."
  },
  {
    "objectID": "materials/activities/making-trees.html",
    "href": "materials/activities/making-trees.html",
    "title": "Activity: making trees",
    "section": "",
    "text": "For the purposes of this activity we’ll use data from the 1994 census (there are fewer variables than the proteomics data and sometimes a little variety is nice).\nYour group’s job is to build a tree to classify high earners and low earners based on a bootstrap sample and a random subset of predictors.\nThe income variable is the response or variable of interest; the rest are potential predictors."
  },
  {
    "objectID": "materials/activities/making-trees.html#building-a-tree",
    "href": "materials/activities/making-trees.html#building-a-tree",
    "title": "Activity: making trees",
    "section": "Building a tree",
    "text": "Building a tree\n\nStep 1: resample the data\nFirst, draw a sample with replacement from the observations. This is known as a bootstrap sample. We’ll keep this relatively small to simplify matters.\n\n\n\n\n\n\nAction\n\n\n\nDraw a bootstrap sample\nCopy the code chunk below and execute once.\n\n# resample data\ncensus_boot <- census %>%\n  sample_n(size = 200, replace = T)\n\n\n\nYou will build your tree using this data.\n\n\nStep 2: select predictors at random\nNext draw a set of predictors at random. We’ll also keep this set small so that you can develop the tree ‘by hand’.\n\n\n\n\n\n\nAction\n\n\n\nSelect two random predictors\nCopy and paste the code chunk below into your script and execute once without modification.\n\n# retrieve column names\npossible_predictors <- census %>% \n  select(-income) %>%\n  colnames()\n\n# grab 2 columns at random\npredictors <- sample(possible_predictors,\n                     size = 2, \n                     replace = F)\n\n# select these columns from the bootstrap sample\ntrain <- census_boot %>% \n  select(c(income, any_of(predictors)))\n\n\n\nYou will build your tree using only these predictors from the bootstrap sample as training data.\n\n\nStep 3a: find your first split\nNow your job is to choose exactly one of the predictors to make a binary split of the data.\n\nfor categorical variables, determine which categories you will classify as high-income vs. low-income\nfor continuous variables, choose a cutoff value so that any observation greater/less than the cutoff is classified as a high/low (or vice-versa) earner\n\nYou do not need to make quantitatively rigorous choices. Try to make a choice that you think is reasonable, but don’t agonize over it. The code below might help you decide which variable to use and how to make the split: use it to inspect each of the two variables and decide which one better distinguishes the income groups.\n\n# comment out -- don't overwrite your bootstrap sample!\ncensus_boot <- census %>% \n  sample_n(size = 200, replace = T)\n\n# for continuous variables\ncensus_boot %>%\n  ggplot(aes(x = age, # replace with predictor name \n             y = income)) +\n  geom_jitter(height = 0.1) +\n  geom_vline(xintercept = 35) # adjust cutoff\n\n\n\ncensus_boot %>%\n  ggplot(aes(x = age, # replace with predictor name\n           y = ..density..)) +\n  geom_density(aes(color = income, fill = income),\n               alpha = 0.5) +\n  geom_vline(xintercept = 35) # adjust cutoff\n\n\n\n# for categorical variables\ncensus_boot %>%\n  group_by(workclass, income) %>%\n  count() %>%\n  spread(income, n) %>%\n  mutate_all(~ replace_na(.x, 0)) %>%\n  mutate(high.inc = `<=50K` > `>50K`)\n\n# A tibble: 7 × 4\n# Groups:   workclass [7]\n  workclass        `<=50K` `>50K` high.inc\n  <chr>              <int>  <int> <lgl>   \n1 ?                     11      1 TRUE    \n2 Federal-gov            4      2 TRUE    \n3 Local-gov             17      7 TRUE    \n4 Private              103     29 TRUE    \n5 Self-emp-inc           4      2 TRUE    \n6 Self-emp-not-inc       5      6 FALSE   \n7 State-gov              4      5 FALSE   \n\n# pick out categories that are majority high income\nhighinc_categories <- census_boot %>%\n  group_by(workclass, # replace with predictor name\n           income) %>%\n  count() %>%\n  spread(income, n) %>%\n  mutate_all(~ replace_na(.x, 0)) %>%\n  mutate(high.inc = `<=50K` < `>50K`) %>%\n  filter(high.inc == T) %>%\n  pull(workclass) # replace with predictor name\n\n\n\n\n\n\n\nAction\n\n\n\nMake your first split\n\nChoose whichever of the two predictors you think best distinguishes the high income and low income groups.\nFind a cutoff value if the variable is continuous, or the categories that you will classify as high income if the variable is categorical.\nIf categorical, store a vector of the category names that are classified as high income (see code above). If continuous, store the cutoff value.\nWrite down the rule.\n\n\n\n\n\nStep 3b: find your second split\nNow filter the data to just those rows classified as (but not necessarily actually) high income based on your first split.\n\n# continuous case -- example\ncensus_boot_sub <- census_boot %>%\n  filter(age > cutoff)\n\n# categorical case -- example\ncensus_boot_sub <- census_boot %>%\n  filter(workclass %in% highinc_categories)\n\n\n\n\n\n\n\nAction\n\n\n\nFind a second split\n\nRepeat step 3a but with the filtered data census_boot_sub instead of the full bootstrap sample.\nWrite down the rule.\n\n\n\n\n\nStep 4: draw the tree\nWe could in theory keep creating binary splits until all observations are correctly classified. However, since we’re doing this by hand and just for illustration purposes, we’ll stop after two splits – it’ll be more of a sapling than a fully grown tree.\n\n\n\n\n\n\nAction\n\n\n\nMake a diagram\nDraw your tree on your group’s whiteboard. It should have just one ‘root’ node and just three ‘leaf’ nodes."
  },
  {
    "objectID": "materials/activities/making-trees.html#classifying-a-new-observation",
    "href": "materials/activities/making-trees.html#classifying-a-new-observation",
    "title": "Activity: making trees",
    "section": "Classifying a new observation",
    "text": "Classifying a new observation\nUse your tree to determine how to classify the following observation:\n\ncensus %>%\n  sample_n(size = 1) %>%\n  t() %>%\n  knitr::kable()\n\n\n\n\nage\n51\n\n\nworkclass\nPrivate\n\n\neducation\nBachelors\n\n\nmarital_status\nDivorced\n\n\noccupation\nAdm-clerical\n\n\nrelationship\nNot-in-family\n\n\nrace\nWhite\n\n\nsex\nFemale\n\n\ncapital_gain\n0\n\n\ncapital_loss\n0\n\n\nhours_per_week\n40\n\n\nnative_country\nUnited-States\n\n\nincome\n<=50K"
  },
  {
    "objectID": "materials/activities/making-trees.html#algorithmic-considerations",
    "href": "materials/activities/making-trees.html#algorithmic-considerations",
    "title": "Activity: making trees",
    "section": "Algorithmic considerations",
    "text": "Algorithmic considerations\nYou just did a loose version of what’s known as recursive partitioning – repeatedly splitting the data. That’s a specific method of constructing a tree.\nHow did you decide which of your two variables to use? Could you write code to make the same choice automatically? This, it turns out, is the main challenge in fully automating the process. The recursive partitioning algorithm requires two things:\n\na criterion by which one split is considered ‘better’ than another\na stopping rule\n\nIt is fairly simple to compute the best cutoff (or categorical mapping) for a given predictor – one can do a brute-force search for the split that minimizes misclassifications. However, when there are many possible variables to split on, a criterion is needed to determine the best choice. You can read about how this is done in MDSR 11.1.1."
  },
  {
    "objectID": "materials/activities/multinomial-logit.html",
    "href": "materials/activities/multinomial-logit.html",
    "title": "Activity: multinomial logistic regression",
    "section": "",
    "text": "While we are getting started, on your table’s workstation open RStudio and execute the following command:\n\nsource('https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/scripts/package-installs.R')\n\nThen open a new script, copy-paste the code chunk below, and execute once.\n\n# packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(modelr)\nlibrary(Matrix)\nlibrary(sparsesvd)\nlibrary(glmnet)\n\n# path to activity files on repo\nurl <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/activities/data/'\n\n# load a few functions for the activity\nsource(paste(url, 'projection-functions.R', sep = ''))\n\n# read in data\nclaims <- paste(url, 'claims-multi-tfidf.csv', sep = '') %>%\n  read_csv()\n\n# preview\nclaims\n\n# A tibble: 552 × 15,871\n   mclass    .id   bclass  adams afternoon  agent android    app arkansas arrest\n   <chr>     <chr> <chr>   <dbl>     <dbl>  <dbl>   <dbl>  <dbl>    <dbl>  <dbl>\n 1 unlawful  url1  relev… 0.0692    0.0300 0.0365  0.0450 0.0330   0.0390 0.0140\n 2 irreleva… url3  irrel… 0         0      0       0      0        0      0     \n 3 other     url4  relev… 0         0      0       0      0        0      0     \n 4 fatality  url5  relev… 0         0      0       0      0        0      0     \n 5 irreleva… url7  irrel… 0         0      0       0      0        0      0.0107\n 6 fatality  url8  relev… 0         0      0       0      0        0      0     \n 7 irreleva… url9  irrel… 0         0      0       0      0        0      0     \n 8 irreleva… url10 irrel… 0         0      0       0      0        0      0     \n 9 unlawful  url11 relev… 0         0      0       0      0        0      0.0716\n10 unlawful  url12 relev… 0         0      0       0      0        0      0.0161\n# … with 542 more rows, and 15,861 more variables: arrive <dbl>, assist <dbl>,\n#   attic <dbl>, barricade <dbl>, block <dbl>, blytheville <dbl>, burn <dbl>,\n#   captain <dbl>, catch <dbl>, check <dbl>, chemical <dbl>, copyright <dbl>,\n#   county <dbl>, custody <dbl>, dehydration <dbl>, demand <dbl>,\n#   department <dbl>, desktop <dbl>, device <dbl>, dispute <dbl>,\n#   division <dbl>, drug <dbl>, enter <dbl>, exit <dbl>, family <dbl>,\n#   federal <dbl>, fire <dbl>, force <dbl>, gas <dbl>, gray <dbl>, …"
  },
  {
    "objectID": "materials/activities/multinomial-logit.html#activity-1-10-min",
    "href": "materials/activities/multinomial-logit.html#activity-1-10-min",
    "title": "Activity: multinomial logistic regression",
    "section": "Activity 1 (10 min)",
    "text": "Activity 1 (10 min)\nYou’ll be given about ten minutes to do the following on your group’s workstation.\n\nPartition the data into training and test sets.\nUsing the training data, find principal components that preserve at least 80% of the total variance and project the data onto those PCs.\nFit a logistic regression model to the training data.\n\n\nStep 1: partitioning\nThis should be familiar from last week’s lab. Use the code chunk below to partition the data. Do not change the RNG seed or split proportion!\n\n# partition data\nset.seed(102722)\npartitions <- claims %>% initial_split(prop = 0.8)\n\n# separate DTM from labels\ntest_dtm <- testing(partitions) %>%\n  select(-.id, -bclass, -mclass)\ntest_labels <- testing(partitions) %>%\n  select(.id, bclass, mclass)\n\n# same, training set\ntrain_dtm <- training(partitions) %>%\n  select(-.id, -bclass, -mclass)\ntrain_labels <- training(partitions) %>%\n  select(.id, bclass, mclass)\n\nNote that we have separated the document term matrix (DTM) from the labels for both partitions. When we project the data onto a subspace, we only want to project the DTM and not the labels.\n\n\nStep 2: projection\nNow find the number of principal components that capture at least 70% of variation and project the document term matrix (DTM) onto those components. Use the custom function projection_fn(.dtm, .prop) .\n\n# find projections based on training data\nproj_out <- projection_fn(.dtm = train_dtm, .prop = 0.7)\ntrain_dtm_projected <- proj_out$data\n\n# how many components were used?\nproj_out$n_pc\n\nNote: projections were found using the training data only. The test data will ultimately be projected onto the same components, as if it were new information we were feeding into a predictive model developed entirely using the training data.\n\n\nStep 3: regression\nBind the binary labels to the projected document term matrix and fit a logistic regression model.\nThe code chunk below gives you the input data frame you need to use glm(). It’s up to you to specify the other arguments needed to fit the model.\n\ntrain <- train_labels %>%\n  transmute(bclass = factor(bclass)) %>%\n  bind_cols(train_dtm_projected)\n\nfit <- glm(..., data = train, ...)\n\nYou will most likely get a warning of some kind – that’s expected. Take note of what the warning says and stop here.\n\n\n\n\n\n\nImportant\n\n\n\nBriefly discuss with your table: what do you think the warning means?"
  },
  {
    "objectID": "materials/activities/multinomial-logit.html#activity-2-10-min",
    "href": "materials/activities/multinomial-logit.html#activity-2-10-min",
    "title": "Activity: multinomial logistic regression",
    "section": "Activity 2 (10 min)",
    "text": "Activity 2 (10 min)\nThis part will guide you through the following steps.\n\nFit a logistic regression model with an elastic net penalty to the training data.\nQuantify classification accuracy on the test data using sensitivity, specificity, and AUROC.\n\n\nStep 1: fit a regularized logistic regression\nglmnet implements the elastic net penalty when a parameter alpha is provided. In the function call, a predictor matrix and response vector are used to specify the model instead of a formula.\nUse the code chunk below to fit the model for a path of regularization strengths, select a strength, and extract the fitted model corresponding to that strength. Do not adjust the RNG seed.\n\n# store predictors and response as matrix and vector\nx_train <- train %>% select(-bclass) %>% as.matrix()\ny_train <- train_labels %>% pull(bclass)\n\n# fit enet model\nalpha_enet <- 0.3\nfit_reg <- glmnet(x = x_train, \n                  y = y_train, \n                  family = 'binomial',\n                  alpha = alpha_enet)\n\n# choose a strength by cross-validation\nset.seed(102722)\ncvout <- cv.glmnet(x = x_train, \n                y = y_train, \n                family = 'binomial',\n                alpha = alpha_enet)\n\n# store optimal strength\nlambda_opt <- cvout$lambda.min\n\n# view results\ncvout\n\n\n\n\n\n\n\nNote\n\n\n\nComment. The elastic net parameter alpha controls the balance between ridge and LASSO penalties: alpha = 0 corresponds to ridge regression, alpha = 1 corresponds to LASSO, and all other values specify a mixture. When the parameter is closer to 1, the LASSO penalty is stronger relative to ridge; and vice-versa when it’s closer to 0.\n\n\n\n\nStep 2: prediction\nTo compute predictions, we’ll need to project the test data onto the same directions used to transform the training data.\nOnce that’s done, we can simply feed the projected test data, the fitted model fit_reg, and the optimal strength lambda_opt to a predict() call.\n\n# project test data onto PCs\ntest_dtm_projected <- reproject_fn(.dtm = test_dtm, proj_out)\n\n# coerce to matrix\nx_test <- as.matrix(test_dtm_projected)\n\n# compute predicted probabilities\npreds <- predict(fit_reg, \n                 s = lambda_opt, \n                 newx = x_test,\n                 type = 'response')\n\nNext bind the test labels to the predictions:\n\n# store predictions in a data frame with true labels\npred_df <- test_labels %>%\n  transmute(bclass = factor(bclass)) %>%\n  bind_cols(pred = as.numeric(preds)) %>%\n  mutate(bclass.pred = factor(pred > 0.5, \n                              labels = levels(bclass)))\n\n# define classification metric panel \npanel <- metric_set(sensitivity, \n                    specificity, \n                    accuracy, \n                    roc_auc)\n\n# compute test set accuracy\npred_df %>% panel(truth = bclass, \n                  estimate = bclass.pred, \n                  pred, \n                  event_level = 'second')\n\n\n\n\n\n\n\nImportant\n\n\n\nBriefly discuss with your table:\n\nHow satisfied are you with the predictive performance?\nDoes the classifier do a better job picking out relevant pages or irrelevant pages?"
  },
  {
    "objectID": "materials/activities/multinomial-logit.html#activity-3-10-min",
    "href": "materials/activities/multinomial-logit.html#activity-3-10-min",
    "title": "Activity: multinomial logistic regression",
    "section": "Activity 3 (10 min)",
    "text": "Activity 3 (10 min)\nNow we’ll fit a multinomial logistic regression model using the multiclass labels rather than the binary ones, still using regularization to prevent overfitting.\n\nStep 1: multinomial regression\nUse the code chunk below to do the fitting. Notice that it’s as simple as supplying the multiclass labels and changing the family = 'binomial' to family = 'multinomial' , but the number of non-intercept parameters is now\n\\[\n\\text{number of predictors} \\times (\\text{number of classes} - 1)\n\\]\nSo in our case, the logistic regression model had \\(p = 55\\) , but when we fit a multinomial model to the data using labels with \\(k = 5\\) classes, we have \\(p(k - 1) = 220\\) parameters!\n\n# get multiclass labels\ny_train_multi <- train_labels %>% pull(mclass)\n\n# fit enet model\nalpha_enet <- 0.2\nfit_reg_multi <- glmnet(x = x_train, \n                  y = y_train_multi, \n                  family = 'multinomial',\n                  alpha = alpha_enet)\n\n# choose a strength by cross-validation\nset.seed(102722)\ncvout_multi <- cv.glmnet(x = x_train, \n                   y = y_train_multi, \n                   family = 'multinomial',\n                   alpha = alpha_enet)\n\n# view results\ncvout\n\n\n\nStep 2: predictions\nThe predictions from this model are a set of proabilities, one per class:\n\npreds_multi <- predict(fit_reg_multi, \n        s = cvout_multi$lambda.min, \n        newx = x_test,\n        type = 'response')\n\nas_tibble(preds_multi[, , 1]) \n\nIf we choose the most probable class as the prediction and cross-tabulate with the actual label, we end up with the following table:\n\npred_class <- as_tibble(preds_multi[, , 1]) %>% \n  mutate(row = row_number()) %>%\n  pivot_longer(-row, \n               names_to = 'label',\n               values_to = 'probability') %>%\n  group_by(row) %>%\n  slice_max(probability, n = 1) %>%\n  pull(label)\n\npred_tbl <- table(pull(test_labels, mclass), pred_class)\n\npred_tbl\n\n\n\n\n\n\n\nImportant\n\n\n\nIf time, take a moment to discuss:\n\nWhat do you think of the overall accuracy?\nWhich classes are well-predicted and which are not?\nDo you prefer the logistic or multinomial regression and why?"
  },
  {
    "objectID": "materials/course-materials.html",
    "href": "materials/course-materials.html",
    "title": "Course materials",
    "section": "",
    "text": "Objectives: set expectations; explore data science raison d’etre; introduce systems and design thinking; introduce software tools and collaborative coding; conduct exploratory/descriptive analysis of class background and interests.\n\n\n\nWednesday meeting: Course orientation [slides]\nAssignments due by next class meeting:\n\ninstall course software and create github account;\nfill out intake form\nread Peng and Parker (2022);\nprepare a reading reading response\n\n\n\n\n\n\nMonday meeting: On projects in(volving) data science [slides]\nSection meeting: software and technology overview [activity] Teams spreadsheet\nWednesday meeting: basic GitHub actions [activity] [slides]\nAssignments due by next class meeting:\n\nread MDSR 9.1 and 9.2\nprepare a reading response\n\n\n\n\n\n\nMonday meeting: Introducing class intake survey data [slides]\nSection meeting: tidyverse basics [activity]\nWednesday meeting: planning group work for analysis of survey data [slides]\nAssignments:\n\nfirst team assignment due Wednesday, October 26, 11:59 PM PST [accept via GH classroom here]Teams spreadsheet"
  },
  {
    "objectID": "materials/course-materials.html#module-1-biomarker-identification",
    "href": "materials/course-materials.html#module-1-biomarker-identification",
    "title": "Course materials",
    "section": "Module 1: biomarker identification",
    "text": "Module 1: biomarker identification\nObjectives: introduce variable selection, classification, and multiple testing problems; discuss classification accuracy metrics and data partitioning; fit logistic regression and random forest classifiers in R; learn to implement multiple testing corrections for FDR control (Benjamini-Hochberg and Benjamini-Yekutieli); discuss selection via penalized estimation. Data from Hewitson et al. (2021) ."
  },
  {
    "objectID": "materials/course-materials.html#module-2-fraud-claims",
    "href": "materials/course-materials.html#module-2-fraud-claims",
    "title": "Course materials",
    "section": "Module 2: fraud claims",
    "text": "Module 2: fraud claims\nObjectives: introduce NLP techniques for converting text to data and web scraping tools in R; discuss dimension reduction techniques; introduce multiclass classification; learn to process text, fit multinomial logistic regression models, and train neural networks in R.\n\nWeek 5\n\nTuesday meeting: data introduction and basic NLP techniques [slides]\nSection meeting: string manipulation and text processing in R [activity]\nThursday meeting: dimension reduction; multinomial logistic regression [slides] [activity]\nOptional further reading:\n\nMDSR Ch. 19\nCambria and White (2014)\nKhan et al. (2010)\n\n\n\n\nWeek 6\n\nTuesday meeting: feedforward neural networks [slides]\nSection meeting: fitting neural nets with keras [activity]\nThursday meeting: assignment review and planning [slides]\nAssignments:\n\nMidquarter assessments [form]\nRequest winter add code [form]\nRead Emmert-Streib et al. (2020) (§1-5, §9) and prepare a reading response\nthird group assignment due Monday, November 14, 11:59pm PST [accept via GH classroom] [group assignments]\n\nOptional further reading:\n\nAlzubaidi et al. (2021)\nGoodfellow, Bengio, and Courville (2016) Ch. 6 (advanced)"
  },
  {
    "objectID": "materials/course-materials.html#module-3-soil-temperatures",
    "href": "materials/course-materials.html#module-3-soil-temperatures",
    "title": "Course materials",
    "section": "Module 3: soil temperatures",
    "text": "Module 3: soil temperatures\nObjectives: build a forecasting model; introduce concepts of spatial and temporal correlation; discuss function approximation and curve fitting with regression techniques; fit elementary time series models and regression with AR errors; spatial interpolation.\n\nWeek 7\n\nTuesday meeting: data introduction; function approximation using basis expansions [slides]\nSection meeting: curve fitting [activity]\nThursday meeting: temporal correlation; a forecasting model [slides]\nOptional further reading (available through UCSB library)\n\nSections 1.1, 1.2, and 2.3 in Shumway and Stoffer (2017)\nPerperoglou et al. (2019)\n\n\n\n\nWeek 8\n\nTuesday meeting: spatial prediction [slides]\nSection meeting: forecasting [activity]\nThursday meeting: NO CLASS\nOptional further reading:\n\n8.1 – 8.3 in Bivand et al. (2008)\nCh. 12 in Dorman (2022, link)"
  },
  {
    "objectID": "materials/course-materials.html#module-4-vignettes",
    "href": "materials/course-materials.html#module-4-vignettes",
    "title": "Course materials",
    "section": "Module 4: vignettes",
    "text": "Module 4: vignettes\nObjectives: learn independently about a method of choice and prepare a teaching vignette illustrating its use; create shared reference material potentially useful for project work.\n\nWeek 9\n\nTuesday meeting: discussion on results of claims module; vignette workshopping [slides]\nSection meeting: NO SECTION MEETING (Thanksgiving)\nThursday meeting: NO CLASS (Thanksgiving)\nAssignments: vignettes [guidelines]\n\ndrafts due in class Thursday, 12/1 2pm PST\nfinal version due Thursday, 12/8 11:59pm PST\n\n\n\n\nWeek 10\n\nTuesday meeting: capstone project overviews [slides]\nSection meeting: office hours for vignette help\nThursday meeting: vignette presentation/exchange/feedback [feedback form]\nAssignments due by Friday, 12/2:\n\nread project abstracts\nfill out preference form (will be active end of day 11/29)"
  },
  {
    "objectID": "materials/labs/lab1-setup/lab1-setup.html",
    "href": "materials/labs/lab1-setup/lab1-setup.html",
    "title": "Course technology overview",
    "section": "",
    "text": "Read this and complete all instructions in the ‘action’ boxes during your lab section. Your TA will walk you through the activity and help to troubleshoot issues and answer any questions along the way.\nObjectives:"
  },
  {
    "objectID": "materials/labs/lab1-setup/lab1-setup.html#prerequisites",
    "href": "materials/labs/lab1-setup/lab1-setup.html#prerequisites",
    "title": "Course technology overview",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nTo complete the activity you’ll need to:\n\nhave all of the software listed on the course technology page installed;\nfind (or create) your GitHub account credentials (if you are creating an account for the first time, see advice on choosing a username).\n\n\n\n\n\n\n\nAction\n\n\n\nPreparations:\n\nLog in to your GitHub account.\nOpen your GitHub client.\nOpen a new session in RStudio.\nCreate a class folder for PSTAT197 somewhere on your machine, e.g., ~/documents/pstat197."
  },
  {
    "objectID": "materials/labs/lab1-setup/lab1-setup.html#rstudio-projects",
    "href": "materials/labs/lab1-setup/lab1-setup.html#rstudio-projects",
    "title": "Course technology overview",
    "section": "RStudio projects",
    "text": "RStudio projects\nFirst we’ll get acquainted with the basic functionality of the RStudio IDE and the use of projects as a means of organizing files. If you’ve already used RStudio, great – this will still serve to introduce you to how we’ll use RStudio projects in this class.\n\nRStudio Setup\n\nYour TA will briefly review the (default) layout of the RStudio IDE. You should be able to identify/find the following:\n\nconsole\nterminal\nfile navigator\nenvironment\nhistory\n\nWe’ll use several R packages throughout the quarter. Some of these we will install on the go, but we can install several that we’ll rely on now.\n\n\n\n\n\n\nAction\n\n\n\nInstall packages\nNavigate to the console and copy-paste the following commands. You only need to do this once. This will take a minute or two to complete.\n\n\n\n# package install list \nurl &lt;- 'https://raw.githubusercontent.com/PSTAT197-F23/pstat197a/main/materials/scripts/package-installs.R'\nsource(url)\n\n\n# clear environment\nrm(list = ls())\n\n\n\nCreate a local project\nProjects are a means of keeping your work organized. When you create a project in a directory on your local machine, RStudio keeps track of project metadata, history, and the working environment so that every time you open the project you see whatever you had open when you last closed it.\n\n\n\n\n\n\nAction\n\n\n\nCreate a new project:\n\nSelect File &gt; New project\nCreate the project in a new directory as a subdirectory of your class folder\nName it example-project\n\nComment: when naming files it’s good practice to avoid spaces, special characters, and the like. A naming convention we try to follow: choose a descriptive name comprising 1-3 words or common abbreviations separated by hyphens.\n\n\nTake a moment to observe the file navigator. It should consist of a single example-project.Rproj file.\n\n\nAdd content\nWe may as well populate the project with a few files – so let’s add a dataset and write a short script, as if we’re just starting a data analysis.\n\n\n\n\n\n\nAction\n\n\n\nRetrieve data and store a local copy\n\nOpen a new script: File &gt; New File &gt; R Script\nIn the navigator, create a folder called data and a folder called scripts\nCopy and paste the code chunk below into your script.\nExecute once, then save in the scripts folder as data-retrieval.R and close\n\n\n\n\nlibrary(tidyverse)\n\n# retrieve pollution data\nurl &lt;- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab1-setup/data/pollution.csv'\npollution &lt;- read_csv(url)\n\n# write as csv to file\nwrite_csv(pollution, file = 'data/pollution.csv')\n\n# clear environment\nrm(list = ls())\n\n\nNext, we’ll do a simple regression analysis.\n\n\n\n\n\n\nAction\n\n\n\nCreate a script\n\nCreate a new script as before\nCopy-paste the code chunk below into your script\nExecute once and examine the results\nSave in the scripts folder as slr-analysis.R\n\n\n\n\nlibrary(tidyverse)\n\n# load data\npollution &lt;- read_csv('data/pollution.csv')\n\n# examine scatterplot with SLR fit\nggplot(pollution,\n       aes(x = log(SO2), y = Mort)) +\n  geom_point() +\n  geom_smooth(method = 'lm')\n\n\n\n# compute SLR fit\nfit &lt;- lm(Mort ~ log(SO2), data = pollution)\nbroom::tidy(fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    887.      17.6      50.4  1.37e-49\n2 log(SO2)        16.7      4.99      3.35 1.40e- 3\n\n# interpret\nfit_ci &lt;- confint(fit, parm = 'log(SO2)')*log(1.2)\n\npaste('With 95% confidence, every 20% increase in sulfur dioxide pollution is associated with an increase in two-year mortality rate between', \n      round(fit_ci[1], 2), \n      'and', \n      round(fit_ci[2], 2), \n      'per 100k', sep = ' ') %&gt;% \n  print()\n\n[1] \"With 95% confidence, every 20% increase in sulfur dioxide pollution is associated with an increase in two-year mortality rate between 1.23 and 4.87 per 100k\"\n\n\n\nCongrats on your first project! You can close the RStudio session now.\nWe’ll be using projects structured much like what you just set up, but with one catch: we’ll link up our RStudio projects with shared repositories so that we can all collaborate on the same set of project files."
  },
  {
    "objectID": "materials/labs/lab1-setup/lab1-setup.html#github-repositories",
    "href": "materials/labs/lab1-setup/lab1-setup.html#github-repositories",
    "title": "Course technology overview",
    "section": "GitHub repositories",
    "text": "GitHub repositories\n\nWe will be distributing course assignments as repositories via GitHub Classroom. A repository is simply a storage space.\nHere we’ll walk you through how to access and copy the files in a repository just as you will for course assignments. The first step is to accept an assignment through a link we’ve given to you – this will create a repository for you with the files we intend for you to have.\nFor now, we’ll make a ‘group sandbox’ that you can play in during our next class meeting.\n\n\n\n\n\n\nAction\n\n\n\nAccept an assignment in GitHub Classroom\n\nFollow the link to accept the group-sandbox assignment. Since it’s a group assignment, you will be prompted to join a team.\nJoin/create a team. Groups should have 3-5 members.\n\nYou should be directed to a team repository on github.com. You may need to refresh your browser. Keep this window open; you will need the URL.\n\n\n\nGit and GitHub\nAt some point in time – possibly quite recently – you had to install Git on your local machine, as well as create a GitHub account. So, Git and GitHub are two different things.\nGit is version control software that enables you to systematically track and control file changes within a repository – a collection of files possibly with some directory structure. (The definition of ‘repository’ is simply ‘storage place’.)\nGitHub is an online platform for hosting repositories remotely. Anyone with access to a repository can make changes to files in the repository, and this enables multiple people to collaborate on code.\n\n\nlocal &lt;&gt; remote\nUsually remote repositories are not updated directly because contributors need to execute codes to test their changes and the remote server that hosts the repository is not equipped to do this.\nInstead, contributors will prepare changes on their own machine where they can test them, and then update the remote repository once their changes are complete.\nThis process of implementing file changes in a repository involves communicating information between local and remote locations. For this purpose a local copy of the remote repository is needed.\n\n\nCloning a repository\nIn Git lingo, a clone is a local copy of a remote repository. Creating a clone copies files and establishes the link between local and remote repositories so that changes can be sent to and received from the remote repository. You only need to create a clone once.\nTo clone a repository, all one needs is:\n\nthe remote location URL;\nthe local destination where the clone will be created;\npermission from the repository owner, if private.\n\nHere you’ll clone the group sandbox repository you just created/joined. You will need the URL; if you happened to close the page when you accepted the assignment earlier, you should be able to find the repository from your home page on github.com.\n\n\n\n\n\n\n\nAction\n\n\n\nClone the sandbox repository:\n\nOpen your GitHub client (GitKracken or GitHub Desktop or similar) and ensure you are logged in to your GitHub account.\nLook for a ‘Clone Repo’ menu item or similar and simply input the URL and the place you’d like to clone it; proceed through any prompts.\nCheck your file navigator to confirm that the repository files were copied.\n\n\n\n\nAn alternative possibility is to create the clone using a terminal command. In the terminal, navigate to the desired destination, and input:\ngit clone https://github.com/USERNAME/REPONAME\n\n\n\n\n\n\nRemarks\n\n\n\nOn terminal commands:\n\nIt’s recommended to manage Git actions through a visual client, as it’s much easier to see and understand what’s happening.\nHowever, if you know exactly what you’re doing, executing simple actions via Git bash in the terminal can be more efficient at times.\nFor example, you can keep a terminal open in RStudio and manage your repository workflow from there, without having to toggle between environments.\nTry experimenting with terminal commands from RStudio after you have a little experience with basic Git actions."
  },
  {
    "objectID": "materials/labs/lab1-setup/lab1-setup.html#checklist",
    "href": "materials/labs/lab1-setup/lab1-setup.html#checklist",
    "title": "Course technology overview",
    "section": "Checklist",
    "text": "Checklist\nHave you completed all of the activity action items?\n\nInstall software: R, RStudio, Git, and a GitHub client\nCreate a GitHub account\nInstall R packages that will be used frequently\nCreate a local project in RStudio\nAccept the group sandbox assignment on GitHub Classroom\nClone the group sandbox repo"
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html",
    "title": "Tidyverse basics",
    "section": "",
    "text": "Read through the R Basics section and then complete all actions in the Tidyverse basics section. This lab is for your own benefit and no submission is expected.\nObjectives:"
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#data-types",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#data-types",
    "title": "Tidyverse basics",
    "section": "Data types",
    "text": "Data types\nThere are five main data types in R.\nNumeric (double- or single-precision floating point) data represent real numbers. Numeric data are abbreviated num and by default are stored as double-precision floating point.\n\n# a number\n4.5\n\n[1] 4.5\n\n# check structure\nstr(4.5)\n\n num 4.5\n\n# stored as double\nis.double(4.5)\n\n[1] TRUE\n\n\nInteger data are integers. For the most part they behave like numeric data, except occupy less memory, which can in some cases be convenient. To distinguish integers from doubles, R uses a trailing L after values; the data type is abbreviated int.\n\n# an integer\n4L\n\n[1] 4\n\n# check structure\nstr(4L)\n\n int 4\n\n\nLogical data are binary and represented in R as having values TRUE and FALSE. They are abbreviated logi in R. Often they are automatically coerced to integer data with values 0 (false) and 1 (true) to perform arithmetic and other operations.\n\n# logical value\nTRUE\n\n[1] TRUE\n\n# check structure\nstr(TRUE)\n\n logi TRUE\n\n# arithmetic\nTRUE + FALSE\n\n[1] 1\n\n# check structure\nstr(FALSE + FALSE)\n\n int 0\n\n\nCharacter data represent strings of text and are sometimes called ‘strings’. They are abbreviated chr in R and values are surrounded by quotation marks; this distinguishes, for example, the character 4 from the number 4. Single quotations can be used to input strings as well as double quotations. Arithmetic is not possible with strings for obvious reasons.\n\n# a character string\n'yay'\n\n[1] \"yay\"\n\n# check structure\nstr('yay')\n\n chr \"yay\"\n\n# string arithmetic won't work\n'4' + '1'\n\nError in \"4\" + \"1\": non-numeric argument to binary operator\n\n# but can be performed after coercing character to string\nas.numeric('4') + as.numeric('1')\n\n[1] 5\n\n\nFactor data represent categorical variables. In R these are encoded numerically according to the number of ‘levels’ of the factor, which represent the unique values of the categorical variable, and each level is labeled. R will print the labels, not the levels, of factors; the data type is abbreviated fct.\n\n# a factor\nfactor(1, levels = c(1, 2), labels = c('blue', 'red'))\n\n[1] blue\nLevels: blue red\n\n# less verbose definition\nfactor('blue', levels = c('blue', 'red'))\n\n[1] blue\nLevels: blue red\n\n# check structure\nstr(factor('blue', levels = c('blue', 'red')))\n\n Factor w/ 2 levels \"blue\",\"red\": 1\n\n\nUsually factors won’t be defined explicitly, but instead interpreted from character data. The levels and labels of factors can be manipulated using a variety of helper functions."
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#object-classes",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#object-classes",
    "title": "Tidyverse basics",
    "section": "Object classes",
    "text": "Object classes\nThe most basic type of object in R is a vector. Vectors are concatenations of data values of the same type. They are defined using the concatenation operator c() and are indexed by consecutive integers; subvectors can be retrieved by specifying the indices between square brackets.\n\n# numeric vector\nc(1, 4, 7)\n\n[1] 1 4 7\n\n# character vector\nc('blue', 'red')\n\n[1] \"blue\" \"red\" \n\n# indexing\nc(1, 4, 7)[1]\n\n[1] 1\n\nc(1, 4, 7)[2]\n\n[1] 4\n\nc(1, 4, 7)[3]\n\n[1] 7\n\nc(1, 4, 7)[2:3]\n\n[1] 4 7\n\nc(1, 4, 7)[c(1, 3)]\n\n[1] 1 7\n\n\nUsually objects are assigned names for easy retrieval. Vectors will not show any special object class if the structure is examined; str() will simply return the data type, index range, and the values.\n\n# assign a name\nmy_vec &lt;- c(1, 4, 7)\n\n# check structure\nstr(my_vec)\n\n num [1:3] 1 4 7\n\n\nNext up in complexity are arrays. These are blocks of data values of the same type indexed along two or more dimensions. For arrays, str() will return the data type, index structure, and data values; when printed directly, data values are arranged according to the indexing.\n\n# an array\nmy_ary &lt;- array(data = c(1, 2, 3, 4, 5, 6, 7, 8), \n           dim = c(2, 4))\n\nmy_ary\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    3    5    7\n[2,]    2    4    6    8\n\nstr(my_ary)\n\n num [1:2, 1:4] 1 2 3 4 5 6 7 8\n\n# another array\nmy_oth_ary &lt;- array(data = c(1, 2, 3, 4, 5, 6, 7, 8), \n           dim = c(2, 2, 2))\n\nmy_oth_ary\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 2\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\nstr(my_oth_ary)\n\n num [1:2, 1:2, 1:2] 1 2 3 4 5 6 7 8\n\n\nFor arrays, elements can be retrieved by index coordinates, and slices can be retrieved by leaving index positions blank, which will return all elements along the corresponding indices.\n\n# one element\nmy_ary[1, 2]\n\n[1] 3\n\n# one element\nmy_oth_ary[1, 2, 1]\n\n[1] 3\n\n# a slice (second row)\nmy_ary[2, ]\n\n[1] 2 4 6 8\n\n# a slice (first layer)\nmy_oth_ary[ , , 1]\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\nNext there are lists, which are perhaps the most flexible data structure. A list is an indexed collection of any objects.\n\n# a list\nlist('cat', c(1, 4, 7), TRUE)\n\n[[1]]\n[1] \"cat\"\n\n[[2]]\n[1] 1 4 7\n\n[[3]]\n[1] TRUE\n\n# a named list\nlist(animal = 'cat',\n     numbers = c(1, 4, 7),\n     short = TRUE)\n\n$animal\n[1] \"cat\"\n\n$numbers\n[1] 1 4 7\n\n$short\n[1] TRUE\n\n\nList elements can be retrieved by index in double square brackets, or by name.\n\n# assign a name\nmy_lst &lt;- list(animal = 'cat',\n               numbers = c(1, 4, 7),\n               short = TRUE)\n\n# check structure\nstr(my_lst)\n\nList of 3\n $ animal : chr \"cat\"\n $ numbers: num [1:3] 1 4 7\n $ short  : logi TRUE\n\n# retrieve an element\nmy_lst[[1]]\n\n[1] \"cat\"\n\n# equivalent\nmy_lst$animal\n\n[1] \"cat\"\n\n\nFinally, data frames are type-heterogeneous lists of vectors of equal length. More informally, they are 2D arrays with columns of differing data types. str() will essentially show the list structure; but when printed, data frames will appear arranged in a table.\n\n# a data frame\nmy_df &lt;- data.frame(animal = c('cat', 'hare', 'tortoise'),\n                    has.fur = c(TRUE, TRUE, FALSE),\n                    weight.lbs = c(9.1, 8.2, 22.7))\n\nstr(my_df)\n\n'data.frame':   3 obs. of  3 variables:\n $ animal    : chr  \"cat\" \"hare\" \"tortoise\"\n $ has.fur   : logi  TRUE TRUE FALSE\n $ weight.lbs: num  9.1 8.2 22.7\n\nmy_df\n\n    animal has.fur weight.lbs\n1      cat    TRUE        9.1\n2     hare    TRUE        8.2\n3 tortoise   FALSE       22.7\n\n\nThe data frame is the standard object type for representing datasets in R. For the most part, modern computing in R is designed around the data frame."
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#packages",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#packages",
    "title": "Tidyverse basics",
    "section": "Packages",
    "text": "Packages\nR packages are add-ons that can include special functions, datasets, object classes, and the like. They are published software and can be installed using install.packages('PACKAGE NAME') and, once installed, loaded via library('PACKAGE NAME') or require('PACKAGE NAME')."
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#concepts",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#concepts",
    "title": "Tidyverse basics",
    "section": "Concepts",
    "text": "Concepts\nThe tidyverse is a collection of packages for data manipulation, visualization, and statistical modeling. Some are specialized, such as forcats or lubridate, which contain functions for manipulating factors and dates and times, respectively. The packages share some common underyling principles.\n\nPackages are built around the data frame\nFunctions are designed to work with the pipe operator %&gt;%\nPackages facilitate readable code\n\nThe tidyverse facilitates programming in readable sequences of steps that are performed on dataframe. For example:\n\nmy_df %&gt;% STEP1() %&gt;% STEP2() %&gt;% STEP3()\n\nIf it helps, imagine that step 1 is defining a new variable, step 2 is selecting a subset of columns, and step 3 is fitting a model of some kind."
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#tibbles",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#tibbles",
    "title": "Tidyverse basics",
    "section": "Tibbles",
    "text": "Tibbles\ntidyverse packages leverage a slight generalization of the data frame called a tibble. For the most part, tibbles behave as data frames do, but they are slightly more flexible in ways you’ll encounter later.\nFor now, think of a tibble as just another name for a data frame."
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#the-pipe-operator",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#the-pipe-operator",
    "title": "Tidyverse basics",
    "section": "The pipe operator %>%",
    "text": "The pipe operator %&gt;%\nIn short, x %&gt;% f(y) is equivalent to f(x, y) .\nIn other words, the pipe operator ‘pipes’ the result of the left-hand operation into the first argument of the right-hand function.\n\n# a familiar example\nmy_vec &lt;- c(1, 2, 5) \nstr(my_vec)\n\n num [1:3] 1 2 5\n\n# use the pipe operator instead\nmy_vec %&gt;% str()\n\n num [1:3] 1 2 5"
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#dplyr-verbs",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#dplyr-verbs",
    "title": "Tidyverse basics",
    "section": "dplyr verbs",
    "text": "dplyr verbs\nThe dplyr package contains functions for manipulating data frames (tibbles). The functions are named with verbs that describe common operations.\n\nCore verbs\n\n\n\n\n\n\nAction\n\n\n\nFor each verb listed below, copy the code chunk into your script and execute.\nGo through the list with your neighbor and check your understanding by describing what the code example accomplishes.\n\n\nfilter – filter the rows of a data frame according to a condition and return a subset of rows meeting that condition\n\n# filter rows\nbackground %&gt;%\n  filter(math.comf &gt; 3)\n\n# A tibble: 37 × 30\n   response.id prog.prof prog.comf math.prof math.comf stat.prof stat.comf\n         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1           5 Int               3 Adv               4 Int               3\n 2           7 Int               3 Adv               4 Int               3\n 3           8 Adv               4 Adv               5 Adv               4\n 4          12 Int               5 Int               4 Int               4\n 5          13 Int               4 Adv               4 Adv               4\n 6          15 Adv               3 Adv               4 Adv               4\n 7          17 Int               3 Int               4 Int               4\n 8          18 Int               4 Adv               5 Adv               5\n 9          19 Adv               5 Adv               5 Adv               5\n10          20 Adv               4 Int               4 Adv               5\n# … with 27 more rows, and 23 more variables: updv.num &lt;chr&gt;, dom &lt;chr&gt;,\n#   consent &lt;chr&gt;, PSTAT100 &lt;chr&gt;, `PSTAT120A-B` &lt;chr&gt;, PSTAT126 &lt;chr&gt;,\n#   PSTAT127 &lt;chr&gt;, PSTAT134 &lt;chr&gt;, CS9 &lt;chr&gt;, PSTAT131 &lt;chr&gt;, PSTAT174 &lt;chr&gt;,\n#   LING104 &lt;chr&gt;, LING105 &lt;chr&gt;, PSTAT115 &lt;chr&gt;, PSTAT122 &lt;chr&gt;,\n#   `PSTAT160A-B` &lt;chr&gt;, CS16 &lt;chr&gt;, `CS5A-B` &lt;chr&gt;, POLS15 &lt;chr&gt;,\n#   LING110 &lt;chr&gt;, `CS130A-B` &lt;chr&gt;, `CS165A-B` &lt;chr&gt;, rsrch &lt;lgl&gt;\n\n\nselect – select a subset of columns from a data frame\n\n# select a column\nbackground %&gt;%\n  select(math.comf)\n\n# A tibble: 59 × 1\n   math.comf\n       &lt;dbl&gt;\n 1         3\n 2         3\n 3         3\n 4         3\n 5         4\n 6         4\n 7         5\n 8         3\n 9         3\n10         4\n# … with 49 more rows\n\n\npull – extract a single column from a data frame\n\n# pull a column\nbackground %&gt;%\n  pull(rsrch)\n\n [1] FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE\n[13] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE\n[25] FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n[37] FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n[49]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE\n\n\nmutate – define a new column as a function of existing columns\n\n# define a new variable\nbackground %&gt;%\n  mutate(avg.comf = (math.comf + prog.comf + stat.comf)/3)\n\n# A tibble: 59 × 31\n   response.id prog.prof prog.comf math.prof math.comf stat.prof stat.comf\n         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1           1 Adv               5 Int               3 Adv               4\n 2           2 Int               5 Int               3 Int               4\n 3           3 Int               3 Int               3 Int               4\n 4           4 Int               3 Int               3 Int               3\n 5           5 Int               3 Adv               4 Int               3\n 6           7 Int               3 Adv               4 Int               3\n 7           8 Adv               4 Adv               5 Adv               4\n 8           9 Int               3 Int               3 Int               3\n 9          10 Int               4 Int               3 Adv               4\n10          12 Int               5 Int               4 Int               4\n# … with 49 more rows, and 24 more variables: updv.num &lt;chr&gt;, dom &lt;chr&gt;,\n#   consent &lt;chr&gt;, PSTAT100 &lt;chr&gt;, `PSTAT120A-B` &lt;chr&gt;, PSTAT126 &lt;chr&gt;,\n#   PSTAT127 &lt;chr&gt;, PSTAT134 &lt;chr&gt;, CS9 &lt;chr&gt;, PSTAT131 &lt;chr&gt;, PSTAT174 &lt;chr&gt;,\n#   LING104 &lt;chr&gt;, LING105 &lt;chr&gt;, PSTAT115 &lt;chr&gt;, PSTAT122 &lt;chr&gt;,\n#   `PSTAT160A-B` &lt;chr&gt;, CS16 &lt;chr&gt;, `CS5A-B` &lt;chr&gt;, POLS15 &lt;chr&gt;,\n#   LING110 &lt;chr&gt;, `CS130A-B` &lt;chr&gt;, `CS165A-B` &lt;chr&gt;, rsrch &lt;lgl&gt;,\n#   avg.comf &lt;dbl&gt;\n\n\nThese operations can be chained together, for example:\n\n# sequence of verbs\nbackground %&gt;%\n  filter(stat.prof == 'Adv') %&gt;%\n  mutate(avg.comf = (math.comf + prog.comf + stat.comf)/3) %&gt;%\n  select(avg.comf, rsrch) \n\n# A tibble: 36 × 2\n   avg.comf rsrch\n      &lt;dbl&gt; &lt;lgl&gt;\n 1     4    FALSE\n 2     4.33 TRUE \n 3     3.67 FALSE\n 4     4    FALSE\n 5     3.67 FALSE\n 6     4.67 TRUE \n 7     5    TRUE \n 8     4.33 TRUE \n 9     4.33 TRUE \n10     4    TRUE \n# … with 26 more rows\n\n\n\n\n\n\n\n\n\nAction\n\n\n\n\nWrite a chain of verbs in order to find the proficiency ratings of all respondents with research experience and 6-8 upper division courses.\nWrite a chain of verbs in order to find the proficiency ratings of all respondents without research experience and the same number of upper division courses\nCompare results and discuss with your neighbor: do these suggest any patterns?\n\n\n\n\n\nSummaries\nSummaries are easily computed across rows using summarize() . So if for example we want to use the filtering and selection from before to find the proportion of advanced students in statistics with research experience, use:\n\n# a summary\nbackground %&gt;%\n  filter(stat.prof == 'Adv') %&gt;%\n  mutate(avg.comf = (math.comf + prog.comf + stat.comf)/3) %&gt;%\n  select(avg.comf, rsrch) %&gt;%\n  summarize(prop.rsrch = mean(rsrch))\n\n# A tibble: 1 × 1\n  prop.rsrch\n       &lt;dbl&gt;\n1      0.611\n\n# equivalent\nbackground %&gt;%\n  filter(stat.prof == 'Adv') %&gt;%\n  mutate(avg.comf = (math.comf + prog.comf + stat.comf)/3) %&gt;%\n  select(avg.comf, rsrch) %&gt;%\n  pull(rsrch) %&gt;%\n  mean()\n\n[1] 0.6111111\n\n\nThe advantage of summarize , however, is that multiple summaries can be computed at once:\n\nbackground %&gt;%\n  filter(stat.prof == 'Adv') %&gt;%\n  mutate(avg.comf = (math.comf + prog.comf + stat.comf)/3) %&gt;%\n  select(avg.comf, rsrch) %&gt;%\n  summarize(prop.rsrch = mean(rsrch),\n            med.comf = median(avg.comf))\n\n# A tibble: 1 × 2\n  prop.rsrch med.comf\n       &lt;dbl&gt;    &lt;dbl&gt;\n1      0.611        4\n\n\nThe variant summarize_all computes the same summary across all columns. (Notice the use of the helper verb contains() to select all columns containing a particular string.)\n\n# average comfort levels across all students\nbackground %&gt;%\n  select(contains('comf')) %&gt;%\n  summarise_all(.funs = mean)\n\n# A tibble: 1 × 3\n  prog.comf math.comf stat.comf\n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1      3.97      3.85      4.08\n\n\nGrouped summaries are summaries computed separately among subsets of observations. To define a grouping structure using an existing column, use group_by() . Notice the ‘groups’ attribute printed with the output.\n\n# create a grouping\nbackground %&gt;%\n  group_by(stat.prof)\n\n# A tibble: 59 × 30\n# Groups:   stat.prof [3]\n   response.id prog.prof prog.comf math.prof math.comf stat.prof stat.comf\n         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1           1 Adv               5 Int               3 Adv               4\n 2           2 Int               5 Int               3 Int               4\n 3           3 Int               3 Int               3 Int               4\n 4           4 Int               3 Int               3 Int               3\n 5           5 Int               3 Adv               4 Int               3\n 6           7 Int               3 Adv               4 Int               3\n 7           8 Adv               4 Adv               5 Adv               4\n 8           9 Int               3 Int               3 Int               3\n 9          10 Int               4 Int               3 Adv               4\n10          12 Int               5 Int               4 Int               4\n# … with 49 more rows, and 23 more variables: updv.num &lt;chr&gt;, dom &lt;chr&gt;,\n#   consent &lt;chr&gt;, PSTAT100 &lt;chr&gt;, `PSTAT120A-B` &lt;chr&gt;, PSTAT126 &lt;chr&gt;,\n#   PSTAT127 &lt;chr&gt;, PSTAT134 &lt;chr&gt;, CS9 &lt;chr&gt;, PSTAT131 &lt;chr&gt;, PSTAT174 &lt;chr&gt;,\n#   LING104 &lt;chr&gt;, LING105 &lt;chr&gt;, PSTAT115 &lt;chr&gt;, PSTAT122 &lt;chr&gt;,\n#   `PSTAT160A-B` &lt;chr&gt;, CS16 &lt;chr&gt;, `CS5A-B` &lt;chr&gt;, POLS15 &lt;chr&gt;,\n#   LING110 &lt;chr&gt;, `CS130A-B` &lt;chr&gt;, `CS165A-B` &lt;chr&gt;, rsrch &lt;lgl&gt;\n\n\nSometimes it can be helpful to simply count the observations in each group:\n\n# count observations\nbackground %&gt;%\n  group_by(stat.prof) %&gt;%\n  count()\n\n# A tibble: 3 × 2\n# Groups:   stat.prof [3]\n  stat.prof     n\n  &lt;chr&gt;     &lt;int&gt;\n1 Adv          36\n2 Beg           2\n3 Int          21\n\n\nTo compute a grouped summary, first group the data frame and then specify the summary of interest:\n\n# a grouped summary\nbackground %&gt;%\n  group_by(stat.prof) %&gt;%\n  select(contains('.comf')) %&gt;%\n  summarize_all(.funs = mean)\n\n# A tibble: 3 × 4\n  stat.prof prog.comf math.comf stat.comf\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Adv            4.06      4.14      4.39\n2 Beg            3.5       2.5       2.5 \n3 Int            3.86      3.48      3.71\n\n\n\n\n\n\n\n\nAction\n\n\n\nGrouped summaries\n\nCompute the median comfort level of all students in each subject area.\nCompute the median comfort level of all students in each subject area after grouping by number of upper division classes taken.\nCompare and discuss with your neighbor: do you notice any interesting patterns?"
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#tidyr-verbs",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#tidyr-verbs",
    "title": "Tidyverse basics",
    "section": "tidyr verbs",
    "text": "tidyr verbs\nIn general, tidyr verbs reshape data frames in various ways. For now, we’ll just cover two tidyr verbs.\nSuppose we want to calculate multiple summaries of multiple variables using the techniques above. By default, the output is one row with one column for each summary/variable combination:\n\n# many variables, many summaries\ncomf_sum &lt;- background %&gt;%\n  select(contains('comf')) %&gt;%\n  summarise_all(.funs = list(mean = mean, \n                             median = median,\n                             min = min, \n                             max = max))\n\ncomf_sum\n\n# A tibble: 1 × 12\n  prog.comf_mean math.comf_mean stat.comf_mean prog.comf_median math.comf_median\n           &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1           3.97           3.85           4.08                4                4\n# … with 7 more variables: stat.comf_median &lt;dbl&gt;, prog.comf_min &lt;dbl&gt;,\n#   math.comf_min &lt;dbl&gt;, stat.comf_min &lt;dbl&gt;, prog.comf_max &lt;dbl&gt;,\n#   math.comf_max &lt;dbl&gt;, stat.comf_max &lt;dbl&gt;\n\n\nIt would be much better to reshape this into a table. gather will reshape the data frame from wide format to long format by ‘gathering’ the columns together.\n\n# gather columns into long format\ncomf_sum %&gt;% gather(stat, val) \n\n# A tibble: 12 × 2\n   stat               val\n   &lt;chr&gt;            &lt;dbl&gt;\n 1 prog.comf_mean    3.97\n 2 math.comf_mean    3.85\n 3 stat.comf_mean    4.08\n 4 prog.comf_median  4   \n 5 math.comf_median  4   \n 6 stat.comf_median  4   \n 7 prog.comf_min     3   \n 8 math.comf_min     2   \n 9 stat.comf_min     2   \n10 prog.comf_max     5   \n11 math.comf_max     5   \n12 stat.comf_max     5   \n\n\nThis is a little better, but it would be more legible in a 2x2 table. We can separate the ‘stat’ variable that has the column names into two columns:\n\n# separate into rows and columns\ncomf_sum %&gt;%\n  gather(stat, val) %&gt;%\n  separate(stat, into = c('variable', 'stat'), sep = '_') \n\n# A tibble: 12 × 3\n   variable  stat     val\n   &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;\n 1 prog.comf mean    3.97\n 2 math.comf mean    3.85\n 3 stat.comf mean    4.08\n 4 prog.comf median  4   \n 5 math.comf median  4   \n 6 stat.comf median  4   \n 7 prog.comf min     3   \n 8 math.comf min     2   \n 9 stat.comf min     2   \n10 prog.comf max     5   \n11 math.comf max     5   \n12 stat.comf max     5   \n\n\nAnd then spread the stat column over a few rows, resulting in a table where the rows are the variables and the columns are the summaries:\n\n# spread into table\ncomf_sum %&gt;%\n  gather(stat, val) %&gt;%\n  separate(stat, into = c('variable', 'stat'), sep = '_') %&gt;%\n  spread(stat, val)\n\n# A tibble: 3 × 5\n  variable    max  mean median   min\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 math.comf     5  3.85      4     2\n2 prog.comf     5  3.97      4     3\n3 stat.comf     5  4.08      4     2"
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#ggplot",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#ggplot",
    "title": "Tidyverse basics",
    "section": "ggplot",
    "text": "ggplot\nThe ggplot package is for data visualization. The syntax takes some getting used to if you haven’t seen it before. We’ll just look at one example.\nSuppose we want to summarize the prior coursework in the class.\n\n# summary of classes taken\nclasses &lt;- background %&gt;%\n  select(11:29) %&gt;%\n  mutate_all(~factor(.x, levels = c('no', 'yes'))) %&gt;%\n  mutate_all(~as.numeric(.x) - 1) %&gt;%\n  summarize_all(mean) %&gt;%\n  gather(class, proportion)\n\nclasses\n\n# A tibble: 19 × 2\n   class       proportion\n   &lt;chr&gt;            &lt;dbl&gt;\n 1 PSTAT100        0.271 \n 2 PSTAT120A-B     0.983 \n 3 PSTAT126        0.932 \n 4 PSTAT127        0.169 \n 5 PSTAT134        0.305 \n 6 CS9             0.576 \n 7 PSTAT131        0.424 \n 8 PSTAT174        0.169 \n 9 LING104         0.0339\n10 LING105         0.0339\n11 PSTAT115        0.102 \n12 PSTAT122        0.356 \n13 PSTAT160A-B     0.458 \n14 CS16            0.492 \n15 CS5A-B          0.0508\n16 POLS15          0.0169\n17 LING110         0.0169\n18 CS130A-B        0.0508\n19 CS165A-B        0.0339\n\n\nWe could report the results in a table, in which case perhaps arranging in descending order may be helpful:\n\nclasses %&gt;% arrange(desc(proportion))\n\n# A tibble: 19 × 2\n   class       proportion\n   &lt;chr&gt;            &lt;dbl&gt;\n 1 PSTAT120A-B     0.983 \n 2 PSTAT126        0.932 \n 3 CS9             0.576 \n 4 CS16            0.492 \n 5 PSTAT160A-B     0.458 \n 6 PSTAT131        0.424 \n 7 PSTAT122        0.356 \n 8 PSTAT134        0.305 \n 9 PSTAT100        0.271 \n10 PSTAT127        0.169 \n11 PSTAT174        0.169 \n12 PSTAT115        0.102 \n13 CS5A-B          0.0508\n14 CS130A-B        0.0508\n15 LING104         0.0339\n16 LING105         0.0339\n17 CS165A-B        0.0339\n18 POLS15          0.0169\n19 LING110         0.0169\n\n\nLet’s say we’d rather plot this data. We’ll put the course number on one axis and the proportion of students who took it on the other.\n\n# plot it\nclasses %&gt;%\n  ggplot(aes(x = proportion, y = class)) +\n  geom_point()\n\n\n\n\nThese commands work by defining plot layers. In the chunk above, the first argument to ggplot() is the data. Then, aes() defines an ‘aesthetic mapping’ of the columns of the input data frame to graphical elements. This defines a set of axes. Then, a layer of points is added to the plot with geom_point() ; no arguments are needed because the geometric object (‘geom’) inherits attributes (x and y coordinates) from the aesthetic mapping.\nAgain we might prefer to arrange the classes by descending order in proportion.\n\nfig &lt;- classes %&gt;%\n  ggplot(aes(x = proportion, y = reorder(class, proportion))) +\n  geom_point()\n\nfig\n\n\n\n\nAnd perhaps fix the plot labels:\n\n# adjust labels\nfig + labs(x = 'proportion of class', y = '')\n\n\n\n\nNotice that ggplot allows for a plot to be stored by name and then further modified with additional layers."
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#checklist",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#checklist",
    "title": "Tidyverse basics",
    "section": "Checklist",
    "text": "Checklist\n\nAll actions were completed.\nAll code chunks were copied into your script.\nYour script is saved in a lab subfolder of your class directory with an associated project."
  },
  {
    "objectID": "materials/labs/lab3-iteration/lab3-iteration.html",
    "href": "materials/labs/lab3-iteration/lab3-iteration.html",
    "title": "Iteration strategies",
    "section": "",
    "text": "In class we discussed multiple testing in the context of an application that involved performing 1,317 \\(t\\)-tests. Implementing these tests involves iteration: repeatedly performing the same computations.\nObjective. Here we’ll look at a few strategies for iteration in R:\n\nloops\nfunctions in the apply family\nfunctional programming using tidyverse\n\nWe’ll illustrate these strategies using the biomarker data and reproduce some of the results shown in class.\n\n\n\n\n\n\nAction\n\n\n\nCreate a new script for lab 3 in your labs project/folder and copy-paste the code chunk below at the top of the script.\n\n\n\nlibrary(tidyverse)\n# install.packages('infer') # execute once then comment out\n\n# data location\nurl <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab3-iteration/data/biomarker-clean.csv'\n\n# function for outlier trimming\ntrim_fn <- function(x){\n  x[x > 3] <- 3\n  x[x < -3] <- -3\n  \n  return(x)\n}\n\n# read in and preprocess data\nasd <- read_csv(url) %>%\n  select(-ados) %>%\n  # log transform\n  mutate(across(.cols = -group, log10)) %>%\n  # center and scale\n  mutate(across(.cols = -group, ~ scale(.x)[, 1])) %>%\n  # trim outliers\n  mutate(across(.cols = -group, trim_fn))"
  },
  {
    "objectID": "materials/labs/lab3-iteration/lab3-iteration.html#loops",
    "href": "materials/labs/lab3-iteration/lab3-iteration.html#loops",
    "title": "Iteration strategies",
    "section": "Loops",
    "text": "Loops\n\nSimple examples\nA loop is a set of instructions to be repeated a specified number of times while incrementing a flag or index value. For example:\n\nfor(i in 1:4){\n  print(2*i)\n}\n\n[1] 2\n[1] 4\n[1] 6\n[1] 8\n\n\nHere the instructions are:\n\ninitialize index/flag i at i = 1\nexecute code within the braces {...}\nincrement i <- i + 1\nrepeat steps 2-3 until i = 5\n\nWe could make the loop a bit more verbose:\n\nflag_vals <- c(1, 2, 3, 4)\nfor(i in flag_vals){\n  out <- 2*i\n  print(out)\n}\n\n[1] 2\n[1] 4\n[1] 6\n[1] 8\n\n\nNow to retain the results in memory, a storage data structure must be defined and the output of each iteration assigned to some element(s) of the storage object.\n\nrslt <- rep(NA, 4)\nfor(i in 1:4){\n  rslt[i] <- 2*i\n}\nrslt\n\n[1] 2 4 6 8\n\n\nIf we want to perform the same calculation for all values in a vector, we might do something like this:\n\nrslt <- rep(NA, 4)\ninput_vals <- c(15, 27, 3, 12.6)\nfor(i in 1:4){\n  rslt[i] <- 2*input_vals[i]\n}\nrslt\n\n[1] 30.0 54.0  6.0 25.2\n\n\n\n\n\n\n\n\nCheck your understanding\n\n\n\nWhy does the following loop produce an NA ?\n\nrslt <- rep(NA, 4)\ninput_vals <- rnorm(n = 3)\nfor(i in 1:4){\n  rslt[i] <- 2*input_vals[i]\n}\n\nrslt\n\n[1] -3.3664950 -0.5316422  1.2368258         NA\n\n\n\n\nLoops are substantially similar in any programming language but usually not optimized for performance. Additionally, they are somewhat verbose and hard to read due to explicit use of indexing in the syntax.\n\n\nMultiple testing with loops\nIn base R, the \\(t\\)-test is performed using t.test(...) , which takes as arguments two vectors of observations (one for each group). For instance:\n\nx <- asd %>% filter(group == 'ASD') %>% pull(CHIP)\ny <- asd %>% filter(group == 'TD') %>% pull(CHIP)\nt.test(x, y, var.equal = F)\n\n\n    Welch Two Sample t-test\n\ndata:  x and y\nt = -0.18812, df = 151.74, p-value = 0.851\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.2588194  0.2138173\nsample estimates:\n  mean of x   mean of y \n-0.04922954 -0.02672849 \n\n\nThe output is a list:\n\nt.test(x, y) %>% str()\n\nList of 10\n $ statistic  : Named num -0.188\n  ..- attr(*, \"names\")= chr \"t\"\n $ parameter  : Named num 152\n  ..- attr(*, \"names\")= chr \"df\"\n $ p.value    : num 0.851\n $ conf.int   : num [1:2] -0.259 0.214\n  ..- attr(*, \"conf.level\")= num 0.95\n $ estimate   : Named num [1:2] -0.0492 -0.0267\n  ..- attr(*, \"names\")= chr [1:2] \"mean of x\" \"mean of y\"\n $ null.value : Named num 0\n  ..- attr(*, \"names\")= chr \"difference in means\"\n $ stderr     : num 0.12\n $ alternative: chr \"two.sided\"\n $ method     : chr \"Welch Two Sample t-test\"\n $ data.name  : chr \"x and y\"\n - attr(*, \"class\")= chr \"htest\"\n\n\nSo if we want the p-value:\n\nt.test(x, y, var.equal = F)$p.value\n\n[1] 0.8510352\n\n\nTo calculate \\(p\\)-values for all tests using a loop, we wrap the code we used to perform one \\(t\\)-test in a for loop and add appropriate indexing. For speed, we’ll just compute the first 100 tests:\n\nn_tests <- 100\np_vals <- rep(NA, n_tests)\nfor(i in 1:n_tests){\n  x <- asd %>% filter(group == 'ASD') %>% pull(i + 1)\n  y <- asd %>% filter(group == 'TD') %>% pull(i + 1)\n  p_vals[i] <- t.test(x, y, var.equal = F)$p.value\n}\n\nTo line these up with the proteins they correspond to, it’s necessary to keep track of the indexing carefully. In this case, the indexing corresponds to the order of columns. So we could create a data frame like so:\n\ntibble(protein = colnames(asd)[2:(n_tests + 1)],\n       p = p_vals)\n\n# A tibble: 100 × 2\n   protein        p\n   <chr>      <dbl>\n 1 CHIP     0.851  \n 2 CEBPB    0.0322 \n 3 NSE      0.350  \n 4 PIAS4    0.104  \n 5 IL-10 Ra 0.0232 \n 6 STAT3    0.00183\n 7 IRF1     0.592  \n 8 c-Jun    0.0351 \n 9 Mcl-1    0.999  \n10 OAS1     0.942  \n# … with 90 more rows\n\n\nAlternatively, we could have set up the loop to output this result:\n\nn_tests <- 100\nrslt <- tibble(protein = colnames(asd)[2:(n_tests + 1)],\n               p = NA)\nfor(i in 1:n_tests){\n  x <- asd %>% filter(group == 'ASD') %>% pull(i + 1)\n  y <- asd %>% filter(group == 'TD') %>% pull(i + 1)\n  rslt$p[i] <- t.test(x, y, var.equal = F)$p.value\n}\n\n\n\n\n\n\n\nAction\n\n\n\nFollow the example above to write a loop that stores both the \\(p\\)-values and the estimated differences for the first 50 proteins."
  },
  {
    "objectID": "materials/labs/lab3-iteration/lab3-iteration.html#apply-family",
    "href": "materials/labs/lab3-iteration/lab3-iteration.html#apply-family",
    "title": "Iteration strategies",
    "section": "Apply family",
    "text": "Apply family\n\nSimple examples\nIn R, the apply family of functions allows one to efficiently iterate a function over an index set. So, to execute our simple for loop using apply , we could do something like this:\n\nvals <- rnorm(n = 4)\nsimple_fn <- function(x){2*x}\nlapply(vals, simple_fn)\n\n[[1]]\n[1] 1.062206\n\n[[2]]\n[1] 1.619845\n\n[[3]]\n[1] 5.018638\n\n[[4]]\n[1] 0.800267\n\n\nThis applies simple_fn to each element of vals , and returns the result as a list. If we want a neater output format, we could use sapply , which is short for sort-apply:\n\nsapply(vals, simple_fn)\n\n[1] 1.062206 1.619845 5.018638 0.800267\n\n\nIn more complex settings it often makes sense to apply a function across an index set. This is very similar conceptually to a for loop, but faster and easier to read.\n\n# apply a function to an index set\nsimple_fn_ix <- function(i){2*vals[i]}\nrslt_apply <- sapply(1:length(vals), simple_fn_ix)\n\n# equivalent for loop\nrslt_loop <- rep(NA, length(vals))\nfor(i in 1:length(vals)){\n  rslt_loop[i] <- 2*vals[i]\n}\n\n# compare\nrbind(rslt_loop, rslt_apply)\n\n               [,1]     [,2]     [,3]     [,4]\nrslt_loop  1.062206 1.619845 5.018638 0.800267\nrslt_apply 1.062206 1.619845 5.018638 0.800267\n\n\n\n\n\\(t\\)-tests using apply\nWe can use apply functions to compute \\(t\\)-tests for the proteins in the ASD data by coercing the data to a list of data frames that contain the grouping and level for each protein.\n\n# number of tests to perform\nn_tests <- 100\n\n# convert to a list\nasd_list <- asd %>% \n  select(1:(n_tests + 1)) %>%\n  pivot_longer(cols = -group,\n               names_to = 'protein',\n               values_to = 'level') %>%\n  group_by(protein) %>%\n  group_split()\n\n# first entry in list\nasd_list[[1]]\n\n# A tibble: 154 × 3\n   group protein                     level\n   <chr> <chr>                       <dbl>\n 1 ASD   14-3-3 protein beta/alpha -0.124 \n 2 ASD   14-3-3 protein beta/alpha  0.487 \n 3 ASD   14-3-3 protein beta/alpha -0.801 \n 4 ASD   14-3-3 protein beta/alpha  2.73  \n 5 ASD   14-3-3 protein beta/alpha  1.24  \n 6 ASD   14-3-3 protein beta/alpha  0.250 \n 7 ASD   14-3-3 protein beta/alpha  0.932 \n 8 ASD   14-3-3 protein beta/alpha  0.0873\n 9 ASD   14-3-3 protein beta/alpha  0.213 \n10 ASD   14-3-3 protein beta/alpha  0.157 \n# … with 144 more rows\n\n\nThe function t.test(...) can also perform the test using a formula of the form y ~ x and a data frame containing x and y, as below.\n\nt.test(level ~ group, data = asd_list[[1]])\n\n\n    Welch Two Sample t-test\n\ndata:  level by group\nt = -1.5671, df = 150.2, p-value = 0.1192\nalternative hypothesis: true difference in means between group ASD and group TD is not equal to 0\n95 percent confidence interval:\n -0.54341111  0.06269287\nsample estimates:\nmean in group ASD  mean in group TD \n       -0.1341683         0.1061909 \n\n\nIf we just want the \\(p\\)-value again, we can wrap this code in a function whose argument is the index \\(i\\). This function will return the \\(p\\)-value for the \\(i\\)th protein.\n\n# p value for ith protein\ntt_fn <- function(i){\n  t.test(level ~ group, data = asd_list[[i]])$p.value\n}\n\n# check\ntt_fn(1)\n\n[1] 0.1191888\n\n\nNow to perform many tests, we can simply iterate this function over consecutive index values 1:n_tests:\n\nsapply(1:n_tests, tt_fn)\n\n  [1] 1.191888e-01 2.972829e-01 8.297144e-01 3.034583e-02 8.136635e-01\n  [6] 9.517828e-01 3.553359e-01 2.671529e-03 6.458878e-01 3.915314e-04\n [11] 1.759142e-01 3.505666e-02 5.095419e-01 4.788545e-07 2.862286e-01\n [16] 5.714296e-01 7.052780e-03 3.220583e-02 8.510352e-01 1.267133e-03\n [21] 9.482110e-03 1.293157e-04 7.804081e-03 2.208460e-04 1.407044e-01\n [26] 1.023033e-01 8.995855e-02 1.578665e-02 3.113212e-04 2.920587e-02\n [31] 4.663516e-07 2.395764e-01 5.709433e-03 8.962287e-02 2.700053e-02\n [36] 5.357313e-01 7.392658e-01 8.665332e-01 3.538260e-02 1.956257e-04\n [41] 8.658766e-01 2.111378e-04 3.286108e-01 5.374305e-01 1.108687e-01\n [46] 1.711403e-01 4.808293e-01 6.775472e-01 3.556564e-03 2.322968e-02\n [51] 3.746226e-01 7.804488e-01 3.175372e-01 4.085249e-01 4.746117e-03\n [56] 5.917788e-01 3.748021e-02 6.433125e-01 1.121721e-03 3.234610e-03\n [61] 4.154758e-03 1.669733e-03 1.726578e-03 9.990017e-01 7.100178e-04\n [66] 4.811425e-01 8.978465e-01 3.503310e-01 9.423978e-01 3.925728e-01\n [71] 3.025965e-01 4.511875e-02 4.219360e-01 2.117196e-01 1.036412e-01\n [76] 5.590746e-01 8.148983e-01 1.399029e-02 5.096269e-04 9.145121e-02\n [81] 3.331394e-02 4.350959e-01 8.721647e-01 3.266951e-03 2.704495e-01\n [86] 4.929196e-01 1.010954e-03 3.144033e-01 7.933758e-04 1.929813e-03\n [91] 6.104791e-02 1.832399e-03 1.333513e-02 6.412884e-01 2.605232e-02\n [96] 4.130732e-01 2.579393e-01 4.096623e-01 2.925137e-01 5.866341e-01\n\n\nYou might have noticed this was much faster than the loop. We can time it:\n\nstart <- Sys.time()\nrslt <- sapply(1:n_tests, tt_fn)\nend <- Sys.time()\n\nend - start\n\nTime difference of 0.209661 secs\n\n\nAnd compare with the for loop:\n\nstart <- Sys.time()\nn_tests <- 100\nrslt <- tibble(protein = colnames(asd)[2:(n_tests + 1)],\n               p = NA)\nfor(i in 1:n_tests){\n  x <- asd %>% filter(group == 'ASD') %>% pull(i + 1)\n  y <- asd %>% filter(group == 'TD') %>% pull(i + 1)\n  rslt$p[i] <- t.test(x, y, var.equal = F)$p.value\n}\nend <- Sys.time()\n\nend - start\n\nTime difference of 11.63856 secs\n\n\nAnother nice feature of sapply is its ability to sort and arrange multiple outputs. For example, if the function is adjusted to return both the \\(p\\)-value and the test statistic:\n\ntt_fn <- function(i){\n  test_rslt <- t.test(level ~ group, data = asd_list[[i]])\n  out <- c(pval = test_rslt$p.value, \n           tstat = test_rslt$statistic)\n  out\n}\n\ntt_fn(1)\n\n      pval    tstat.t \n 0.1191888 -1.5671297 \n\n\nThen sapply will return a matrix:\n\nsapply(1:5, tt_fn) %>% t() %>% as_tibble()\n\n# A tibble: 5 × 2\n    pval tstat.t\n   <dbl>   <dbl>\n1 0.119   -1.57 \n2 0.297   -1.05 \n3 0.830   -0.215\n4 0.0303  -2.19 \n5 0.814    0.236\n\n\n\n\n\n\n\n\nAction\n\n\n\n\nUse sapply to obtain the estimated differences and standard errors for the groupwise comparisons for the first 50 proteins.\nArrange the result in a data frame with a column indicating the protein, a column indicating the estimated group difference, and a column indicating the standard error."
  },
  {
    "objectID": "materials/labs/lab3-iteration/lab3-iteration.html#tidyverse",
    "href": "materials/labs/lab3-iteration/lab3-iteration.html#tidyverse",
    "title": "Iteration strategies",
    "section": "Tidyverse",
    "text": "Tidyverse\nA final strategy for iteration comes from functional programming tools in tidyverse . The basic idea is:\n\ndefine a grouping structure using relevant variables (in this case, proteins)\ncollapse the data into separate data frames by group\napply a function to each data frame that produces test output given input data\n\n\nNesting\nOne thing that tibbles can do that data frames cannot is store list-columns: columns that are lists of arbitrary objects. This allows for the arrangement of a much more general collection of objects in tabular form.\nAn intuitive example is nested data: a list-column of data frames having the same columns. If we nest the ASD data by protein, we obtain a data frame that looks like this:\n\nasd_nested <- asd %>%\n  pivot_longer(-group, \n               names_to = 'protein', \n               values_to = 'level') %>%\n  nest(data = c(level, group))\n\nasd_nested %>% head(5)\n\n# A tibble: 5 × 2\n  protein  data              \n  <chr>    <list>            \n1 CHIP     <tibble [154 × 2]>\n2 CEBPB    <tibble [154 × 2]>\n3 NSE      <tibble [154 × 2]>\n4 PIAS4    <tibble [154 × 2]>\n5 IL-10 Ra <tibble [154 × 2]>\n\n\nThe data column consists of data frames, one per protein, containing the variables group and level :\n\nasd_nested %>%\n  slice(1L) %>%\n  pull(data)\n\n[[1]]\n# A tibble: 154 × 2\n     level group\n     <dbl> <chr>\n 1  0.335  ASD  \n 2 -0.0715 ASD  \n 3 -0.406  ASD  \n 4 -0.102  ASD  \n 5 -0.395  ASD  \n 6 -0.126  ASD  \n 7  0.486  ASD  \n 8 -0.990  ASD  \n 9 -0.108  ASD  \n10  0.485  ASD  \n# … with 144 more rows\n\n\n\n\nThe map() function\nIn an ordinary data frame one can define a new variable as a function of other variables. The same can be done with list-columns in a tibble: one can define a new variable as a function of the elements of a list stored in another column. To do this, one uses the map() function, which is essentially the tidyverse version of lapply() :\n\nmap(.x, .fn) means roughly “apply the function .fn to each element in .x”\n\nHere we can write a function that takes a data frame with protein level and group as input, and returns a t test as output; then computing each test is as simple as calling mutate :\n\ntt_fn <- function(.df){\n  t.test(level ~ group, data = .df)\n}\n\nrslt <- asd_nested %>%\n  slice(1:10) %>%\n  mutate(ttest.out = map(data, tt_fn))\n\nrslt\n\n# A tibble: 10 × 3\n   protein  data               ttest.out\n   <chr>    <list>             <list>   \n 1 CHIP     <tibble [154 × 2]> <htest>  \n 2 CEBPB    <tibble [154 × 2]> <htest>  \n 3 NSE      <tibble [154 × 2]> <htest>  \n 4 PIAS4    <tibble [154 × 2]> <htest>  \n 5 IL-10 Ra <tibble [154 × 2]> <htest>  \n 6 STAT3    <tibble [154 × 2]> <htest>  \n 7 IRF1     <tibble [154 × 2]> <htest>  \n 8 c-Jun    <tibble [154 × 2]> <htest>  \n 9 Mcl-1    <tibble [154 × 2]> <htest>  \n10 OAS1     <tibble [154 × 2]> <htest>  \n\nrslt %>% slice(1L) %>% pull(ttest.out)\n\n[[1]]\n\n    Welch Two Sample t-test\n\ndata:  level by group\nt = -0.18812, df = 151.74, p-value = 0.851\nalternative hypothesis: true difference in means between group ASD and group TD is not equal to 0\n95 percent confidence interval:\n -0.2588194  0.2138173\nsample estimates:\nmean in group ASD  mean in group TD \n      -0.04922954       -0.02672849 \n\n\nWhile all the data we might want are there, the output is a little unwieldy. Luckily, the infer package contains a pipe-operator-friendly function infer::t_test that returns results in a tidy fashion.\n\nasd_nested %>% \n  slice(1L) %>% \n  unnest(cols = data) %>% \n  infer::t_test(formula = level ~ group,\n         order = c('ASD', 'TD'),\n         alternative = 'two-sided',\n         var.equal = F)\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      <dbl> <dbl>   <dbl> <chr>          <dbl>    <dbl>    <dbl>\n1    -0.188  152.   0.851 two.sided    -0.0225   -0.259    0.214\n\n\nWe can create a wrapper around this function suitable for our purposes and then apply it to the list-column in asd_nested :\n\n# wrapper around infer::t_test\ntt_fn <- function(.df){\n  infer::t_test(.df, \n         formula = level ~ group,\n         order = c('ASD', 'TD'),\n         alternative = 'two-sided',\n         var.equal = F)\n}\n\n# compute test results\ntt_out <- asd_nested %>%\n  slice(1:n_tests) %>%\n  mutate(ttest = map(data, tt_fn))\n\n# preview\ntt_out %>% head(4)\n\n# A tibble: 4 × 3\n  protein data               ttest           \n  <chr>   <list>             <list>          \n1 CHIP    <tibble [154 × 2]> <tibble [1 × 7]>\n2 CEBPB   <tibble [154 × 2]> <tibble [1 × 7]>\n3 NSE     <tibble [154 × 2]> <tibble [1 × 7]>\n4 PIAS4   <tibble [154 × 2]> <tibble [1 × 7]>\n\n\nNotice that ttest is also a list-column comprised of separate data frames. This column can be un-nested to show the output of infer::t_test explicitly:\n\ntt_out %>% \n  unnest(ttest) %>%\n  head(4)\n\n# A tibble: 4 × 9\n  protein data     statistic  t_df p_value alternative estimate lower_ci\n  <chr>   <list>       <dbl> <dbl>   <dbl> <chr>          <dbl>    <dbl>\n1 CHIP    <tibble>    -0.188  152.  0.851  two.sided    -0.0225  -0.259 \n2 CEBPB   <tibble>     2.16   150.  0.0322 two.sided     0.317    0.0273\n3 NSE     <tibble>     0.937  151.  0.350  two.sided     0.148   -0.164 \n4 PIAS4   <tibble>     1.64   152.  0.104  two.sided     0.222   -0.0459\n# … with 1 more variable: upper_ci <dbl>\n\n\nThis approach has a few advantages, namely, it is syntactically more readable than either of the other approaches and it works with the pipe operator, so could in theory be incorporated into a chain that performs additional calculations on, say, the results of the \\(t\\)-test. However, the drawback is that it is slow:\n\n# time it\nstart <- Sys.time()\ntt_out <- asd_nested %>%\n  slice(1:n_tests) %>%\n  mutate(ttest = map(data, tt_fn))\nend <- Sys.time()\n\nend - start\n\nTime difference of 4.896297 secs\n\n\nIt’s not as slow as a for loop, but it’s much slower than apply functions. If speed is a concern or the number of iterations is especially large, apply would be a better choice.\n\n\nAdjusting p-values\nTo adjust the \\(p\\)-values, we simply manipulate the p_value column:\n\n# bonferroni correction\ntt_out %>% \n  unnest(ttest) %>%\n  mutate(p_adj = p_value*n_tests) %>%\n  select(protein, p_value, p_adj) %>%\n  arrange(p_adj) %>%\n  head(4)\n\n# A tibble: 4 × 3\n  protein     p_value     p_adj\n  <chr>         <dbl>     <dbl>\n1 FSTL1   0.000000466 0.0000466\n2 C1QR1   0.000000479 0.0000479\n3 DSC2    0.000129    0.0129   \n4 HIF-1a  0.000196    0.0196   \n\n\n\n\n\n\n\n\nAction\n\n\n\nImplement the Benjamini-Hochberg correction\n\nSort the raw \\(p\\)-values using arrange()\nAdd a rank column of consecutive integers (neat trick: try using row_number())\nAdd a column p_adj containing \\(\\frac{m}{i} p_{(i)}\\) where \\(m\\) is the number of tests, \\(i\\) is the rank of the \\(p\\)-value, and \\(p_{(i)}\\) is the \\(i\\)th smallest \\(p\\)-value\nFind the collection of proteins with significantly different serum levels between the ASD and TD groups while controlling the false discovery rate at 1%.\n\nDevelop working code to execute 50 tests. Then use it to compute all 1317 tests."
  },
  {
    "objectID": "materials/labs/lab3-iteration/lab3-iteration.html#checklist",
    "href": "materials/labs/lab3-iteration/lab3-iteration.html#checklist",
    "title": "Iteration strategies",
    "section": "Checklist",
    "text": "Checklist\n\nYou’ve computed \\(p\\)-values iteratively using a loop, sapply , and nest+map .\nYou have commented codes in your script for each action item.\nYou’ve obtained a list of the significant proteins with 1% FDR.\nYou’ve saved your work somewhere where you can access it later."
  },
  {
    "objectID": "materials/labs/lab4-logistic/lab4-logistic.html",
    "href": "materials/labs/lab4-logistic/lab4-logistic.html",
    "title": "Measuring classification accuracy",
    "section": "",
    "text": "In class you saw how to fit a logistic regression model using glm() and some basic classification accuracy measures.\nObjectives\nIn this lab you’ll carry out a more rigorous quantification of predictive accuracy by data partitioning. You’ll learn to use:\nPrerequisites\nFollow the action item below to get set up for the lab. You may need to install one or more packages if the library() calls return an error."
  },
  {
    "objectID": "materials/labs/lab4-logistic/lab4-logistic.html#data-partitioning",
    "href": "materials/labs/lab4-logistic/lab4-logistic.html#data-partitioning",
    "title": "Measuring classification accuracy",
    "section": "Data partitioning",
    "text": "Data partitioning\nIn class we fit a logistic regression model to the data and evaluated classification accuracy on the very same data.\nBecause the parameter estimates optimize errors on the data used to fit the model, evaluating accuracy on the same data gives an overly optimistic assessment, because the errors have been made as small as possible.\nData partitioning consists in setting aside a random subset of observations that will be used only to assess predictive accuracy and not to fit any models. The held out data is treated as a ‘test’ set of observations to try to predict. This provides a more realistic assessment of predictive accuracy that is closer to what can be expected if genuinely new data is collected.\nPartitioning is easy to do using rsample::initial_split and specifying the proportion of data that should be retained for model fitting (the ‘training’ set).\nPartitions are computed at random, so for reproducibility it is necessary to set the RNG seed at a fixed value.\n\n\n\n\n\n\nAction\n\n\n\nPartition the biomarker data into training and test sets\nCopy-paste the code chunk below into your script and execute once.\nRemarks:\n\nset.seed() needs to be executed together with the lines that follow to ensure the same result is rendered every time\nthe output simply summarizes the partitions\n\n\n# for reproducibility\nset.seed(102022)\n\n# partition data\npartitions <- biomarker %>%\n  initial_split(prop = 0.8)\n\n# examine\npartitions\n\n<Training/Testing/Total>\n<123/31/154>\n\n\n\n\nTo retrieve the data partitions, one needs to use the helper functions training() and testing() :\n\n# training set\ntraining(partitions) %>% head(4)\n\n# A tibble: 4 × 6\n    DERM   RELT    IgD     PTN  FSTL1 class\n   <dbl>  <dbl>  <dbl>   <dbl>  <dbl> <lgl>\n1 -0.409  0.108  1.82  -0.457  -1.31  TRUE \n2  1.25   0.802 -0.461  0.910   0.973 FALSE\n3 -0.697 -0.921 -1.26   0.0444 -1.46  TRUE \n4 -0.591  1.34   2.31   0.269   0.802 TRUE \n\n# testing set\ntesting(partitions) %>% head(4)\n\n# A tibble: 4 × 6\n      DERM   RELT    IgD     PTN  FSTL1 class\n     <dbl>  <dbl>  <dbl>   <dbl>  <dbl> <lgl>\n1  0.140   -0.586 -1.53  -1.72   -1.96  TRUE \n2 -0.276    0.410  0.454 -0.0375 -1.41  TRUE \n3  0.927   -1.12   1.15  -0.290   0.190 TRUE \n4 -0.00743 -0.319 -0.762 -1.27   -1.05  TRUE"
  },
  {
    "objectID": "materials/labs/lab4-logistic/lab4-logistic.html#model-fitting",
    "href": "materials/labs/lab4-logistic/lab4-logistic.html#model-fitting",
    "title": "Measuring classification accuracy",
    "section": "Model fitting",
    "text": "Model fitting\nFitting a logistic regression model is, as you saw in class, a one-line affair:\n\n# fit glm\nfit <- glm(class ~ ., \n           data = biomarker, \n           family = binomial(link = \"logit\"))\n\nThe glm() function can fit many kinds of generalized linear models. Logistic regression is just one of this class of models in which:\n\nthe response follows a binomial distribution (the Bernoulli distribution is the binomial with \\(n = 1\\))\nthe log-odds or logit transformation of the event/class probability \\(p_i\\) is linear in the predictors\n\nThe parameter estimates reported in tabular form are:\n\ntidy(fit)\n\n# A tibble: 6 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)  -0.0933     0.199    -0.468 0.640  \n2 DERM         -0.603      0.280    -2.16  0.0311 \n3 RELT         -0.438      0.286    -1.53  0.126  \n4 IgD          -0.662      0.213    -3.11  0.00189\n5 PTN          -0.234      0.273    -0.857 0.392  \n6 FSTL1        -0.360      0.259    -1.39  0.165  \n\n\n\n\n\n\n\n\nAction\n\n\n\nFit the model and interpret a parameter\n\nCopy-paste the code above in your script and execute to fit the logistic regression model.\nConfer with your neighbor and interpret one of the parameters of your choosing.\n\nBy interpret, we mean: what is the estimated change in P(ASD) associated with a +1SD change in log protein level?"
  },
  {
    "objectID": "materials/labs/lab4-logistic/lab4-logistic.html#predictions",
    "href": "materials/labs/lab4-logistic/lab4-logistic.html#predictions",
    "title": "Measuring classification accuracy",
    "section": "Predictions",
    "text": "Predictions\nThe modelr package makes it relatively easy to compute predictions for a wide range of model objects in R. Its pipe-friendly add_predictions(.df, .mdl, type) function will add a column of predictions of type type using model .mdl to data frame .df .\n\n# compute predictions on the test set\ntesting(partitions) %>%\n  add_predictions(fit)\n\n# A tibble: 31 × 7\n       DERM    RELT    IgD     PTN  FSTL1 class    pred\n      <dbl>   <dbl>  <dbl>   <dbl>  <dbl> <lgl>   <dbl>\n 1  0.140   -0.586  -1.53  -1.72   -1.96  TRUE   2.20  \n 2 -0.276    0.410   0.454 -0.0375 -1.41  TRUE   0.108 \n 3  0.927   -1.12    1.15  -0.290   0.190 TRUE  -0.925 \n 4 -0.00743 -0.319  -0.762 -1.27   -1.05  TRUE   1.23  \n 5 -0.484   -0.612  -0.625 -0.990  -1.16  TRUE   1.53  \n 6  0.111   -0.489  -1.15   0.0483 -1.12  TRUE   1.21  \n 7 -0.972   -0.535   0.502 -0.594  -0.427 TRUE   0.687 \n 8 -0.344    0.0337  0.998 -2.00   -0.309 TRUE   0.0179\n 9 -1.13    -0.385  -0.929  0.892  -0.597 TRUE   1.38  \n10  0.398    0.325  -1.17  -0.0114 -0.415 TRUE   0.450 \n# … with 21 more rows\n\n\nInspect the pred column. Notice that the predictions are not classes or probabilities. The default type of predictions are log-odds. One could back-transform:\n\n# manually transform to probabilities\ntesting(partitions) %>%\n  add_predictions(fit) %>%\n  mutate(probs = 1/(1 + exp(-pred))) %>%\n  select(class, pred, probs) %>%\n  head(5)\n\n# A tibble: 5 × 3\n  class   pred probs\n  <lgl>  <dbl> <dbl>\n1 TRUE   2.20  0.900\n2 TRUE   0.108 0.527\n3 TRUE  -0.925 0.284\n4 TRUE   1.23  0.774\n5 TRUE   1.53  0.822\n\n\nOr simply change the type of predictions to response in order to obtain predicted probabilities:\n\n# predict on scale of response\ntesting(partitions) %>%\n  add_predictions(fit, type = 'response') %>%\n  select(class, pred) %>%\n  head(5)\n\n# A tibble: 5 × 2\n  class  pred\n  <lgl> <dbl>\n1 TRUE  0.900\n2 TRUE  0.527\n3 TRUE  0.284\n4 TRUE  0.774\n5 TRUE  0.822\n\n\nIf we want to convert these predicted class probabilities into predicted classes, we can simply define a new variable based on whether the probabilities exceed 0.5 (or any other threshold):\n\n# predict classes\ntesting(partitions) %>%\n  add_predictions(fit, type = 'response') %>%\n  mutate(pred.class = (pred > 0.5)) %>%\n  select(class, pred, pred.class) %>%\n  head(5)\n\n# A tibble: 5 × 3\n  class  pred pred.class\n  <lgl> <dbl> <lgl>     \n1 TRUE  0.900 TRUE      \n2 TRUE  0.527 TRUE      \n3 TRUE  0.284 FALSE     \n4 TRUE  0.774 TRUE      \n5 TRUE  0.822 TRUE"
  },
  {
    "objectID": "materials/labs/lab4-logistic/lab4-logistic.html#accuracy-measures",
    "href": "materials/labs/lab4-logistic/lab4-logistic.html#accuracy-measures",
    "title": "Measuring classification accuracy",
    "section": "Accuracy measures",
    "text": "Accuracy measures\nThe classification accuracy measures we discussed in class are based on tabulating observation counts when grouping by predicted and observed classes.\nThis tabulation can be done in a base-R way by piping a data frame of the predicted and observed classes into table() :\n\n# tabulate\ntesting(partitions) %>%\n  add_predictions(fit, type = 'response') %>%\n  mutate(pred.class = (pred > 0.5)) %>%\n  select(class, pred.class) %>%\n  table()\n\n       pred.class\nclass   FALSE TRUE\n  FALSE    10    6\n  TRUE      3   12\n\n\nHowever, the metrics discussed in class are somewhat painful to compute from the output above. Luckily, yardstick makes that process easier: the package has specialized functions that compute each metric. One need only:\n\nprovide the predicted and true labels as factors\nindicate which factor is the truth and which is the prediction\nindicate which factor level is considered a ‘positive’\n\n\n# store predictions as factors\npred_df <- testing(partitions) %>%\n  add_predictions(fit, type = 'response') %>%\n  mutate(pred.class = (pred > 0.5),\n         group = factor(class, labels = c('TD', 'ASD')),\n         pred.group = factor(pred.class, labels = c('TD', 'ASD'))) \n\n# check order of factor levels\npred_df %>% pull(group) %>% levels()\n\n[1] \"TD\"  \"ASD\"\n\n# compute specificity\npred_df %>%\n  specificity(truth = group, \n              estimate = pred.group,\n              event_level = 'second')\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 specificity binary         0.625\n\n\nThe second level is ASD, which in this context is a positive. We knew since we supplied the labels in defining the factors, and the order of levels will match the order of labels. However, we can also check as above using levels() . Hence, event_level = 'second' .\nSensitivity, accuracy, and other metrics can be computed similarly:\n\n# sensitivity\npred_df %>%\n  sensitivity(truth = group,\n              estimate = pred.group,\n              event_level = 'second')\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 sensitivity binary           0.8\n\n\nYou can check the package documentation for a complete list of available metrics.\n\n\n\n\n\n\nAction\n\n\n\nCompute the accuracy\nFind the appropriate function from the package documentation (link immediately above) and use it to compute the accuracy.\nRemark: from the table, you know the result should be \\(\\frac{10 + 12}{10 + 12 + 6 + 3} \\approx 0.7097\\) .\n\n\nThe package also has a helper function that allows you to define a panel of metrics so that you can compute several simultaneously. If we want a panel of specificity and sensitivity, the following will do the trick:\n\n# define panel (arguments must be yardstick metric function names)\npanel_fn <- metric_set(sensitivity, specificity)\n\n# compute\npred_df %>%\n  panel_fn(truth = group,\n           estimate = pred.group,\n           event_level = 'second')\n\n# A tibble: 2 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 sensitivity binary         0.8  \n2 specificity binary         0.625\n\n\n\n\n\n\n\n\nAction\n\n\n\nCompute a panel of precision, recall, and F1 score\n\nFind the appropriate yardstick functions and define a metric panel\nCompute on the test data\n\n\n\nAs a final comment, the table of classifications can be obtained in yardstick using the conf_mat() function. (The cross-classification table of predicted versus actual classes is called a confusion matrix.)\n\npred_df %>% conf_mat(truth = group, estimate = pred.group)\n\n          Truth\nPrediction TD ASD\n       TD  10   3\n       ASD  6  12"
  },
  {
    "objectID": "materials/labs/lab4-logistic/lab4-logistic.html#checklist",
    "href": "materials/labs/lab4-logistic/lab4-logistic.html#checklist",
    "title": "Measuring classification accuracy",
    "section": "Checklist",
    "text": "Checklist\n\nYou partitioned the data into training and test sets\nYou fit a logistic regression model using the training set\nYou evaluated accuracy on the test set"
  },
  {
    "objectID": "materials/labs/lab4-logistic/lab4-logistic.html#extras",
    "href": "materials/labs/lab4-logistic/lab4-logistic.html#extras",
    "title": "Measuring classification accuracy",
    "section": "Extras",
    "text": "Extras\nIf there is extra time, or you’re interested in exploring a bit further on your own, read on.\n\nROC curves\nThe yardstick package also supplies functions for computing class probability metrics based on the estimated class probabilities rather than the estimated classes. ROC curves and AUROC are examples.\nroc_curve() will find all unique probability thresholds and, for each threshold, calculate sensitivity and specificity:\n\npred_df %>%\n  roc_curve(truth = group, estimate = pred)\n\n# A tibble: 33 × 3\n   .threshold specificity sensitivity\n        <dbl>       <dbl>       <dbl>\n 1  -Inf           0            1    \n 2     0.0664      0            1    \n 3     0.117       0            0.938\n 4     0.158       0            0.875\n 5     0.186       0.0667       0.875\n 6     0.191       0.0667       0.812\n 7     0.197       0.0667       0.75 \n 8     0.262       0.0667       0.688\n 9     0.284       0.0667       0.625\n10     0.299       0.133        0.625\n# … with 23 more rows\n\n\nThis can be used to plot the ROC curve:\n\npred_df %>%\n  roc_curve(truth = group, \n            estimate = pred,\n            event_level = 'second') %>%\n  ggplot(aes(y = sensitivity, x = 1 - specificity)) +\n  geom_path() +\n  geom_abline(slope = 1, intercept = 0)\n\n\n\n\nIn this case the ROC curve is so choppy because the test set only includes 31 observations. In general, for a collection of \\(n\\) observations there are at most \\(n + 1\\) unique thresholds and usually considerably fewer.\nThe area under the ROC curve is also easy to compute:\n\npred_df %>% roc_auc(truth = group, \n                    estimate = pred,\n                    event_level = 'second')\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.712\n\n\n\n\nCombined metric types\nIf you wish to compute both classification metrics based on a class prediction and class probability metrics based on a probability prediction in a metric panel, the classification should be provided as the argument to estimate = ... and class probability column can be provided as an unnamed argument following the estimate argument.\nFor example:\n\npanel <- metric_set(roc_auc, accuracy) \n\npred_df %>% panel(truth = group,\n                  estimate = pred.group,\n                  pred,\n                  event_level = 'second')\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.710\n2 roc_auc  binary         0.712\n\n\n\n\nExploring partitions\nReview this section if you want a deeper understanding of data partitioning.\nThe rationale for partitioning is that held out data will give a more honest assessment. Conversely, evaluating accuracy on data used to fit a model will provide an overly optimistic assessment.\nWe can experimentally confirm this intuition by:\n\nrepeatedly partitioning the data at random\nevaluating accuracy on both partitions\naveraging across partitions\n\nThis procedure will reveal that on average the accuracy is better on training data. Don’t worry too much about the computations; focus on the output and the concepts.\nWe’ll use the tidyverse iteration strategy from lab 3. First we’ll need some helper functions that are basically wrappers around each step we went through in this lab:\n\nfitting a model\nadding predictions\nevaluating metrics\n\n\n# define some helper functions\nfit_fn <- function(.df){\n  glm(class ~ ., family = 'binomial', data = .df)\n}\n\npred_fn <- function(.df, .mdl){\n  .df %>% add_predictions(.mdl, type = 'response')\n}\n\npanel <- metric_set(sensitivity, specificity, accuracy, roc_auc)\n\neval_fn <- function(.df){\n  .df %>%\n    mutate(group = factor(class, labels = c('TD', 'ASD')),\n           pred.group = factor(pred > 0.5, labels = c('TD', 'ASD'))) %>%\n    panel(truth = group,\n          estimate = pred.group,\n          pred,\n          event_level = 'second')\n  \n}\n\nNow let’s create 400 random partitions of the data and perform the steps in this lab for every partition. In addition, we’ll compute predictions and evaluate accuracy on the training set, which we did not do above and is generally not done.\n\nset.seed(101922)\nn_splits <- 400\nout <- tibble(partition = 1:n_splits,\n       split = map(partition, ~ initial_split(biomarker, prop = 0.8)),\n       train = map(split, training),\n       test = map(split, testing),\n       fit = map(train, fit_fn),\n       pred_test = map2(test, fit, pred_fn),\n       pred_train = map2(train, fit, pred_fn),\n       eval_test = map(pred_test, eval_fn),\n       eval_train = map(pred_train, eval_fn))\n\nout %>% head(4)\n\n# A tibble: 4 × 9\n  partition split            train    test     fit    pred_test pred_train\n      <int> <list>           <list>   <list>   <list> <list>    <list>    \n1         1 <split [123/31]> <tibble> <tibble> <glm>  <tibble>  <tibble>  \n2         2 <split [123/31]> <tibble> <tibble> <glm>  <tibble>  <tibble>  \n3         3 <split [123/31]> <tibble> <tibble> <glm>  <tibble>  <tibble>  \n4         4 <split [123/31]> <tibble> <tibble> <glm>  <tibble>  <tibble>  \n# … with 2 more variables: eval_test <list>, eval_train <list>\n\n\nWe can extract the accuracy of predictions on each of the training and test sets as follows:\n\ntest_accuracy <- out %>% \n  select(partition, contains('eval')) %>%\n  unnest(eval_test) %>%\n  select(partition, .metric, .estimate) %>%\n  pivot_wider(names_from = .metric, values_from = .estimate)\n\ntrain_accuracy <- out %>% \n  select(partition, contains('eval')) %>%\n  unnest(eval_train) %>%\n  select(partition, .metric, .estimate) %>%\n  pivot_wider(names_from = .metric, values_from = .estimate)\n\ntest_accuracy %>% head(4)\n\n# A tibble: 4 × 5\n  partition sensitivity specificity accuracy roc_auc\n      <int>       <dbl>       <dbl>    <dbl>   <dbl>\n1         1       0.6         0.562    0.581   0.675\n2         2       0.778       0.538    0.677   0.645\n3         3       0.684       0.917    0.774   0.846\n4         4       0.684       0.833    0.742   0.842\n\ntrain_accuracy %>% head(4)\n\n# A tibble: 4 × 5\n  partition sensitivity specificity accuracy roc_auc\n      <int>       <dbl>       <dbl>    <dbl>   <dbl>\n1         1       0.803       0.806    0.805   0.858\n2         2       0.741       0.831    0.789   0.858\n3         3       0.737       0.742    0.740   0.825\n4         4       0.719       0.773    0.748   0.825\n\n\nLastly, we can average the metrics over all partitions and also check the variability across partitions. We’ll start with the training set:\n\ntrain_summaries <- train_accuracy %>%\n  rename(roc.auc = roc_auc) %>%\n  select(-partition) %>%\n  summarize_all(.funs = list(mean = mean, sd = sd)) %>%\n  gather() %>%\n  separate(key, into = c('metric', 'stat'), sep = '_') %>%\n  spread(stat, value)\n\nNow compute the average and variability on the test set:\n\ntest_summaries <- test_accuracy %>%\n  rename(roc.auc = roc_auc) %>%\n  select(-partition) %>%\n  summarize_all(.funs = list(mean = mean, sd = sd)) %>%\n  gather() %>%\n  separate(key, into = c('metric', 'stat'), sep = '_') %>%\n  spread(stat, value)\n\nNow let’s put them side-by-side:\n\nleft_join(train_summaries, \n          test_summaries,\n          by = 'metric',\n          suffix = c('.train', '.test')) %>%\n  select(metric, contains('mean'), contains('sd')) %>%\n  knitr::kable()\n\n\n\n\nmetric\nmean.train\nmean.test\nsd.train\nsd.test\n\n\n\n\naccuracy\n0.7573374\n0.7225806\n0.0207045\n0.0702816\n\n\nroc.auc\n0.8348238\n0.7994956\n0.0158188\n0.0706822\n\n\nsensitivity\n0.7565383\n0.7093282\n0.0308425\n0.1073645\n\n\nspecificity\n0.7569019\n0.7416470\n0.0282898\n0.1148716\n\n\n\n\n\nNotice that:\n\nThe apparent accuracy according to all metrics is higher on average on the training data across partitionings\nThe accuracy metrics on the test data are more variable across partitionings\n\nThis behavior is the justification for data partitioning: evaluating predictions on the same data that was used to fit the model overestimates the accuracy compared with data that was not used in fitting."
  },
  {
    "objectID": "materials/labs/lab5-text/lab5-text.html",
    "href": "materials/labs/lab5-text/lab5-text.html",
    "title": "Text processing",
    "section": "",
    "text": "In this lab you’ll learn some basic text processing following what was presented in class and do a little exploratory analysis using token frequency measures.\nObjectives\nThe text we’ll work with comprises four Dr. Seuss books. The raw data are read in line-by-line, so that seuss_lines is a vector in which each element is a line from one of the four books. Lines are rendered in order."
  },
  {
    "objectID": "materials/labs/lab5-text/lab5-text.html#text-preprocessing",
    "href": "materials/labs/lab5-text/lab5-text.html#text-preprocessing",
    "title": "Text processing",
    "section": "Text preprocessing",
    "text": "Text preprocessing\nFor us, ‘preprocessing’ operations will refer to coercing a document into one long uniformly-formatted string.\n\nDistinguishing documents\nTo start, we have all four books lumped together. A quick visual scan of the text file will confirm that each book is set off by the title on one line followed by ‘By Dr. Seuss’ on the next line.\nWe can leverage this structure to distinguish the books: the chunk below\n\ncreates a ‘flag’ by pattern-matching each line with Dr. Seuss,\nthen shifts the lines down by one so that the flag matches the title line instead of the author line\nthen assigns a document ID to each line by computing the total number of flags in all preceding lines.\n\nThe last two commands correct for having ‘lagged’ the lines.\n\n# flag lines with a document id\nseuss_lines_df <- tibble(line_lag = c(seuss_lines, NA)) %>%\n  mutate(flag = str_detect(line_lag, 'Dr. Seuss'),\n         line = lag(line_lag, n = 1),\n         doc = cumsum(flag)) %>% \n  select(doc, line) %>%\n  slice(-1) %>%\n  fill(doc)\n\nWe may as well assign labels to the document IDs.\n\n# grab titles\ntitles <- seuss_lines_df %>% \n  group_by(doc) %>%\n  slice_head() %>%\n  pull(line) %>%\n  tolower()\n\n# label docs\nseuss_lines_df <- seuss_lines_df %>%\n  mutate(doc = factor(doc, labels = titles))\n\nFinally, we’ll strip the title and author information, because all books are by the same author and the title is now recorded in the document ID.\nThe chunk below adds a document-specific line number and removes the first two lines from every document. Since each row is a line, this amounts to a simple row numbering and filtering.\n\n# remove header lines (title/author)\nseuss_lines_clean <- seuss_lines_df %>%\n  group_by(doc) %>%\n  mutate(line_num = row_number() - 2) %>%\n  filter(line_num > 0)\n\n\n\n\n\n\n\nAction\n\n\n\nLine summaries\nSee if you can answer the following questions:\n\nHow many lines are in each book?\nHow many lines in each book contain the word ‘bump’?\n\nWork with a neighbor. Hint: you might find it handy to use str_detect() , and grouped operations and/or summaries.\n\n\n\n\nCollapsing lines and cleaning text\nFirst, concatenate all the lines using str_c() .\n\n# collapse lines into one long string\nseuss_text <- seuss_lines_clean %>% \n  summarize(text = str_c(line, collapse = ' '))\n\nIn this case the resulting text strings for each document don’t contain too many elements in need of removal: just punctuation and capital letters.\n\ncat_in_hat <- seuss_text %>% slice(1) %>% pull(text)\n\nTo strip these elements, we can exclude matching patterns from the collection of punctuation marks and then use tolower() to replace upper-case letters with lower-case letters. Shorthand for punctuation in stringr is '[[:punct:]]' .\n\ncat_in_hat %>%\n  str_remove_all('[[:punct:]]') %>%\n  tolower()\n\n[1] \"the sun did not shine it was too wet to play so we sat in the house all that cold cold wet day i sat there with sally we sat there we two and i said how i wish we had something to do too wet to go out and too cold to play ball so we sat in the house we did nothing at all so all we could do was to sit sit sit sit and we did not like it not one little bit bump and then something went bump how that bump made us jump we looked then we saw him step in on the mat we looked and we saw him the cat in the hat and he said to us why do you sit there like that i know it is wet and the sun is not sunny but we can have lots of good fun that is funny i know some good games we could play said the cat i know some new tricks said the cat in the hat a lot of good tricks i will show them to you your mother will not mind at all if i do then sally and i did not know what to say our mother was out of the house for the day but our fish said no no make that cat go away tell that cat in the hat you do not want to play he should not be here he should not be about he should not be here when your mother is out now now have no fear have no fear said the cat my tricks are not bad said the cat in the hat why we can have lots of good fun if you wish with a game that i call upupup with a fish put me down said the fish this is no fun at all put me down said the fish i do not wish to fall have no fear said the cat i will not let you fall i will hold you up high as i stand on a ball with a book on one hand and a cup on my hat but that is not all i can do said the cat look at me look at me now said the cat with a cup and a cake on the top of my hat i can hold up two books i can hold up the fish and a litte toy ship and some milk on a dish and look i can hop up and down on the ball but that is not all oh no that is not all look at me look at me look at me now it is fun to have fun but you have to know how i can hold up the cup and the milk and the cake i can hold up these books and the fish on a rake i can hold the toy ship and a little toy man and look with my tail i can hold a red fan i can fan with the fan as i hop on the ball but that is not all oh no that is not all that is what the cat said then he fell on his head he came down with a bump from up there on the ball and sally and i we saw all the things fall and our fish came down too he fell into a pot he said do i like this oh no i do not this is not a good game said our fish as he lit no i do not like it not one little bit now look what you did said the fish to the cat now look at this house look at this look at that you sank our toy ship sank it deep in the cake you shook up our house and you bent our new rake you should not be here when our mother is not you get out of this house said the fish in the pot but i like to be here oh i like it a lot said the cat in the hat to the fish in the pot i will not go away i do not wish to go and so said the cat in the hat so so so i will show you another good game that i know and then he ran out and then fast as a fox the cat in the hat came back in with a box a big red wood box it was shut with a hook now look at this trick said the cat take a look then he got up on top with a tip of his hat i call this game funinabox said the cat in this box are two things i will show to you now you will like these two things said the cat with a bow i will pick up the hook you will see something new two things and i call them thing one and thing two these things will not bite you they want to have fun then out of the box came thing two and thing one and they ran to us fast they said how do you do would you like to shake hands with thing one and thing two and sally and i did not know what to do so we had to shake hands with thing one and thing two we shook their two hands but our fish said no no those things should not be in this house make them go they should not be here when your mother is not put them out put them out said the fish in the pot have no fear little fish said the cat in the hat these things are good things and he gave them a pat they are tame oh so tame they have come here to play they will give you some fun on this wet wet wet day now here is a game that they like said the cat they like to fly kites said the cat in the hat no not in the house said the fish in the pot they should not fly kites in a house they should not oh the things they will bump oh the things they will hit oh i do not like it not one little bit then sally and i saw them run down the hall we saw those two things bump their kites on the wall bump thump thump bump down the wall in the hall thing two and thing one they ran up they ran down on the string of one kite we saw mothers new gown her gown with the dots that are pink white and red then we saw one kite bump on the head of her bed then those things ran about with big bumps jumps and kicks and with hops and big thumps and all kinds of bad tricks and i said i do not like the way that they play if mother could see this oh what would she say then our fish said look look and our fish shook with fear your mother is on her way home do you hear oh what will she do to us what will she say oh she will not like it to find us this way so do something fast said the fish do you hear i saw her your mother your mother is near so as fast as you can think of something to do you will have to get rid of thing one and thing two so as fast as i could i went after my net and i said with my net i can get them i bet i bet with my net i can get those things yet then i let down my net it came down with a plop and i had them at last thoe two things had to stop then i said to the cat now you do as i say you pack up those things and you take them away oh dear said the cat you did not like our game oh dear what a shame what a shame what a shame then he shut up the things in the box with the hook and the cat went away with a sad kind of look that is good said the fish he has gone away yes but your mother will come she will find this big mess and this mess is so big and so deep and so tall we ca not pick it up there is no way at all and then who was back in the house why the cat have no fear of this mess said the cat in the hat i always pick up all my playthings and so i will show you another good trick that i know then we saw him pick up all the things that were down he picked up the cake and the rake and the gown and the milk and the strings and the books and the dish and the fan and the cup and the ship and the fish and he put them away then he said that is that and then he was gone with a tip of his hat then our mother came in and she said to us two did you have any fun tell me what did you do and sally and i did not know what to say should we tell her the things that went on there that day should we tell her about it now what should we do well what would you do if your mother asked you\"\n\n\nTo apply this to all four texts, simply create a function wrapper for the processing commands and then use dplyr to pass the text through the processing function.\n\nclean_fn <- function(.text){\n  str_remove_all(.text, '[[:punct:]]') %>% tolower()\n}\n\nseuss_text_clean <- seuss_text %>%\n  mutate(text = clean_fn(text))\n\nYou could also create a manual list of punctuation to remove.\n\n\n\n\n\n\nAction\n\n\n\nThe regular expression for matching a or b is a | b . Write an alternative to the previous code chunk that lists the punctuation to remove explicitly and does not use '[[:punct:]]' ."
  },
  {
    "objectID": "materials/labs/lab5-text/lab5-text.html#basic-nlp",
    "href": "materials/labs/lab5-text/lab5-text.html#basic-nlp",
    "title": "Text processing",
    "section": "Basic NLP",
    "text": "Basic NLP\nAs you saw in class, once we have a string of clean text for each document, tokenization and lemmatization are largely automated.\n\nTokenization\nunnest_tokens() will tokenize and return the result in tidy format; lemmatize_words() can be applied to the resulting column of tokens using dplyr commands.\n\nstpwrd <- stop_words %>%\n  pull(word) %>%\n  str_remove_all('[[:punct:]]')\n\nseuss_tokens_long <- seuss_text_clean %>%\n  unnest_tokens(output = token, # specifies new column name\n                input = text, # specifies column containing text\n                token = 'words', # how to tokenize\n                stopwords = stpwrd) %>% # optional stopword removal\n  mutate(token = lemmatize_words(token)) \n\n\n\n\n\n\n\nAction\n\n\n\nBased on the data frame above, use row counting (count() ) to answer the following questions:\n\nWhat’s the most frequently used word in each book?\nWhat’s the most frequently used word in all books?\n\nCompare with your neighbor to check your answers.\nIf there’s time: refer to the documentation ?unnest_tokens to determine how to tokenize as bigrams. Find the most frequent bigrams in each book.\n\n\n\n\nFrequency measures\nThe frequency measures discussed in class – term frequency (TF), inverse document frequency (IDF), and their product (TF-IDF) – can be computed from token counts using tidytext::bind_tf_idf() .\n\nseuss_tfidf <- seuss_tokens_long %>%\n  count(doc, token) %>%\n  bind_tf_idf(term = token,\n              document = doc,\n              n = n) \n\nseuss_df <- seuss_tfidf %>%\n  pivot_wider(id_cols = doc, \n              names_from = token,\n              values_from = tf_idf,\n              values_fill = 0)\n\nseuss_df\n\n# A tibble: 4 × 246\n  doc         bad    ball     bed    bend     bet    bite   book     bow     box\n  <fct>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>  <dbl>   <dbl>   <dbl>\n1 \"the c… 0.00396 0.00990 0.00198 0.00198 0.00792 0.00792 0.0158 0.00396 0.00411\n2 \"fox i… 0       0       0       0.00891 0       0       0      0       0.00431\n3 \"green… 0       0       0       0       0       0       0      0       0.0139 \n4 \"hop o… 0.00788 0.0118  0.0118  0       0       0.00394 0      0       0      \n# … with 236 more variables: bump <dbl>, ca <dbl>, cake <dbl>, call <dbl>,\n#   cat <dbl>, cold <dbl>, cup <dbl>, day <dbl>, dear <dbl>, deep <dbl>,\n#   dish <dbl>, dot <dbl>, fall <dbl>, fan <dbl>, fast <dbl>, fear <dbl>,\n#   fish <dbl>, fly <dbl>, fox <dbl>, fun <dbl>, funinabox <dbl>, funny <dbl>,\n#   game <dbl>, gown <dbl>, hall <dbl>, hand <dbl>, hat <dbl>, head <dbl>,\n#   hear <dbl>, hit <dbl>, hold <dbl>, home <dbl>, hook <dbl>, hop <dbl>,\n#   house <dbl>, jump <dbl>, kick <dbl>, kind <dbl>, kite <dbl>, light <dbl>, …\n\n\nWe can use this data to compute a variety of summaries of the text. For example, the two words that distinguish each book most from the other books are, by book:\n\nseuss_tfidf %>%\n  group_by(doc) %>%\n  slice_max(tf_idf, n = 2)\n\n# A tibble: 10 × 6\n# Groups:   doc [4]\n   doc                  token      n     tf   idf tf_idf\n   <fct>                <chr>  <int>  <dbl> <dbl>  <dbl>\n 1 \"the cat in the hat\" cat       26 0.0743 0.693 0.0515\n 2 \"the cat in the hat\" fish      20 0.0571 0.693 0.0396\n 3 \"fox in socks \"      sir       37 0.0792 1.39  0.110 \n 4 \"fox in socks \"      sock      19 0.0407 1.39  0.0564\n 5 \"green eggs and ham\" samiam    13 0.0897 1.39  0.124 \n 6 \"green eggs and ham\" egg       10 0.0690 1.39  0.0956\n 7 \"green eggs and ham\" green     10 0.0690 1.39  0.0956\n 8 \"green eggs and ham\" ham       10 0.0690 1.39  0.0956\n 9 \"hop on pop\"         brown     10 0.0568 1.39  0.0788\n10 \"hop on pop\"         pup        8 0.0455 1.39  0.0630\n\n\nBut the two most common words in each book are:\n\nseuss_tfidf %>%\n  group_by(doc) %>%\n  slice_max(tf, n = 2)\n\n# A tibble: 8 × 6\n# Groups:   doc [4]\n  doc                  token      n     tf   idf tf_idf\n  <fct>                <chr>  <int>  <dbl> <dbl>  <dbl>\n1 \"the cat in the hat\" cat       26 0.0743 0.693 0.0515\n2 \"the cat in the hat\" fish      20 0.0571 0.693 0.0396\n3 \"fox in socks \"      sir       37 0.0792 1.39  0.110 \n4 \"fox in socks \"      sock      19 0.0407 1.39  0.0564\n5 \"green eggs and ham\" eat       24 0.166  0.288 0.0476\n6 \"green eggs and ham\" samiam    13 0.0897 1.39  0.124 \n7 \"hop on pop\"         brown     10 0.0568 1.39  0.0788\n8 \"hop on pop\"         pat       10 0.0568 0.693 0.0394\n\n\n\n\n\n\n\n\nAction\n\n\n\nDiscuss with your neighbor how you might determine how ‘different’ any two books are using an appropriate frequency measure and comparison between rows.\n\nCompute your difference measure for all pairs of books. Which pair is most distinct?\nUse the same idea to compute difference from the ‘average’ Dr. Seuss book. Which book is most different from the rest?"
  },
  {
    "objectID": "materials/labs/lab6-nn/lab6-nn.html",
    "href": "materials/labs/lab6-nn/lab6-nn.html",
    "title": "Training neural networks",
    "section": "",
    "text": "Please complete this activity before arriving at your section meeting. It should only take you about 15 minutes.\n\n\nWe will use packages that interface to python from R to train neural networks in this lab. For this a current python installation is needed.\nSelect the download appropriate for your operating system here and follow installation instructions.\n\n\n\nOpen RStudio and execute the following commands in the console. You should have already installed keras with other packages in the first lab; however, if library(keras) returns an error execute install.packages('keras') and then try again. This will install Tensorflow (for python) in a manner suitable for use in R, along with a few other packages.\n\nlibrary(keras)\ninstall_keras()\n\nTo confirm the installation worked, try:\n\nlibrary(tensorflow)\ntf$constant('Hello world')\n\ntf.Tensor(b'Hello world', shape=(), dtype=string)\n\n\nYou may see a long message related to CUDA libraries in addition to the output shown above, but if you see this output at the end, your installation was successful.\nIf you did not see the expected output, try configuring a virtual environment for the installation explicitly as shown here.\nIf you are unable to troubleshoot after a short period of time, partner with a classmate for the lab activity and then ask for help from course staff."
  },
  {
    "objectID": "materials/labs/lab6-nn/lab6-nn.html#lab-activity",
    "href": "materials/labs/lab6-nn/lab6-nn.html#lab-activity",
    "title": "Training neural networks",
    "section": "Lab activity",
    "text": "Lab activity\n\nSetup\n\n\n\n\n\n\nAction\n\n\n\nSetup\nOpen a new script for this lab, copy-paste the code chunk below at the top of the script, and execute once.\n\n\n\n# packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(tidytext)\nlibrary(keras)\nlibrary(tensorflow)\n\n# data location\nurl <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab6-nn/data/claims-clean.csv'\n\n# read in data\nclean <- read_csv(url)\n\nNow partition the data into training and test sets.\n\n\n\n\n\n\nAction\n\n\n\nData partitioning\n\nCopy the code chunk below into your script but do not run the resulting lines.\nCoordinate with your neighbor: choose a new RNG seed and split proportion and input the same values in each of your scripts.\nExecute lines to partition the data.\n\n\n\n\n# partition\nset.seed(102722)\npartitions <- clean %>%\n  mutate(text_clean = str_trim(text_clean)) %>%\n  filter(str_length(text_clean) > 5) %>%\n  initial_split(prop = 0.8)\n\nNow use the code chunk below to preprocess the training partition into a TF-IDF document term matrix (DTM), as before.\n\ntrain_dtm <- training(partitions) %>%\n  unnest_tokens(output = 'token', \n                input = text_clean) %>%\n  group_by(.id, bclass) %>%\n  count(token) %>%\n  bind_tf_idf(term = token, \n              document = .id, \n              n = n) %>%\n  pivot_wider(id_cols = c(.id, bclass), \n              names_from = token, \n              values_from = tf_idf,\n              values_fill = 0) %>%\n  ungroup()\n\n\n\nLogistic regression as NN\nTo get a feel for keras, first we’ll fit a logistic regression model.\nRecall that in class it was mentioned that standard statistical models can be described by neural networks with no hidden layers; along these lines, standard statistical models can also be fit using optimization routines for neural network training.\nUse the code chunk below to get the TF-IDF values for the (alphabetically) first ten tokens. We’ll use these as predictors.\n\n# extract first ten features\nx_train <- train_dtm %>%\n  ungroup() %>%\n  select(-.id, -bclass) %>%\n  select(1:10) %>%\n  as.matrix()\n\n# extract labels and coerce to binary\ny_train <- train_dtm %>% \n  pull(bclass) %>%\n  factor() %>%\n  as.numeric() - 1\n\nThis is purely for illustration purposes; any model using these variables should not perform well at all because ten tokens won’t contain much information about the classes.\nTo use keras, we’ll go through a few steps that are generally not done separately for fitting statistical models:\n\nModel specification, i.e.,defining an architecture\nModel configuration, i.e., specifying a loss function and fitting method\nModel training, i.e., computing estimates for the parameters\n\nModel specification\nModel architecture is defined layer-by-layer. Keras has some preconfigured model types: for feedforward networks, use keras_model_sequential() .\n\n# specify model type\nmodel <- keras_model_sequential(input_shape = 10)\n\nThe input_shape argument specifies the number of units for the input layer – in other words, the number of predictors.\nAt this stage, the model is just scaffolding:\n\nsummary(model)\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n================================================================================\nTotal params: 0\nTrainable params: 0\nNon-trainable params: 0\n________________________________________________________________________________\n\n\nNow layers can be added one-by-one. For now we’ll just add an output layer – one unit. layer_dense will specify that the previous layer is fully-connected to the added layer.\n\n# add output layer\nmodel <- model %>% layer_dense(1) \n\nThe model summary now shows the output layer.\n\nsummary(model)\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense (Dense)                      (None, 1)                       11          \n================================================================================\nTotal params: 11\nTrainable params: 11\nNon-trainable params: 0\n________________________________________________________________________________\n\n\nLastly, we’ll add a sigmoid activation function:\n\nmodel <- model %>% \n  layer_activation(activation = 'sigmoid')\n\nSince there is no hidden layer, our model is\n\\[\n\\begin{aligned}\n\\mathbb{E}Y &= \\frac{1}{1 + \\exp\\{-T\\}} \\\\\nT &= X\\beta\n\\end{aligned}\n\\]\nNotice that this is the logistic regression model (without the distributional assumption).\nModel configuration\nConfiguring a keras model consists in equipping it with a loss and an optimization method. Optionally, metrics that you’d like computed at each training epoch can be included.\n\nmodel %>% compile(\n  loss = 'binary_crossentropy',\n  optimizer = optimizer_sgd(),\n  metrics = 'binary_accuracy'\n)\n\nThis says that to train the model, we’ll minimize binary cross-entropy on the training data using stochastic gradient descent.\nTo train for 10 epochs, pipe the model into fit() and supply the training data. Note that the training data must be numeric, not a data frame.\n\nhistory <- model %>%\n  fit(x = x_train, \n      y = y_train,\n      epochs = 10)\n\nThe following commands will retrieve weights, evaluated loss ans specified metrics, and predictions.\n\n# retrieve weights\nget_weights(model)\n\n[[1]]\n              [,1]\n [1,] -0.309943348\n [2,] -0.607303262\n [3,] -0.684534788\n [4,]  0.080475129\n [5,] -0.047711506\n [6,]  0.232125387\n [7,] -0.203573957\n [8,] -0.295428514\n [9,]  0.001397455\n[10,] -0.489086449\n\n[[2]]\n[1] 0.02782592\n\n# evaluate on specified data\nevaluate(model, x_train, y_train)\n\n           loss binary_accuracy \n      0.6928954       0.5226244 \n\n# compute predictions\nmodel(x_train) %>% head()\n\ntf.Tensor(\n[[0.49740595]\n [0.5064572 ]\n [0.506311  ]\n [0.5053739 ]\n [0.5067238 ]\n [0.5057194 ]], shape=(6, 1), dtype=float32)\n\n\n\n\n\n\n\n\nAction\n\n\n\nCheck your understanding\nDiscuss with your neighbor:\n\nHow many parameters does this model have?\nDo the number of parameters match your expectations?\nWhy will the parameter estimates not match the result of glm() ?\nWould further training epochs improve the performance?\n\n\n\n\n\nSingle-layer network\nNow that you have a sense of the basic keras syntax and model specification/configuration/training procedure, we can train a proper network with one (or more!) hidden layers.\nFirst coerce the DTM into the format needed for training.\n\n# store full DTM as a matrix\nx_train <- train_dtm %>%\n  select(-bclass, -.id) %>%\n  as.matrix()\n\nNow configure a model with one hidden layer having 10 units. Notice that the architecture can be defined by one sequence of pipes rather than stepwise as before.\n\nmodel <- keras_model_sequential(input_shape = ncol(x_train)) %>%\n  layer_dense(10) %>%\n  layer_dense(1) %>%\n  layer_activation(activation = 'sigmoid')\n\nsummary(model)\n\nModel: \"sequential_1\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_2 (Dense)                    (None, 10)                      176540      \n dense_1 (Dense)                    (None, 1)                       11          \n activation_1 (Activation)          (None, 1)                       0           \n================================================================================\nTotal params: 176,551\nTrainable params: 176,551\nNon-trainable params: 0\n________________________________________________________________________________\n\n\nNotice the number of parameters. (Does this match your expectation?) Configure the model:\n\nmodel %>%\n  compile(\n    loss = 'binary_crossentropy',\n    optimizer = optimizer_sgd(),\n    metrics = 'binary_accuracy'\n  )\n\nAnd finally, train:\n\nhistory <- model %>%\n  fit(x = x_train,\n      y = y_train,\n      epochs = 50)\n\nplot(history)\n\n\n\n\nNotice that even after 50 epochs the results are still quite poor. As mentioned in class, the choice of optimization method can have a big impact on the quality of estimates. If we train the model instead using Adam, good accuracy is achieved after just a few epochs:\n\n# change the optimizer\nmodel %>%\n  compile(\n    loss = 'binary_crossentropy',\n    optimizer = 'adam',\n    metrics = 'binary_accuracy'\n  )\n\n# re-train\nhistory <- model %>%\n  fit(x = x_train,\n      y = y_train,\n      epochs = 10)\n\nplot(history)\n\n\n\n\n\n\nValidation data\nOften training data are sub-partitioned into training and ‘validation’ sets. The validation set can be used to provide a soft estimate of accuracy during training.\nThis provides one strategy to avoid overfitting – the practitioner should only train as long as validation accuracy continues to increase.\nKeras makes that easy by supplying an extra argument to fit(). The code chunk below trains for longer and uses 20% of the training data for validation. You should see that the training accuracy gets quite high, but the validation accuracy plateaus around 80%.\n\n# redefine model\nmodel <- keras_model_sequential(input_shape = ncol(x_train)) %>%\n  layer_dense(10) %>%\n  layer_dense(1) %>%\n  layer_activation(activation = 'sigmoid')\n\nmodel %>%\n  compile(\n    loss = 'binary_crossentropy',\n    optimizer = 'adam',\n    metrics = 'binary_accuracy'\n  )\n\n# train with validation split\nhistory <- model %>%\n  fit(x = x_train,\n      y = y_train,\n      epochs = 20,\n      validation_split = 0.2)\n\nplot(history)\n\n\n\n\n\n\n\n\n\n\nAction\n\n\n\nCompute predictions from your trained network on the test partition. Estimate the predictive accuracy. Is it any better than what we managed with principal component regression in class?"
  },
  {
    "objectID": "materials/labs/lab7-curvefitting/lab7-curvefitting.html",
    "href": "materials/labs/lab7-curvefitting/lab7-curvefitting.html",
    "title": "Curve fitting",
    "section": "",
    "text": "replicate the estimation of seasonal trend in the soil data as shown in class;\nexplore curve-fitting methods in greater depth.\n\n\n\n\n\n\n\nAction\n\n\n\nSetup: open RStudio, create a new script for this lab, and copy-paste the code chunk below into the first several lines of the script. Execute once to load and preview the data.\n\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(modelr)\nlibrary(fda)\n\nurl <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab7-curvefitting/data/soiltemp.csv'\n\nsoil <- read_csv(url)\nsoil %>% head()\n\n# A tibble: 6 × 6\n  site    year   day date        elev  temp\n  <chr>  <dbl> <dbl> <date>     <dbl> <dbl>\n1 B21K-1  2017   226 2017-08-14    96  3.82\n2 B21K-1  2017   227 2017-08-15    96  3.72\n3 B21K-1  2017   228 2017-08-16    96  3.31\n4 B21K-1  2017   229 2017-08-17    96  4.95\n5 B21K-1  2017   230 2017-08-18    96  5.46\n6 B21K-1  2017   231 2017-08-19    96  5.80\n\n\nOur overall objective is to model temp as a function of day. As discussed in class, this amounts to estimating an unknown function \\(f(\\text{day})\\) in the model:\n\\[\n\\text{temp}_i = f(\\text{day}_i) + \\text{error}_i\n\\]\n(Here we’re using \\(i\\) to index observations and not sites – so we will use 16,799 observations to estimate \\(f\\). Note that there are only 57 unique sites.)\nAlthough visually it makes more sense to plot paths by site, since we’re pooling all the observations together to estimate \\(f\\) we’re essentially treating the data as a collection of unrelated points. We’ll make use of the following visualization throughout to underscore this point.\n\n# scatterplot of temperatures against day of year\ntemp_pts <- soil %>%\n  ggplot(aes(x = day, y = temp)) +\n  geom_point(alpha = 0.1)\n\ntemp_pts\n\n\n\n\n\nLOESS curves\nIt’s worth being aware that ggplot has a built-in ability to compute local nonparametric conditional mean estimates using LOESS (LOcally Estimated Scatterplot Smoothing). This can often be a quick-and-dirty strategy for visualizing trends; it is less often (but sometimes!) used as a model for prediction and the like.\n\ntemp_pts + \n  geom_smooth(formula = 'y ~ x',\n              method = 'loess',\n              span = 0.5,\n              se = F)\n\n\n\n\nThis technique consists in computing a weighted average temp for every value of day that up-weights nearby points.\nThe span parameter controls the size of the neighborhood that contributes meaningfully to the weighted average – a smaller span, and thus a smaller neighborhood, produces a more erratic curve.\n\n\n\n\n\n\nAction\n\n\n\nTry changing the smoothing span. Note that the units for span are standard deviations, e.g., span = 1 says adjust the smoothing neighborhood in proportion to \\(\\pm 1 \\text{SD}\\) .\n\nPick a larger value.\nPick a smaller value.\nFind a value that you think captures the trend best.\n\n\n\nThe LOESS curve is fully nonparametric, meaning no assumptions are made about the form of \\(f\\).\nThis is an advantage in some senses – it’s a model-free method – but a disadvantage in others – it doesn’t provide any meaningful data reduction since the curve is just a bunch of averages.\n\n\nPolynomial regression\nIf we want an estimate that does yield some data reduction, a straightforward approach might be to fit a polynomial in day .\nThis can also be done through ggplot using a regression smooth (geom_smooth(..., method = 'lm', ...)). The following superimposes a quadratic fit (order 2 polynomial) to the data:\n\n# quadratic fit\ntemp_pts + \n  geom_smooth(formula = 'y ~ poly(x, 2)',\n              method = 'lm')\n\n\n\n\nAs an aside, poly is a handy R function that will compute polynomial terms in is argument, as illustrated below. (The argument raw = T ensures that ‘raw’ polynomials are returned rather than orthogonal polynomials. Orthogonal polynomials are the Gram-Schmidt orthognalization of the matrix of raw polynomials.)\n\npoly(1:5, degree = 3, raw = T, simple = T)\n\n     1  2   3\n[1,] 1  1   1\n[2,] 2  4   8\n[3,] 3  9  27\n[4,] 4 16  64\n[5,] 5 25 125\n\n\nSo the quadratic model is:\n\\[\n\\text{temp}_i = \\beta_0 + \\beta_1 \\text{day}_i + \\beta_1 \\text{day}^2_i + \\epsilon_i\n\\]\nThis could be fit explicitly as a regression model:\n\n# fit a polynomial model\nfit_poly <- lm(temp ~ poly(day, degree = 2, raw = T),\n               data = soil)\n\n# compute predictions\npred_df <- tibble(day = 1:365) %>%\n  add_predictions(fit_poly)\n\n# visualize\ntemp_pts + \n  geom_path(data = pred_df, \n            aes(y = pred),\n            color = 'blue')\n\n\n\n\n\n\n\n\n\n\nAction\n\n\n\nExperiment with the polynomial degree and fit a model that you think best approximates the curve.\nSuggestion: see what happens if you overfit by choosing a polynomial of unrealistically large order, e.g., a polynomial of degree 25.\n\n\n\n\nSpline regression\nSpline regression refers to fitting piecewise models constrained to join together at the “knot” points that divide the pieces.\nThis can be accomplished by some creative manipulation of the model formula with indicator variables. An example is shown below of a linear spline with one knot at day 200. It’s not so important that you understand the formula manipulation as much as the idea.\n\n# linear spline with a knot at day 200\nfit_spline <- lm(temp ~ day + I((day - 200)*(day > 200)) - 1,\n                 data = soil)\n\n# compute predictions\npred_df <- tibble(day = 1:365) %>%\n  add_predictions(fit_spline)\n\n# plot it\ntemp_pts + \n  geom_path(data = pred_df, \n            aes(y = pred),\n            color = 'blue')\n\n\n\n\nThe formulae would get a little more complicated, but the principle would be the same for (a) introducing additional knots and/or (b) fitting polynomials to each segment.\n\n\nSpline basis (a.k.a. “b-spline”)\nA better solution than working out the formula each time is to use a basis expansion for the set of all possible regression splines of a given order.\nThis is what we discussed in class, and is illustrated in the code chunk below. The bs() function works similarly to poly() in the sense that it returns a matrix of values of the basis functions (cf. polynomial functions) for each value of an input vector.\n\n# define knot points\nknotpts <- c(100, 200, 300)\n\n# fit an order 3 regression spline with three internal knots\nfit_bs <- lm(temp ~ bs(day, degree = 1, knots = knotpts),\n             data = soil) \n\n# compute predictions\npred_df <- tibble(day = 1:365) %>%\n  add_predictions(fit_bs)\n\n# plot it\ntemp_pts + \n  geom_path(data = pred_df, \n            aes(y = pred),\n            color = 'blue') +\n  geom_vline(xintercept = knotpts, \n             linetype = 'dashed')\n\n\n\n\n\n\n\n\n\n\nAction\n\n\n\n\nExperiment with the polynomial order and the placement of knot points.\nFind a fitted curve that you feel captures the pattern well.\nCompare with your neighbor. What did you choose similarly? What did you choose differently?\n\n\n\nFor the curious: you can also inspect the basis functions used to generate the regression spline and their dependence on the knot placement using the code chunk below.\n\n# define knots\nknotpts <- c(100, 200, 300)\n\n# input variable\nx <- 1:365\n\n# calculate basis expansion and plot it\nbs(x, knots = knotpts, degree = 3) %>%\n  as_tibble() %>%\n  bind_cols(x = x) %>%\n  pivot_longer(-x, names_to = 'basis') %>%\n  ggplot(aes(x = x, y = value)) +\n  geom_path(aes(group = basis, color = basis)) +\n  geom_vline(xintercept = knotpts, linetype = 'dashed')\n\n\n\nFourier basis\nLastly, it was noted in class that spline regression does not produce a harmonic function and so is not well-suited to approximating a cyclical trend. Using a Fourier basis solves this problem.\nThe Fourier basis functions (expressed as real-valued functions) are the sequence of sine and cosine pairs characterized by\n\\[\n\\phi_{2n}(x) = k\\cos\\left(\\frac{2\\pi nx}{\\lambda}\\right)\n\\quad\\text{and}\\quad\n\\phi_{2n + 1}(x) = k\\sin\\left(\\frac{2 \\pi n x}{\\lambda}\\right) \\qquad\nn = 1, 2, \\dots\n\\]\nwhere \\(k\\) is a scaling factor. The first basis function is a constant \\(\\phi_1 (x) = c\\).\nThis basis can be used to approximate any continuous function. One need only choose a number of pairs to employ; by default, the initial wavelength \\(\\lambda\\) is set to the span of the data.\nA nice consequence for us in the current context is that the resulting estimate, as a linear combination of harmonic functions, is itself harmonic.\n\n# fit the model with the fourier basis expansion\nfit_fbs <- lm(temp ~ fourier(day, nbasis = 4, period = 365) - 1,\n              data = soil)\n\n# compute predictions\npred_df <- tibble(day = 1:365) %>%\n  add_predictions(fit_fbs)\n\n# plot it\ntemp_pts + \n  geom_path(data = pred_df, \n            aes(y = pred),\n            color = 'blue')\n\n\n\n\nThe model is fit without an intercept (temp ~ ... - 1 ) because fourier(...) returns a constant term.\nNotice also that there is no need to specify knots in this case. The Fourier basis is generated using the period specified – each additional pair of sine and cosine basis functions simply halves the frequency of the previous pair.\n\n\n\n\n\n\nAction\n\n\n\n\nExperiment with the number of bases (note nbasis must be an even number).\nUsing a single pair of basis functions, plot estimates of the mean temperature over a period of 2 years. What happens when the input variable is outside the interval [1, 365]?\nMake a time course plot of the temperature data with the estimated mean from the fit using the Fourier basis expansion overlaid."
  },
  {
    "objectID": "materials/labs/lab8-forecasting/lab8-forecasting.html",
    "href": "materials/labs/lab8-forecasting/lab8-forecasting.html",
    "title": "Regression with AR errors",
    "section": "",
    "text": "Objectives: select AR order for an error model using exploratory diagnostic techniques; fit a regression model with AR errors and generate forecasts for future series.\n\nSetup\n\n\n\n\n\n\nAction\n\n\n\nOpen RStudio and set up a new script in your lab directory. Copy and paste the code chunk below and execute once.\nYou may need to install the forecast package.\n\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(forecast)\nlibrary(fda)\n\nurl <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab8-forecasting/data/soiltemp-200cm.csv'\n\nsoil <- read_csv(url) %>%\n  dplyr::select(-year, -elev) %>%\n  filter(!str_starts(site, 'SHA'))\n\n\n\nSite selection\nAlthough you saw models fit to every site in class, for this lab we’ll just focus on building a model for a single site. You will pick a site and carry out the model-building process; your neighbor(s) will likely pick different sites, so you can compare results with them throughout.\nThe process you go through in this lab is what would be iterated over every site to reproduce the analysis shown in class.\n\n\n\n\n\n\nAction\n\n\n\nTo start, pick a site to work with using the code chunk below. Check with your neighbor and ensure you choose different sites.\n\n\n\n# choose a site at random\nset.seed(111522) # comment out!\nnsites <- soil %>% pull(site) %>% unique() %>% length()\nsite_ix <- sample(1:nsites, size = 1)\n\n# filter rows\nsite_data <- soil %>% \n  filter(site == unique(soil$site)[site_ix])\n\n# preview\nsite_data %>% head()\n\n# A tibble: 6 × 4\n  site     day date        temp\n  <chr>  <dbl> <date>     <dbl>\n1 B21K-1   226 2017-08-14  3.82\n2 B21K-1   227 2017-08-15  3.72\n3 B21K-1   228 2017-08-16  3.31\n4 B21K-1   229 2017-08-17  4.95\n5 B21K-1   230 2017-08-18  5.46\n6 B21K-1   231 2017-08-19  5.80\n\n\n\n\nAR order selection\nOur model (for one site) is\n\\[\n\\begin{aligned}\nY_t &= f(t) + \\epsilon_t \\\\\n\\epsilon_t &= \\sum_{d = 1}^D \\alpha_d \\epsilon_{t - d} + \\xi_t\n\\end{aligned}\n\\]\nThere are a few ways to pick the order \\(D\\), but the most common is to inspect the correlation structure of the residuals.\nSo first we need to get the residuals.\n\n\n\n\n\n\nAction\n\n\n\nCopy the code chunk below verbatim and execute. This will fit the regression part of our model and store the residuals along with other model quantities.\n\n\n\n# predictor matrix\nxreg <- site_data %>%\n  pull(day) %>%\n  fda::fourier(nbasis = 4, period = 365)\n\n# response\ny <- pull(site_data, temp)\n\n# create a data frame\nreg_df <- bind_cols(temp = y, \n                    xreg)\n\n# fit the model\nfit <- lm(temp ~ . - 1, data = reg_df)\n\n# obtain fitted values, residuals, etc.\nfit_df <- broom::augment(fit) %>%\n  bind_cols(date = site_data$date)\n\nTo start, let’s just plot the residual series:\n\n# plot residual series\nfit_df %>%\n  ggplot(aes(x = date, y = .resid)) +\n  geom_path()\n\n\n\n\nIdeally, this should be (approximately) “stationary”, which means the series has:\n\nconstant mean over time\nconstant variance over time\n\n\n\n\n\n\n\nAction\n\n\n\nPlot the residual series for your site and compare with your neighbor.\nDiscuss: do your residual series look stationary, or close enough? What do you look to in the graphic to make that assessment?\n\n\nIf we plot the residuals against themselves at several time lags, we can observe the correlation decaying gradually.\n\n# plot residuals at various lags\nfit_df %>%\n  dplyr::select(.resid) %>%\n  mutate(lag1 = lag(.resid, n = 1),\n         lag2 = lag(.resid, n = 2),\n         lag3 = lag(.resid, n = 3),\n         lag4 = lag(.resid, n = 4),\n         lag5 = lag(.resid, n = 5),\n         lag6 = lag(.resid, n = 6)) %>%\n  pivot_longer(-.resid) %>%\n  ggplot(aes(x = .resid, y = value)) +\n  geom_point() + \n  facet_wrap(~ name)\n\n\n\n\nWe can capture this pattern succinctly by computing the correlation coefficient at each lag. This is called the (sample) autocorrelation function.\n\nresid_acf <- acf(fit_df$.resid, plot = F)\n\nplot(resid_acf, main = '')\n\n\n\n\nIf you’re curious, try computing a few correlations ‘by hand’ and checking that they closely match resid_acf (they won’t match exactly).\n\n\n\n\n\n\nCheck your understanding\n\n\n\nDiscuss with your neighbor:\n\nWhat do the heights of the vertical lines in the ACF plot show?\nWhat does the ACF plot capture about the residuals?\n\n\n\nNow, any autoregressive process will have some autocorrelation even at distant lags, regardless of the order.\nTo select the order, the partial autocorrelation is used – the correlation at a given lag after adjusting for previous lags.\n\nresid_pacf <- pacf(fit_df$.resid, plot = F)\n\nplot(resid_pacf, main = '')\n\n\n\n\n\n\n\n\n\n\nAside\n\n\n\nThe partial autocorrelations are computed iteratively by:\n\nComputing the correlation between \\(e_t\\) and \\(e_{t - 1}\\)\nRemoving the part of \\(e_t\\) explainable based on \\(e_{t - 1}\\) and obtaining an adjusted residual \\(\\tilde{e}_t = e_t - \\beta_1 e_{t - 1}\\)\nRepeat 1-2 for lags 2, 3, …, replacing \\(e_{t - h}\\) with \\(\\tilde{e}_{t - h}\\)\n\n\n\nThe simplest heuristic for AR order selection is to choose as \\(D\\) the number of significant partial autocorrelations.\n\n\n\n\n\n\nAction\n\n\n\n\nCompute and plot the PACF for your residual series.\nDetermine the order of autoregression: how many partial autocorrelations are outside the confidence bands?\nCompare with your neighbor.\n\n\n\n\n\nFitting an AR model\nWe could at this point fit our full model in one go. However, let’s instead start by fitting the error model directly to the residuals.\n\n# fit error model\nfit_resid <- Arima(fit_df$.resid,\n      order = c(2, 0, 0),\n      include.mean = F,\n      method = 'ML')\n\nFrom the fitted model, we can obtain the theoretical autocorrelation and compare it with the sample:\n\nresid_acf_fitted <- ARMAacf(ar = coef(fit_resid), \n                            lag.max = 25)\n\nplot(resid_acf, main = '')\nlines(resid_acf_fitted, col = 'red')\n\n\n\n\nForecasting based on this model is straightforward. There is a predict method:\n\npredict(fit_resid, n.ahead = 5)\n\n$pred\nTime Series:\nStart = 644 \nEnd = 648 \nFrequency = 1 \n[1] 1.598966 1.500999 1.380972 1.253629 1.127627\n\n$se\nTime Series:\nStart = 644 \nEnd = 648 \nFrequency = 1 \n[1] 0.1988213 0.3622029 0.5026578 0.6179443 0.7102430\n\n\nBut the forecast package returns predictions a bit more neatly arranged:\n\nforecast(fit_resid, h = 5)\n\n    Point Forecast     Lo 80    Hi 80       Lo 95    Hi 95\n644       1.598966 1.3441662 1.853766  1.20928339 1.988649\n645       1.500999 1.0368175 1.965181  0.79109459 2.210904\n646       1.380972 0.7367903 2.025154  0.39578099 2.366163\n647       1.253629 0.4617016 2.045557  0.04248054 2.464778\n648       1.127627 0.2174142 2.037840 -0.26442347 2.519678\n\n\nNow we could mimic what was done in our first class on this data and add the residual forecasts to predictions from the regression model, but it is much more efficient to fit a single model (rather than two) that captures both the regression and the error part.\n\n\n\n\n\n\nTakeaway\n\n\n\nAn autoregressive model (and time series models generally) may look like it’s a model describing the conditional mean of a series in the present given the past. However, it’s really about the correlation structure.\n\n\n\n\nRegression with AR errors\nNow that we’ve identified the error model we can fit the full model simultaneously using maximum likelihood.\nWe would typically write this model not as a two-level model but as:\n\\[\nY_{t} = f(t) + \\epsilon_t\n\\;,\\qquad\n\\epsilon_t \\sim AR(2)\n\\] Writing the model more explicitly in terms of the basis expansion for \\(f\\):\n\\[\nY_{t} = \\beta_0 + \\sum_{j = 1}^4 \\beta_j f_j(t) + \\epsilon_t\n\\;,\\qquad\n\\epsilon_t \\sim AR(2)\n\\]\nThis can also be fitted using Arima(). Because we want to do forecasting, let’s hold out the last month or so of data.\n\n# determine a point at which to cut the series\ncutpt <- nrow(xreg) - 30\n\n# training series\ny_train <- y[1:cutpt]\nx_train <- xreg[1:cutpt, ]\n\n# fit the model\nfit_full <- Arima(y_train,\n                  order = c(2, 0, 0),\n                  xreg = x_train,\n                  include.mean = F,\n                  method = 'ML')\n\nbroom::tidy(fit_full) %>% knitr::kable()\n\n\n\n\nterm\nestimate\nstd.error\n\n\n\n\nar1\n1.5192225\n0.0332166\n\n\nar2\n-0.5663175\n0.0332869\n\n\nconst\n60.3315298\n3.4003353\n\n\nsin1\n-59.3071214\n3.5694270\n\n\ncos1\n-61.6934230\n3.1327989\n\n\nsin2\n23.9546701\n3.2289715\n\n\ncos2\n16.1671305\n3.2584591\n\n\n\n\n\n\n\n\n\n\n\nWhy no mean?\n\n\n\nThe argument include.mean = F tells Arima() not to ‘de-mean’ the residual series, i.e., fit an intercept.\nThe reason for this choice is that the regressors include a constant term, so an intercept is already being fit at the level of the regression model. Including another intercept produces an unidentifiable model.\nIf you’re curious, try changing to include.mean = T. R should throw an error.\n\n\n\n\n\n\n\n\nAction\n\n\n\nCompare the (non-AR parameter) estimates from this model with the coefficient estimates assuming \\(iid\\) errors that we fit in selecting an error model (i.e., fit ). Are they close? Why are they different at all?\n\n\n\n\nForecasts\nNow, to forecast, we need to supply future regressor values. Luckily for us, the regressors are deterministic functions of day of the year, so that is easy. (If we had covariates, however, this might be a potential problem.)\n\n# testing series\ny_test <- y[(cutpt + 1):nrow(xreg)]\nx_test <- xreg[(cutpt + 1):nrow(xreg), ]\n\npreds <- forecast(fit_full, \n                  h = nrow(x_test), \n                  xreg = x_test)\n\npreds %>% as_tibble() %>% head()\n\n# A tibble: 6 × 5\n  `Point Forecast` `Lo 80` `Hi 80` `Lo 95` `Hi 95`\n             <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1           -0.189  -0.450  0.0722  -0.588   0.210\n2           -0.266  -0.741  0.209   -0.992   0.460\n3           -0.356  -1.01   0.301   -1.36    0.649\n4           -0.452  -1.26   0.354   -1.68    0.781\n5           -0.547  -1.47   0.376   -1.96    0.865\n6           -0.641  -1.66   0.374   -2.19    0.912\n\n\nTo plot the forecasts, bind the fitted values and predictions with the original data and visualize:\n\nfig_forecast <- site_data %>%\n  dplyr::select(date, temp) %>%\n  bind_cols(pred = c(fit_full$fitted, preds$mean),\n            status = c(rep('obs', nrow(x_train)),\n                       rep('pred', nrow(x_test)))) %>%\n  filter(date >= ymd('2018-01-01')) %>% # adjust if needed\n  ggplot(aes(x = date)) +\n  geom_path(aes(y = temp, linetype = status)) +\n  geom_path(aes(y = pred), color = 'blue', alpha = 0.5)\n\nfig_forecast\n\n\n\n\n\nci_df <- site_data %>% \n  slice_tail(n = nrow(x_test)) %>% \n  dplyr::select(date) %>%\n  bind_cols(lwr = preds$lower[, 2],\n            upr = preds$upper[, 2])\n\nfig_forecast +\n  geom_ribbon(aes(x = date, ymin = lwr, ymax = upr),\n              alpha = 0.3, fill = 'blue',\n              data = ci_df)\n\n\n\n\n\n\n\n\n\n\nAction\n\n\n\nRemake the plot of forecasts with uncertainty quantification and zoom in on the forecasted region (by filtering out more of the dates when creating the base layer fig_forecast).\nCompare and contrast with your neighbor.\n\nHow good are the forecasts and for how long?\nWhat happens to the uncertainty for longer-term forecasts?"
  },
  {
    "objectID": "materials/slides/week0-intro.html#before-we-begin",
    "href": "materials/slides/week0-intro.html#before-we-begin",
    "title": "Course orientation",
    "section": "Before we begin…",
    "text": "Before we begin…\n\nIdentify the row number you are sitting.\nPlease sign in using the attendance reporting form found here:\nAttendance reporting form"
  },
  {
    "objectID": "materials/slides/week0-intro.html#welcome",
    "href": "materials/slides/week0-intro.html#welcome",
    "title": "Course orientation",
    "section": "Welcome",
    "text": "Welcome\nPSTAT197A/CMPSC190DD is the first course in UCSB’s year-long data science capstone sequence.\n\nAudience: undergraduate students of any discipline with a basic background in data science and an interest in research\nAim: prepare for an independent research or project experience"
  },
  {
    "objectID": "materials/slides/week0-intro.html#capstone-projects",
    "href": "materials/slides/week0-intro.html#capstone-projects",
    "title": "Course orientation",
    "section": "Capstone projects",
    "text": "Capstone projects\nMost students are preparing for capstone projects in winter and spring. Course foci were chosen with this in mind.\n\nProjects are varied ➜ emphasize problem patterns over methodology\nProjects are collaborative ➜ emphasize teamwork and discussion\nProjects are specialized ➜ practice independent learning based on use cases\n\nRead about past projects at https://centralcoastdatascience.org/projects"
  },
  {
    "objectID": "materials/slides/week0-intro.html#continuing-in-capstones",
    "href": "materials/slides/week0-intro.html#continuing-in-capstones",
    "title": "Course orientation",
    "section": "Continuing in capstones",
    "text": "Continuing in capstones\nContinuation in PSTAT197B-C/CMPSC190DE-DF during winter and spring:\n\nstudents admitted to this course in spring have a seat;\nstudents admitted from the waitlist are on the waitlist."
  },
  {
    "objectID": "materials/slides/week0-intro.html#outcomes",
    "href": "materials/slides/week0-intro.html#outcomes",
    "title": "Course orientation",
    "section": "Outcomes",
    "text": "Outcomes\nI hope to support all of you in:\n\nusing modern software with version control for collaboration;\nrecognizing problem patterns based on data semantics and research questions;\nidentifying and accessing resources for independent learning given a problem of interest;\ncommunicating data analysis and/or research findings."
  },
  {
    "objectID": "materials/slides/week0-intro.html#classroom-environment",
    "href": "materials/slides/week0-intro.html#classroom-environment",
    "title": "Course orientation",
    "section": "Classroom environment",
    "text": "Classroom environment\nWe are in an interactive classroom for a reason: to interact!\nLet’s acknowledge:\n\nPreparations and areas of expertise vary widely among the class\nIt’s okay not to know things\nIf you have a question, probably someone else does too"
  },
  {
    "objectID": "materials/slides/week0-intro.html#resources",
    "href": "materials/slides/week0-intro.html#resources",
    "title": "Course orientation",
    "section": "Resources",
    "text": "Resources\nAll course content is hosted on our website\nhttps://pstat197.github.io/pstat197a/"
  },
  {
    "objectID": "materials/slides/week0-intro.html#modules",
    "href": "materials/slides/week0-intro.html#modules",
    "title": "Course orientation",
    "section": "Modules",
    "text": "Modules\nThe course is configured in modules defined by a dataset and questions (much like a project).\nA module typically comprises:\n\nOne session on data introduction (lecture/discussion)\nTwo sessions on problem patterns and related methodology (lecture)\nTwo labs with related examples (section meeting)\nOne session on sharing data analysis results (discussion)"
  },
  {
    "objectID": "materials/slides/week0-intro.html#module-content",
    "href": "materials/slides/week0-intro.html#module-content",
    "title": "Course orientation",
    "section": "Module content",
    "text": "Module content\nThe module datasets are currently as follows:\n\nClass intake survey data (exploratory/descriptive analysis)\nBiomarkers of autism (predictive modeling and variable selection)\nWeb fraud (text processing and deep learning)\nSoil temperatures (correlated data)"
  },
  {
    "objectID": "materials/slides/week0-intro.html#group-assignments",
    "href": "materials/slides/week0-intro.html#group-assignments",
    "title": "Course orientation",
    "section": "Group assignments",
    "text": "Group assignments\nEach module you will be assigned a working group.\nYour group’s objective is to produce an analysis of the dataset:\n\nReproduce analysis presented/discussed in class meeting\nExtend the analysis by\n\napplying an alternative method that addresses the same question(s)\nor addressing a corollary question"
  },
  {
    "objectID": "materials/slides/week0-intro.html#vignettes",
    "href": "materials/slides/week0-intro.html#vignettes",
    "title": "Course orientation",
    "section": "Vignettes",
    "text": "Vignettes\nAt the end of the class in place of a fifth module you will create a vignette (short demonstration) on a topic of interest.\n\npresent a use case\nexplain methodology\ndemonstrate implementation with example code"
  },
  {
    "objectID": "materials/slides/week0-intro.html#expectations-and-assessments",
    "href": "materials/slides/week0-intro.html#expectations-and-assessments",
    "title": "Course orientation",
    "section": "Expectations and assessments",
    "text": "Expectations and assessments\n\n\nStudents are expected to:\n\nprepare for class meetings as directed;\nattend and actively participate in class and section meetings;\ncontribute meaningfully to group activities and assignments.\n\n\nStudents are assessed on:\n\nattendance, preparation, and participation;\nquality of submitted work;\nindividual contributions to group assignments;\noral interview/presentation."
  },
  {
    "objectID": "materials/slides/week0-intro.html#next-time",
    "href": "materials/slides/week0-intro.html#next-time",
    "title": "Course orientation",
    "section": "Next time",
    "text": "Next time\nWe’ll discuss:\n\ndata science as a discipline;\nthe research landscape;\nsystems and design thinking for data science."
  },
  {
    "objectID": "materials/slides/week0-intro.html#checklist",
    "href": "materials/slides/week0-intro.html#checklist",
    "title": "Course orientation",
    "section": "Checklist",
    "text": "Checklist\nComplete all of the following before our next meeting.\n\nReview all content in the about section of the course webpage.\nInstall course software and create a GitHub account.\nFill out capstone project intake form.\nRead Peng, R. D., & Parker, H. S. (2022). Perspective on data science. Annual Review of Statistics and Its Application, 9, 1-20. (access online via UCSB library).\nPrepare a reading response."
  },
  {
    "objectID": "materials/slides/week1-github.html#announcementsreminders",
    "href": "materials/slides/week1-github.html#announcementsreminders",
    "title": "Basic GitHub actions",
    "section": "Announcements/reminders",
    "text": "Announcements/reminders\n\ntoday, sit with the members of your team (From Tuesday’s section)\nassignment for next time:\n\nread MDSR 9.1 – 9.2\nprepare a reading response"
  },
  {
    "objectID": "materials/slides/week1-github.html#objective-for-today",
    "href": "materials/slides/week1-github.html#objective-for-today",
    "title": "Basic GitHub actions",
    "section": "Objective for today",
    "text": "Objective for today\nLearn how to interact with GitHub repositories:\n\nretrieve and submit file changes;\nexamine repository updates;\nuse branches for parallel workflow;\nresolve conflicts."
  },
  {
    "objectID": "materials/slides/week1-github.html#basic-git-actions",
    "href": "materials/slides/week1-github.html#basic-git-actions",
    "title": "Basic GitHub actions",
    "section": "Basic Git actions",
    "text": "Basic Git actions\n\nCommunication actions for moving file changes between locations"
  },
  {
    "objectID": "materials/slides/week1-github.html#branching-workflow",
    "href": "materials/slides/week1-github.html#branching-workflow",
    "title": "Basic GitHub actions",
    "section": "Branching workflow",
    "text": "Branching workflow\n\nTypical use of repository branches for development of new features"
  },
  {
    "objectID": "materials/slides/week1-github.html#activity-overview",
    "href": "materials/slides/week1-github.html#activity-overview",
    "title": "Basic GitHub actions",
    "section": "Activity overview",
    "text": "Activity overview\n\nMake individual changes to files and create ‘commits’\nCreate repository branches to enable you to work more efficiently in parallel.\nMerge branches with the main branch via pull request.\nCreate and resolve a merge conflict."
  },
  {
    "objectID": "materials/slides/week1-github.html#setup",
    "href": "materials/slides/week1-github.html#setup",
    "title": "Basic GitHub actions",
    "section": "Setup",
    "text": "Setup\n\nhave everyone open their GitHub client, the sandbox project in RStudio, and the group sandbox repository in the browser on github.com"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#announcementsreminders",
    "href": "materials/slides/week1-perspectives.html#announcementsreminders",
    "title": "On data science",
    "section": "Announcements/reminders",
    "text": "Announcements/reminders\n\nJoin Slack workspace, monitor channel #F23-pstat197a for announcements.\nOffice hours: Mondays 3:30-4:30pm, Old Gym 1201\nInstall course software and bring your laptop to section meetings."
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#google-trends-data-science",
    "href": "materials/slides/week1-perspectives.html#google-trends-data-science",
    "title": "On data science",
    "section": "Google trends: data science",
    "text": "Google trends: data science\n\n\nData science emerged as a term of art in the last decade\nInterest exploded in the last five years"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#origins-data-analysis",
    "href": "materials/slides/week1-perspectives.html#origins-data-analysis",
    "title": "On data science",
    "section": "Origins: ‘data analysis’",
    "text": "Origins: ‘data analysis’\nTukey advocated for ‘data analysis’ as a broader field than statistics (Tukey 1962), including:\n\nstatistical theory and methodology;\nvisualization and data display techniques;\ncomputation and scalability;\nbreadth of application.\n\n\nLook famililar? Tukey’s ‘data analysis’ is proto-modern data science."
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#early-data-analysis-concepts",
    "href": "materials/slides/week1-perspectives.html#early-data-analysis-concepts",
    "title": "On data science",
    "section": "Early data analysis concepts",
    "text": "Early data analysis concepts\nIn the 1960’s and 1970’s, these concepts meant very different things.\n\nvisualization meant drawing\ncomputation meant data re-expression by hand\n\n\nBut the ideas were still somewhat radical. At the time most relied on highly reductive numerical results to interpret data:\n\nANOVA tables\nregression tables\np-values"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#example-boxplots",
    "href": "materials/slides/week1-perspectives.html#example-boxplots",
    "title": "On data science",
    "section": "Example: boxplots",
    "text": "Example: boxplots\n\nFigure from (Tukey et al. 1977)"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#early-data-analysis-concepts-1",
    "href": "materials/slides/week1-perspectives.html#early-data-analysis-concepts-1",
    "title": "On data science",
    "section": "Early data analysis concepts",
    "text": "Early data analysis concepts\nThe new techniques allowed for iterative investigation:\n\nformulate a question\nexamine data graphics and summaries\nadjust computations and graphics to hone in on content of interest\nrefine the question"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#birth-to-death-ratio-by-state",
    "href": "materials/slides/week1-perspectives.html#birth-to-death-ratio-by-state",
    "title": "On data science",
    "section": "Birth-to-death ratio by state",
    "text": "Birth-to-death ratio by state\nSuppose we want to explain variation in birth-to-death ratios in the U.S. 1\n\nInitial question: is population density an associated factor?\n\nThis example follows (Tukey et al. 1977)"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#first-iteration",
    "href": "materials/slides/week1-perspectives.html#first-iteration",
    "title": "On data science",
    "section": "First iteration",
    "text": "First iteration\n\nA first attempt"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#first-iteration-1",
    "href": "materials/slides/week1-perspectives.html#first-iteration-1",
    "title": "On data science",
    "section": "First iteration",
    "text": "First iteration\n\nWhat if we adjust the computation?"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#second-iteration",
    "href": "materials/slides/week1-perspectives.html#second-iteration",
    "title": "On data science",
    "section": "Second iteration",
    "text": "Second iteration\n\nWhat about median age instead?"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#second-iteration-1",
    "href": "materials/slides/week1-perspectives.html#second-iteration-1",
    "title": "On data science",
    "section": "Second iteration",
    "text": "Second iteration\n\nAdjust computations for easy linear approximation"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#third-iteration",
    "href": "materials/slides/week1-perspectives.html#third-iteration",
    "title": "On data science",
    "section": "Third iteration",
    "text": "Third iteration\n\nAre there outliers?"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#fourth-iteration",
    "href": "materials/slides/week1-perspectives.html#fourth-iteration",
    "title": "On data science",
    "section": "Fourth iteration",
    "text": "Fourth iteration\n\nAre outliers spatially correlated?"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#a-bit-of-history",
    "href": "materials/slides/week1-perspectives.html#a-bit-of-history",
    "title": "On data science",
    "section": "A bit of history",
    "text": "A bit of history\nIt’s worth noting that in the first half of the 20th century, much of statistics focused on methodology and theory for the analysis of small iid samples, and in particular:\n\ninference on means and inference on tables;\nanalysis of variance;\ntests of distribution.\n\n\nThe inferential framework brought to bear on these ‘simpler’ problems largely carried over when the field began to specialize."
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#contrasting-approaches",
    "href": "materials/slides/week1-perspectives.html#contrasting-approaches",
    "title": "On data science",
    "section": "Contrasting approaches",
    "text": "Contrasting approaches\nFrom 1960-2010, adopters of the ‘data analysis as a field’ view were largely industry practitioners and applied statisticians who advocated for training and practice that included empirical methods and computation in addition to statistical inference (Donoho 2017).\n\nTheir ideas evolved into an alternative approach to working with data:\n\ndata-driven rather than theory-driven;\niterative rather than conclusive."
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#confirmatory-approach",
    "href": "materials/slides/week1-perspectives.html#confirmatory-approach",
    "title": "On data science",
    "section": "Confirmatory approach",
    "text": "Confirmatory approach\nThe “confirmatory” approach of the classical inferential framework.\n\n\n\n\n\n\n\nconfirm\n\n \n\ncluster_1\n\n data generation  \n\ncluster_2\n\n data analysis  \n\ncluster_3\n\n decision   \n\nsci\n\n domain  knowledge   \n\nhyp\n\n hypotheses   \n\nsci-&gt;hyp\n\n    \n\nexp\n\n designed  experiment   \n\nhyp-&gt;exp\n\n    \n\nmdl\n\n statistical  model   \n\nexp-&gt;mdl\n\n    \n\ndat\n\n data   \n\nexp-&gt;dat\n\n    \n\nyay\n\n supporting  evidence   \n\nmdl-&gt;yay\n\n    \n\nnay\n\n opposing  evidence   \n\nmdl-&gt;nay\n\n    \n\ndat-&gt;yay\n\n    \n\ndat-&gt;nay\n\n   \n\n\n\n\n\n\noutput is a decision\nstatistical model determined by experimental design\nanalysis based on statistical theory"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#exploratory-approach",
    "href": "materials/slides/week1-perspectives.html#exploratory-approach",
    "title": "On data science",
    "section": "Exploratory approach",
    "text": "Exploratory approach\nThe “exploratory” approach of iterative modern data analysis.\n\n\n\n\n\n\n\nexplore\n\n \n\ncluster_1\n\n data analysis  \n\ncluster_2\n\n findings   \n\nsci\n\n domain  knowledge   \n\nq\n\n question  formulation   \n\nsci-&gt;q\n\n    \n\ndat\n\n data   \n\nq-&gt;dat\n\n    \n\nmdl\n\n statistical  model   \n\ndat-&gt;mdl\n\n    \n\nmdl-&gt;q\n\n    \n\nf1\n\n finding 1   \n\nmdl-&gt;f1\n\n    \n\nf2\n\n finding 2   \n\nmdl-&gt;f2\n\n    \n\ndots\n\n ⋮  \n\n\n\n\n\n\noutputs are findings\nstatistical model determined by data\nanalysis techniques include empirical methods"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#drivers-of-change",
    "href": "materials/slides/week1-perspectives.html#drivers-of-change",
    "title": "On data science",
    "section": "Drivers of change",
    "text": "Drivers of change\nIn the 2000s and especially after 2010, the iterative approach enjoys broader applicability than it used to:\n\ndue to automated and/or scalable data collection\n\nobservational data is widely available across domains\nand includes large numbers of variables\n\nhighly specialized data problems evade methodology with theoretical support\nmore accessible to analysts without advanced statistical training"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#machine-learning",
    "href": "materials/slides/week1-perspectives.html#machine-learning",
    "title": "On data science",
    "section": "Machine learning",
    "text": "Machine learning\nMachine learning was largely advanced by computer scientists through 2010 and later (Emmert-Streib et al. 2020), most notably:\n\nneural networks and deep learning\noptimization\nalgorithmic analysis\n\n\nThis was a major driver in advancing modern predictive modeling, and engaging with these tools required going beyond statistics."
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#a-theory-about-data-science",
    "href": "materials/slides/week1-perspectives.html#a-theory-about-data-science",
    "title": "On data science",
    "section": "A theory about data science",
    "text": "A theory about data science\n\nAround mid-century, it was proposed that specialists should be trained in computational as well as statistical methods\nOver time practitioners developed iterative processes for data-driven problem solving that was more flexible than the classical inferential framework\nComputer scientists advanced the field of machine learning substantially\nIterative problem solving together with applied machine learning was well-suited to meet the demands of modern data, but the area was not codified in an academic discipline"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#what-is-research",
    "href": "materials/slides/week1-perspectives.html#what-is-research",
    "title": "On data science",
    "section": "What is research?",
    "text": "What is research?\nResearch is systematic investigation undertaken in order to establish or discover facts.\n\nWhat are facts in data science?\n\nmethod M outperforms method M’ at task T\nwe analyzed data D and reached the conclusion that…"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#the-research-landscape",
    "href": "materials/slides/week1-perspectives.html#the-research-landscape",
    "title": "On data science",
    "section": "The research landscape",
    "text": "The research landscape\nFormal communities – i.e., journals, departments, conferences – have not coalesced around data science research to date.\n\nRelevant research largely occurs in statistics, computer science, and application domains, and can be divided broadly into:\n\nmethodology – creating new techniques to analyze data\napplications – applying existing methods to generate new findings"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#methodological-research",
    "href": "materials/slides/week1-perspectives.html#methodological-research",
    "title": "On data science",
    "section": "Methodological research",
    "text": "Methodological research\nMethodological research might involve:\n\ndesigning a faster algorithm for solving a particular problem\nproposing a new technique for analyzing a particular type of data\ngeneralizing a technique to a broader range of problems"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#applied-research",
    "href": "materials/slides/week1-perspectives.html#applied-research",
    "title": "On data science",
    "section": "Applied research",
    "text": "Applied research\nApplied research might involve:\n\nanalyzing a specific dataset or producing a novel analysis of existing data\ncreating ad-hoc methods for a domain-specific problem\nimporting methodology from another area to bear on a domain-specific problem"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#data-science-capstones",
    "href": "materials/slides/week1-perspectives.html#data-science-capstones",
    "title": "On data science",
    "section": "Data science capstones",
    "text": "Data science capstones\nMost of the time, our data science capstones fall pretty squarely in the applied domain:\n\nsponsor provides data and high-level goals\nstudent team works on producing an analysis or analyses\nmentor advises on methodology"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#relevant-skills",
    "href": "materials/slides/week1-perspectives.html#relevant-skills",
    "title": "On data science",
    "section": "Relevant skills",
    "text": "Relevant skills\nThere are a few avenues to prepare for this sort of work.\n\nWe’ll focus on:\n\nrecognizing problem patterns\ndeveloping a functional view of methodology\ncollaborating efficiently\nindependent learning strategies\nengaging with literature constructively\n\n\n\nIt won’t provide you with exhaustive methodological preparation, but should support you in learning ‘on the job’."
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#reading-responses",
    "href": "materials/slides/week1-perspectives.html#reading-responses",
    "title": "On data science",
    "section": "Reading responses",
    "text": "Reading responses\nQuestions on the perspectives paper (Peng and Parker 2022) to review:\n\nWhat is meant by a ‘systems approach’ to data science?\nWhat is meant by ‘design thinking’ in data science?\n(Why) Are these useful concepts?"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#systems-approach",
    "href": "materials/slides/week1-perspectives.html#systems-approach",
    "title": "On data science",
    "section": "Systems approach",
    "text": "Systems approach\n\n\n\n\nSeveral systems affect the relationship between expected and actual results. Where would you locate them on the figure?\n\nData analytic\nSoftware\nScientific"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#example-systems-for-data-cleaning",
    "href": "materials/slides/week1-perspectives.html#example-systems-for-data-cleaning",
    "title": "On data science",
    "section": "Example systems for data cleaning",
    "text": "Example systems for data cleaning\n\n\nHow might this diagram help an analyst?"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#design-thinking",
    "href": "materials/slides/week1-perspectives.html#design-thinking",
    "title": "On data science",
    "section": "Design thinking",
    "text": "Design thinking\nThe design thinking framework might be summed up:\n\ndata scientists trade in data analyses\na data analysis is a designed product\nthinking about design principles can help make a better product\n\n\nMany of you focused on how design principles are a response to project constraints. Are there other ways a design perspective might be useful?"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#scenario-1",
    "href": "materials/slides/week1-perspectives.html#scenario-1",
    "title": "On data science",
    "section": "Scenario 1",
    "text": "Scenario 1\nYou’re working at a news organization and developing a recommender system for targeted article previews to deploy on the organization’s website. It will show users article previews based on their behavior. Assume you don’t have any significant resource constraints, and can access users’ profiles in full and log interactions in near-real-time.\n\nGoal: show previews most likely to attract interest.\n\n\nConsiderations:\n\nwhat material should be shown in the preview? headlines? images? text?\nwhat behavior can/should be leveraged for the recommender system?\nwhat are a few relevant design aspects of how the system should behave?\nare there ethical concerns?"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#scenario-2",
    "href": "materials/slides/week1-perspectives.html#scenario-2",
    "title": "On data science",
    "section": "Scenario 2",
    "text": "Scenario 2\nYou’re working on a research team studying ecological impacts of land use. The team has access to longitudinal species surveys at locations of interest across the U.S., quarterly county-level land allocation statistics, satellite images, and state budget information for sustainability, restoration, and conservation initiatives.\n\nGoal: identify intervention opportunities that are most likely to positively impact ecological diversity.\n\n\nConsiderations:\n\nwhat data would you use and how would you combine data sources?\nare there external data that might be useful?\nwhat analysis outputs would be most important for identifying intervention opportunities?\ncan you think of other design features that might be useful for the data analysis?"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#a-few-design-principles",
    "href": "materials/slides/week1-perspectives.html#a-few-design-principles",
    "title": "On data science",
    "section": "A few design principles",
    "text": "A few design principles\nLet’s look at some design principles from (McGowan, Peng, and Hicks 2021)."
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#design-principles-matchedness",
    "href": "materials/slides/week1-perspectives.html#design-principles-matchedness",
    "title": "On data science",
    "section": "Design principles: matchedness",
    "text": "Design principles: matchedness"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#design-principles-exhuastiveness",
    "href": "materials/slides/week1-perspectives.html#design-principles-exhuastiveness",
    "title": "On data science",
    "section": "Design principles: exhuastiveness",
    "text": "Design principles: exhuastiveness"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#design-principles-transparency",
    "href": "materials/slides/week1-perspectives.html#design-principles-transparency",
    "title": "On data science",
    "section": "Design principles: transparency",
    "text": "Design principles: transparency"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#design-principles-reproducibility",
    "href": "materials/slides/week1-perspectives.html#design-principles-reproducibility",
    "title": "On data science",
    "section": "Design principles: reproducibility",
    "text": "Design principles: reproducibility"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#next-time",
    "href": "materials/slides/week1-perspectives.html#next-time",
    "title": "On data science",
    "section": "Next time",
    "text": "Next time\nWe’ll do a github icebreaker activity.\n\nComplete lab activity from Tuesday section meeting\nBring laptops"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#references",
    "href": "materials/slides/week1-perspectives.html#references",
    "title": "On data science",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nDonoho, David. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4): 745–66.\n\n\nEmmert-Streib, Frank, Zhen Yang, Han Feng, Shailesh Tripathi, and Matthias Dehmer. 2020. “An Introductory Review of Deep Learning for Prediction Models with Big Data.” Frontiers in Artificial Intelligence 3: 4.\n\n\nMcGowan, Lucy D’Agostino, Roger D Peng, and Stephanie C Hicks. 2021. “Design Principles for Data Analysis.” arXiv Preprint arXiv:2103.05689.\n\n\nPeng, Roger D, and Hilary S Parker. 2022. “Perspective on Data Science.” Annual Review of Statistics and Its Application 9: 1–20.\n\n\nTukey, John W. 1962. “The Future of Data Analysis.” The Annals of Mathematical Statistics 33 (1): 1–67.\n\n\nTukey, John W et al. 1977. Exploratory Data Analysis. Vol. 2. Reading, MA."
  },
  {
    "objectID": "materials/slides/week2-classdata.html#lessons-from-last-time",
    "href": "materials/slides/week2-classdata.html#lessons-from-last-time",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Lessons from last time",
    "text": "Lessons from last time\n\nadd .DS_Store to .gitignore\nopen repo project in RStudio session (not another project or new session)\nrepo clone directory must be kept intact; can move the entire directory but not individual files\nuse client not terminal, at least to start out\nothers?"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#today",
    "href": "materials/slides/week2-classdata.html#today",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Today",
    "text": "Today\n\nreview sampling concepts\nintroduce class survey data\npresent descriptive analysis"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#samples-and-populations",
    "href": "materials/slides/week2-classdata.html#samples-and-populations",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Samples and populations",
    "text": "Samples and populations\npopulation: collection of all subjects/units of interest\nsample: subjects/units observed in a study\n\nstatistical methodology strives to account for the possibility that the sample could have been different in order to make reliable inferences about the population based on knowledge of the sampling mechanism"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#what-if-inferences-arent-possible",
    "href": "materials/slides/week2-classdata.html#what-if-inferences-arent-possible",
    "title": "Sampling concepts and descriptive analysis",
    "section": "What if inferences aren’t possible?",
    "text": "What if inferences aren’t possible?\nEven if inference isn’t possible, data still have value and could be used for:\n\ndescriptive analysis of the sample;\nhypothesis generation;\ndeveloping analysis pipelines."
  },
  {
    "objectID": "materials/slides/week2-classdata.html#what-about-prediction",
    "href": "materials/slides/week2-classdata.html#what-about-prediction",
    "title": "Sampling concepts and descriptive analysis",
    "section": "What about prediction?",
    "text": "What about prediction?\nPrediction is a separate goal but still a form of generalization.\n\nsamples must reflect a broader population for predictions to be accurate at the population level\n\n\n\nif an analyst can’t expect sample statistics to provide reliable estimates of population quantities, they shouldn’t expect predictions based on the sample to be reliable either"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#common-problems",
    "href": "materials/slides/week2-classdata.html#common-problems",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Common problems",
    "text": "Common problems\nSeveral issues arise very often in practice that compromise or complicate an analyst’s ability to make inferences (or predictions). Among them:\n\nscope of inference from the sample doesn’t match the study population\nsubjects/units are selected haphazardly or by convenience\nresearcher conflates sample size with number of observations, i.e., takes lots of measurements on few subjects/units"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#helpful-questions",
    "href": "materials/slides/week2-classdata.html#helpful-questions",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Helpful questions",
    "text": "Helpful questions\nThe following questions can help make an assessment of the scope of inference:\n\n(protocol) how were subjects/units chosen for measurement and how were measurements collected?\n(mechanism) was there any random selection mechanism?\n(exclusion) are there any subjects/units that couldn’t possibly have been chosen?\n(nonresponse) were any subjects/units selected but not measured?"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#class-survey-data",
    "href": "materials/slides/week2-classdata.html#class-survey-data",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Class survey data",
    "text": "Class survey data\n\nsurvey distributed to all students offered enrollment in PSTAT197A fall 2023\n\\(n = 62\\) responses\n\nincludes a few students who did not enroll\ndoes not include several students who did not enroll\n\nno random selection"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#can-the-data-support-inference",
    "href": "materials/slides/week2-classdata.html#can-the-data-support-inference",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Can the data support inference?",
    "text": "Can the data support inference?\nFrom the reading responses:\n\nIt depends on the question. If you want to draw conclusions about the pstat197a class specifically, this sample is the population and thus will have reliable data. If you want to draw conclusions about the pstat department as a whole, then this is a bad sample because it is likely biased and thus unreliable"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#alternative-perspectives",
    "href": "materials/slides/week2-classdata.html#alternative-perspectives",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Alternative perspectives",
    "text": "Alternative perspectives\nThe comment points to two ways to view the data:\n\na census of PSTAT197A enrollees\na convenience sample of…\n\ncapstone applicants OR\nstudents qualified for capstones OR\nstudents interested in data science OR\nall UCSB students???"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#is-there-a-right-answer",
    "href": "materials/slides/week2-classdata.html#is-there-a-right-answer",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Is there a right answer?",
    "text": "Is there a right answer?\nEither way – census or convenience sample – excludes inference.\n\ncensus \\(\\longrightarrow\\) no inference needed\nconvenience \\(\\longrightarrow\\) no inference possible\n\n\nSo on a practical level, it won’t make much difference for designing an analysis of the survey data."
  },
  {
    "objectID": "materials/slides/week2-classdata.html#descriptive-analysis",
    "href": "materials/slides/week2-classdata.html#descriptive-analysis",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Descriptive analysis",
    "text": "Descriptive analysis\nAny analysis of survey data should be regarded as descriptive in nature:\n\nsummary statistics and/or models are not reliable measures of any broader population\nresults should be interpreted narrowly in terms of the sample at hand"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#a-general-approach",
    "href": "materials/slides/week2-classdata.html#a-general-approach",
    "title": "Sampling concepts and descriptive analysis",
    "section": "A general approach",
    "text": "A general approach\nStart simple and add complexity gradually.\nFrom simpler to more complex consider questions involving:\n\nSample characteristics\nSingle-variable summaries\nMultivariate summaries\nModel-based outputs (estimates, predictions, etc.)"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#questions-of-interest",
    "href": "materials/slides/week2-classdata.html#questions-of-interest",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Questions of interest",
    "text": "Questions of interest\nSample characteristics\n\nIs the proportion of men/women in the class equal (taking into account randomness)?\n\nSingle-variable summaries\n\nAmong the students offered a seat in PSTAT197, what data science classes are the students most interested in?\nWhat level of comfort do students interested in data analysis at UCSB have with mathematics?"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#questions-of-interest-1",
    "href": "materials/slides/week2-classdata.html#questions-of-interest-1",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Questions of interest",
    "text": "Questions of interest\nMultivariate summaries\n\nAre students who ranked themselves as strong in statistics, mathematics, and computing more likely or less likely to select an ‘industry’ project as the project type that they want to work on?\n\nModel-based outputs\n\nAre there distinct groups of students in the class defined by self-assessed proficiencies and/or comfort levels with mathematics, statistics, and programming?"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#sample-characteristics",
    "href": "materials/slides/week2-classdata.html#sample-characteristics",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Sample characteristics",
    "text": "Sample characteristics\nIs the proportion of men/women in the class equal (taking into account randomness)?\n\nClass standingGenderRaceData sharing\n\n\n\n\n\n\n\nstanding\nn\n\n\n\n\nJunior\n7\n\n\nSenior\n55\n\n\n\n\n\n\n\n\n\n\n\n\ngender\nn\n\n\n\n\nFemale\n28\n\n\nMale\n32\n\n\nPrefer not to say\n2\n\n\n\n\n\n\n\n\n\n\n\n\nrace\nn\n\n\n\n\nAmerican Indian or Alaska Native\n1\n\n\nAsian\n50\n\n\nCaucasian\n6\n\n\nPrefer not to say\n2\n\n\nUnknown\n3\n\n\n\n\n\n\n\nColumns: consent to share project preferences\nRows: consent to share background and preparation\n\n\n\n\n\nconsent\nNo\nYes\n\n\n\n\nNo\n6\n1\n\n\nYes\n2\n53"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#majors",
    "href": "materials/slides/week2-classdata.html#majors",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Majors",
    "text": "Majors"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#response-timing",
    "href": "materials/slides/week2-classdata.html#response-timing",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Response timing",
    "text": "Response timing"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#privacy",
    "href": "materials/slides/week2-classdata.html#privacy",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Privacy",
    "text": "Privacy\nThe following information have been removed from the dataset that will be distributed to the class:\n\npersonal information from section 1 of the survey\nlong text and free response answers, contain some personal details\nresponses from students who did not consent to share\ntype distinction between research experiences"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#single-variable-summaries",
    "href": "materials/slides/week2-classdata.html#single-variable-summaries",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Single-variable summaries",
    "text": "Single-variable summaries\nWhat level of comfort do students interested in data analysis at UCSB have with mathematics?\n\nComfortProficiency (numeric)Proficiency (factor)\n\n\n\n\n\n\n\nvariable\nmax\nmean\nmedian\nmin\n\n\n\n\nmath.comf\n5\n3.903226\n4\n1\n\n\nprog.comf\n5\n4.032258\n4\n2\n\n\nstat.comf\n5\n4.145161\n4\n1\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\n\n\n\n\nmath\n2.355932\n2\n\n\nprog\n2.237288\n2\n\n\nstat\n2.576271\n3\n\n\n\n\n\n\n\n\n\n\n\n\nprog\nn1\nmath\nn2\nstat\nn3\n\n\n\n\nBeg\n3\nBeg\n3\nBeg\n2\n\n\nInt\n39\nInt\n32\nInt\n21\n\n\nAdv\n17\nAdv\n24\nAdv\n36"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#multivariable-summaries",
    "href": "materials/slides/week2-classdata.html#multivariable-summaries",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Multivariable summaries",
    "text": "Multivariable summaries\nAre students who ranked themselves as strong in statistics, mathematics, and computing more likely or less likely to select an ‘industry’ project as the project type that they want to work on?\n\nCountsProportions\n\n\n\n\n\n\n\nmean.proficiency.fac\nboth\nind\nlab\n\n\n\n\n[1,2.33]\n6\n25\n1\n\n\n(2.33,2.67]\n5\n9\n1\n\n\n(2.67,3]\n3\n6\n1\n\n\n\n\n\n\n\n\n\n\n\n\nmean.proficiency.fac\nboth\nind\nlab\nn\n\n\n\n\n[1,2.33]\n0.188\n0.781\n0.031\n32\n\n\n(2.33,2.67]\n0.333\n0.600\n0.067\n15\n\n\n(2.67,3]\n0.300\n0.600\n0.100\n10"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#combinations",
    "href": "materials/slides/week2-classdata.html#combinations",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Combinations",
    "text": "Combinations\nConsider the distinct combinations of comfort and proficiency ratings (separately):\n\nProficiencyComfort\n\n\n\n\n\n\n\nprog\nmath\nstat\nn\n\n\n\n\n1\n1\n1\n1\n\n\n1\n2\n2\n1\n\n\n1\n2\n3\n1\n\n\n2\n1\n1\n1\n\n\n2\n1\n2\n1\n\n\n2\n2\n2\n13\n\n\n2\n2\n3\n11\n\n\n2\n3\n2\n3\n\n\n2\n3\n3\n10\n\n\n3\n2\n2\n2\n\n\n3\n2\n3\n4\n\n\n3\n3\n2\n1\n\n\n3\n3\n3\n10\n\n\n\n\n\n\n\n\n\n\n\n\nprog\nmath\nstat\nn\n\n\n\n\n3\n2\n2\n1\n\n\n3\n3\n3\n2\n\n\n3\n3\n4\n2\n\n\n3\n3\n5\n2\n\n\n3\n4\n3\n4\n\n\n3\n4\n4\n7\n\n\n4\n3\n3\n2\n\n\n4\n3\n4\n5\n\n\n4\n3\n5\n2\n\n\n4\n4\n3\n1\n\n\n4\n4\n4\n5\n\n\n4\n4\n5\n3\n\n\n4\n5\n3\n1\n\n\n4\n5\n4\n4\n\n\n4\n5\n5\n2\n\n\n5\n3\n4\n5\n\n\n5\n3\n5\n1\n\n\n5\n4\n4\n2\n\n\n5\n4\n5\n1\n\n\n5\n5\n4\n1\n\n\n5\n5\n5\n6"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#clustering",
    "href": "materials/slides/week2-classdata.html#clustering",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Clustering",
    "text": "Clustering\nCan students be grouped based on combinations of preferences and comfort levels?\n\nCentersVisualizationMethodInterpretation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprog.prof\nmath.prof\nstat.prof\nprog.comf\nmath.comf\nstat.comf\nsize\ncluster\n\n\n\n\n2.478\n2.739\n2.957\n4.435\n4.522\n4.522\n23\n1\n\n\n2.048\n1.857\n2.238\n4.048\n3.048\n4.048\n21\n2\n\n\n2.133\n2.467\n2.467\n3.133\n3.933\n3.467\n15\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustering method, “k means”, groups data by nearest Euclidean distance to each of \\(k\\) centers. \\(k\\) is user-specified; the method finds the centers that minimize within-cluster variance.\n\n\nBased on the centers:\n\nCluster 1: advanced proficiency, very comfortable\nCluster 2: intermediate with less mathematical preparation\nCluster 3: intermediate with less programming preparation"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#assignment",
    "href": "materials/slides/week2-classdata.html#assignment",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Assignment",
    "text": "Assignment\nYour task is to extend this analysis with your group by next Tuesday.\n\nHere are some ideas:\n\nexplore variable associations further (e.g., coursework and self-evaluations)\nexperiment with clustering on different variable subsets or using different methods\nsummarize domain or area of interest variables (requires some text manipulation)"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#next-time",
    "href": "materials/slides/week2-classdata.html#next-time",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Next time",
    "text": "Next time\nMost of next meeting we’ll devote to planning your group’s task.\n\nDo a little brainstorming on your own\nCome with a few questions/ideas"
  },
  {
    "objectID": "materials/slides/week2-workshop.html#today",
    "href": "materials/slides/week2-workshop.html#today",
    "title": "Group assignment workshops",
    "section": "Today",
    "text": "Today\n\nsetup for first group assignment due next Thursday\n\nassignment objectives and instructions\nreview repository\n\nworkshop ideas and plan tasks in groups"
  },
  {
    "objectID": "materials/slides/week2-workshop.html#assignment-objective",
    "href": "materials/slides/week2-workshop.html#assignment-objective",
    "title": "Group assignment workshops",
    "section": "Assignment objective",
    "text": "Assignment objective\nYour task: prepare and present a descriptive analysis of the survey responses addressing 2-3 questions or goals of your choosing.\n\nquestions or goals should be of moderate complexity; easy to state and understand but should require a little work to answer\n\ntoo simple: what proportion of students have research experience?\nbetter: are students with research experience more confident/comfortable with technical skills than students without, considering coursework history?"
  },
  {
    "objectID": "materials/slides/week2-workshop.html#outcomes",
    "href": "materials/slides/week2-workshop.html#outcomes",
    "title": "Group assignment workshops",
    "section": "Outcomes",
    "text": "Outcomes\nThe learning outcomes for this assignment are less focused on methodology:\n\nformulate questions and plan a simple analysis\npractice using a GitHub repo in a team project setting\nprepare a report\nlearn one or more new-to-you data manipulation techniques"
  },
  {
    "objectID": "materials/slides/week2-workshop.html#assignment-set-up",
    "href": "materials/slides/week2-workshop.html#assignment-set-up",
    "title": "Group assignment workshops",
    "section": "Assignment set-up",
    "text": "Assignment set-up\n\naccept GH classroom assignment here; this will create/add your team repo\nclone repo to local\nreview repo contents:\n\ndata with survey responses and metadata\nscripts with preprocessing and in-class analysis\nresults with report template for preparing write-up\nREADME.md with assignment instructions"
  },
  {
    "objectID": "materials/slides/week2-workshop.html#deliverable",
    "href": "materials/slides/week2-workshop.html#deliverable",
    "title": "Group assignment workshops",
    "section": "Deliverable",
    "text": "Deliverable\nYour ‘submission’ will be in the form of commits to the group repository, in particular:\n\nan updated results/report.qmd file containing your write-up source\na rendered results/report.html file\n\n\nPlease make final commits by Thursday, October 26, 11:59pm PST.\n\n\nRecall there is a one-hour grace period; any commits submitted after Friday 1:00am will not receive review."
  },
  {
    "objectID": "materials/slides/week2-workshop.html#resources",
    "href": "materials/slides/week2-workshop.html#resources",
    "title": "Group assignment workshops",
    "section": "Resources",
    "text": "Resources\n\nlab 2 and in-class analysis scripts/week2-surveys.R\nslack and course staff OH\nMDSR ch. 4 on data wrangling\nRStudio cheatsheets and tidyverse documentation, esp. dplyr, tidyr, ggplot2"
  },
  {
    "objectID": "materials/slides/week2-workshop.html#workshopping-today",
    "href": "materials/slides/week2-workshop.html#workshopping-today",
    "title": "Group assignment workshops",
    "section": "Workshopping today",
    "text": "Workshopping today\nTry to accomplish three goals:\n\npool ideas for questions or themes to explore in the data\npair up, divide work, and assign tasks\nagree on a communication plan for finishing work\n\nslack groupchat or similar\nmeeting outside of class sometime"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#announcementsreminders",
    "href": "materials/slides/week3-biomarkers.html#announcementsreminders",
    "title": "Multiple testing corrections",
    "section": "Announcements/reminders",
    "text": "Announcements/reminders\n\ndon’t forget to fill out attendance form for each class meeting\n\nbut don’t fill it out if you don’t come to class\n\nfirst group assignment due Friday 10/14 11:59pm PST\n\nplease add your lab scripts from labs 1, 2\nlabs/labN-TITLE-USERNAME.R\n\nsection attendance is expected"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#background",
    "href": "materials/slides/week3-biomarkers.html#background",
    "title": "Multiple testing corrections",
    "section": "Background",
    "text": "Background\nLevels of proteins in plasma/serum are altered in autism spectrum disorder (ASD).\n\nGoal: identify a panel of proteins useful as a blood biomarker for early detection of ASD.\n\na ‘panel’ is a handful of tests that help distinguish between conditions\nso in other words, find proteins whose serum levels are predictive of ASD"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#dataset",
    "href": "materials/slides/week3-biomarkers.html#dataset",
    "title": "Multiple testing corrections",
    "section": "Dataset",
    "text": "Dataset\nData from Hewitson et al. (2021)\n\nSerum samples from 76 boys with ASD and 78 typically developing (TD) boys, 18 months-8 years of age\nA total of 1,125 proteins were analyzed from each sample\n\n1,317 measured, 192 failed quality control\n(we don’t know which ones failed QC so will use all)"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#sample-characteristics",
    "href": "materials/slides/week3-biomarkers.html#sample-characteristics",
    "title": "Multiple testing corrections",
    "section": "Sample characteristics",
    "text": "Sample characteristics\n\nAgeDemographicsComorbiditiesMedications\n\n\n\n\n\n\n\nAttribute\nASD (n = 76)\nTD (n = 78)\n\n\n\n\nAge: mean (SD) years\n5.6 (1.7)\n5.7 (2.0)\n\n\n\n\n\n\n\n\n\n\n\n\nAttribute\nASD (n = 76)\nTD (n = 78)\n\n\n\n\nWhite/Caucasian\n33 (45.2%)\n40 (51.9%)\n\n\nHispanic/Latino\n26 (35.6%)\n6 (7.8%)\n\n\nAfrican American/Black\n3 (4.1%)\n14 (18.2%)\n\n\nAsian or Pacific Islander\n2 (2.6%)\n3 (3.9%)\n\n\nMultiple ethnicities or Other\n9 (12.3%)\n14 (18.2%)\n\n\nNot reported\n3 (4.1%)\n1 (1.2%)\n\n\n\n\n\n\n\n\n\n\n\n\nAttribute\nASD (n = 76)\nTD (n = 78)\n\n\n\n\nNone\n38 (52.8%)\n58 (75.3%)\n\n\nADHD\n2 (2.8%)\n1 (1.3%)\n\n\nSeasonal Allergies\n30 (41.7%)\n17 (22.4%)\n\n\nAsthma\n2 (2.8%)\n0 (0%)\n\n\nCeliac Disease\n1 (1.4%)\n0 (0%)\n\n\nGERD\n1 (1.4%)\n0 (0%)\n\n\nPTSD\n0 (0%)\n1 (1.3%)\n\n\nSleep Apnea\n2 (2.8%)\n0 (0%)\n\n\nNot reported\n4 (5.6%)\n1 (1.3%)\n\n\n\n\n\n\n\n\n\n\n\n\nAttribute\nASD (n = 76)\nTD (n = 78)\n\n\n\n\nNone\n69 (92%)\n75 (97.4%)\n\n\nAnti-depressant\n2 (2.7%)\n0 (0%)\n\n\nAnti-psychotic\n0 (0%)\n1 (1.3%)\n\n\nSedative\n1 (1.3%)\n0 (0%)\n\n\nSSRI\n2 (2.27%)\n0 (0%)\n\n\nStimulant\n1 (1.3%)\n1 (1.3%)\n\n\nNot reported\n1 (1.3%)\n1 (1.3%)"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#data-glimpse",
    "href": "materials/slides/week3-biomarkers.html#data-glimpse",
    "title": "Multiple testing corrections",
    "section": "Data glimpse",
    "text": "Data glimpse\n\nExample rowsGroup sizes\n\n\n\nasd_clean %>% head(5)\n\n# A tibble: 5 × 1,318\n  group    CHIP  CEBPB     NSE   PIAS4 `IL-10 Ra`  STAT3   IRF1 `c-Jun` `Mcl-1`\n  <chr>   <dbl>  <dbl>   <dbl>   <dbl>      <dbl>  <dbl>  <dbl>   <dbl>   <dbl>\n1 ASD    0.335   0.520 -0.554   0.650      -0.358  0.305 -0.484   0.309  1.57  \n2 ASD   -0.0715  1.01   3       1.28       -0.133  1.13   0.253   0.408  0.0643\n3 ASD   -0.406  -0.531 -0.0592  1.13        0.554 -0.334  0.287  -0.845  1.42  \n4 ASD   -0.102  -0.251  1.47    0.0773     -0.705  0.893  2.61   -0.372 -0.467 \n5 ASD   -0.395  -0.536  0.0410 -0.299      -0.830  0.899  1.01   -0.843 -1.15  \n# … with 1,308 more variables: OAS1 <dbl>, `c-Myc` <dbl>, SMAD3 <dbl>,\n#   SMAD2 <dbl>, `IL-23` <dbl>, PDGFRA <dbl>, `IL-12` <dbl>, STAT1 <dbl>,\n#   STAT6 <dbl>, LRRK2 <dbl>, Osteocalcin <dbl>, `IL-5` <dbl>, GPDA <dbl>,\n#   IgA <dbl>, LPPL <dbl>, HEMK2 <dbl>, PDXK <dbl>, TLR4 <dbl>, REG4 <dbl>,\n#   `HSP 27` <dbl>, `YKL-40` <dbl>, `Alpha enolase` <dbl>, `Apo L1` <dbl>,\n#   CD38 <dbl>, CD59 <dbl>, FABPL <dbl>, `GDF-11` <dbl>, BTC <dbl>,\n#   `HIF-1a` <dbl>, S100A6 <dbl>, SECTM1 <dbl>, RSPO3 <dbl>, PSP <dbl>, …\n\n\n\n\n\n\n# A tibble: 2 × 2\n  group     n\n  <chr> <int>\n1 ASD      76\n2 TD       78"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#module-objectives",
    "href": "materials/slides/week3-biomarkers.html#module-objectives",
    "title": "Multiple testing corrections",
    "section": "Module objectives",
    "text": "Module objectives\nMethodology\n\nmultiple testing\nclassification: logistic regression; random forests\nvariable selection: LASSO regularization\nclassification accuracy measures\n\n\nConcepts\n\ndata partitioning for predictive modeling\nmodel interpretability\nhigh dimensional data \\(n < p\\)"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#marginal-differences",
    "href": "materials/slides/week3-biomarkers.html#marginal-differences",
    "title": "Multiple testing corrections",
    "section": "Marginal differences",
    "text": "Marginal differences\nIdea: test for a significant difference in serum levels between groups for a given protein, say protein \\(i\\).\n\nNotation:\n\n\\(\\mu^i_{ASD}\\): mean serum level of protein \\(i\\) in the ASD group\n\\(\\mu^i_{TD}\\): mean serum level of protein \\(i\\) in the TD group\n\\(\\delta_i\\): difference in means \\(\\mu^i_{ASD} - \\mu^i_{TD}\\)\nhats indicate sample estimates (e.g. \\(\\hat{\\delta}_i\\))"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#review-t-test",
    "href": "materials/slides/week3-biomarkers.html#review-t-test",
    "title": "Multiple testing corrections",
    "section": "Review: \\(t\\)-test",
    "text": "Review: \\(t\\)-test\nThe \\(t\\)-test tests \\(H_{0i}: \\delta_i = 0\\) against its negation \\(\\neg H_{0i}: \\delta_i \\neq 0\\) using the rule\n\\[\n\\text{reject $H_{0i}$ if}\\qquad \\left|\\frac{\\hat{\\delta}_i}{SE(\\hat{\\delta}_i)}\\right| > t_\\alpha\n\\]\n\n\\(SE(\\hat{\\delta}_i)\\) is a standard error for the difference estimate; quantifies variability of the estimate\nprocedure controls type I error at \\(\\alpha\\), ensuring \\(P\\left(\\text{reject}_i|H_i\\right) \\leq 0.05\\)"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#review-p-values",
    "href": "materials/slides/week3-biomarkers.html#review-p-values",
    "title": "Multiple testing corrections",
    "section": "Review: \\(p\\)-values",
    "text": "Review: \\(p\\)-values\nThe \\(p\\)-value for a test is the probability of obtaining a sample at least as contrary to \\(H_{0i}\\) as the sample in hand, assuming \\(H_{0i}\\) is true.\n\nBy construction, \\(p < \\alpha\\) just in case the test rejects with type I error controlled at \\(\\alpha\\).\n\n\nSo a common heuristic is:\n\\[\n\\text{reject $H_{0i}$ if} \\qquad p_i \\leq \\alpha\n\\]"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#one-test",
    "href": "materials/slides/week3-biomarkers.html#one-test",
    "title": "Multiple testing corrections",
    "section": "One test",
    "text": "One test\nHere is R output for one test.\n\nasd %>%\n  t_test(formula = CHIP ~ group,\n         order = c('ASD', 'TD'),\n         alternative = 'two-sided',\n         var.equal = F)\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      <dbl> <dbl>   <dbl> <chr>          <dbl>    <dbl>    <dbl>\n1     0.927  75.7   0.357 two.sided       384.    -441.    1210.\n\n\n\nQuestions:\n\nWhat are the hypotheses in words?\nWhat are the test assumptions?\nWhat is the conclusion of the test?"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#many-tests",
    "href": "materials/slides/week3-biomarkers.html#many-tests",
    "title": "Multiple testing corrections",
    "section": "Many tests",
    "text": "Many tests\nA plausible approach for identifying a protein panel, then, is to select all those proteins for which the \\(t\\)-test indicates a significant difference.\n\n1,317 tests\neasy to compute\nconceptually straightforward\n\n\nHow likely are mistakes?"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#test-outcomes",
    "href": "materials/slides/week3-biomarkers.html#test-outcomes",
    "title": "Multiple testing corrections",
    "section": "Test outcomes",
    "text": "Test outcomes\nLet \\(H_i\\) denote the \\(i\\)th null hypothesis and \\(R_i\\) denote the event that \\(H_i\\) is rejected.\n\n\n\n\n\n\\(H_i\\)\n\\(\\neg H_i\\)\n\n\n\n\n\\(R_i\\)\n\\(V\\) false rejections\n\\(S\\) correct\n\n\n\\(\\neg R_i\\)\n\\(T\\) correct\n\\(W\\) false non-rejections\n\n\n\n\n\nThe multiple testing problem is that individual error rates compound over multiple tests."
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#familywise-error",
    "href": "materials/slides/week3-biomarkers.html#familywise-error",
    "title": "Multiple testing corrections",
    "section": "Familywise error",
    "text": "Familywise error\nFamilywise error rate (FWER) is the probability of one or more type I errors: \\(P(V \\geq 1)\\).\n\nSuppose there are \\(m\\) true hypotheses \\(\\mathcal{H}: \\{H_i: i \\in C\\}\\).\n\n\nIf the tests are independent and exact then:\n\\[\n\\begin{aligned}\nP(V \\geq 1)\n&= P\\left[ \\bigcup_{i \\in C} R_i | \\mathcal{H} \\right] \\\\\n&= 1 - \\prod_{i \\in C} \\left( 1- P(R_i|H_i) \\right) \\\\\n&= 1 - (1 - \\alpha)^m\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#fwer-example",
    "href": "materials/slides/week3-biomarkers.html#fwer-example",
    "title": "Multiple testing corrections",
    "section": "FWER Example",
    "text": "FWER Example\nIf individual tests are exactly controlled at \\(\\alpha = 0.05\\) and independent, at least one error is nearly certain by 100 tests.\n\nFamilywise error rate as a function of the number of tests, assuming tests are independent with exact type I error 0.05."
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#bonferroni-correction",
    "href": "materials/slides/week3-biomarkers.html#bonferroni-correction",
    "title": "Multiple testing corrections",
    "section": "Bonferroni correction",
    "text": "Bonferroni correction\nThe simplest multiple testing correction is based on the Bonferroni inequality:\n\\[\nP\\left[ \\bigcup_{i \\in C} R_i | \\mathcal{H} \\right] \\leq \\sum_{i \\in C} P(R_i|\\mathcal{H})\n\\]\n\nIf the individual tests are controlled at level \\(\\alpha\\), then \\(FWER \\leq m\\alpha\\).\n\n\nSo a simple solution is to test at level \\(\\alpha^* = \\frac{\\alpha}{m}\\).\n\n\nIn other words, reject if \\(p_i < \\frac{\\alpha}{m}\\)."
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#false-discovery-rate",
    "href": "materials/slides/week3-biomarkers.html#false-discovery-rate",
    "title": "Multiple testing corrections",
    "section": "False discovery rate",
    "text": "False discovery rate\nFWER control will limit false rejections, but at the cost of power; controlling the probability of one type I error is a conservative approach.\n\nMore common in modern applications are procedures to control false discovery rate: the expected proportion of rejections that are false.\n\\[\n\\text{FDR} = \\mathbb{E}\\left[\\frac{\\text{false rejections}}{\\text{total rejections}}\\right]\n\\]\n\n\nConceptually, if say FDR is controlled at \\(0.05\\), then one would expect 5% of rejections to be false."
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#benjamini-hochberg-correction",
    "href": "materials/slides/week3-biomarkers.html#benjamini-hochberg-correction",
    "title": "Multiple testing corrections",
    "section": "Benjamini-Hochberg correction",
    "text": "Benjamini-Hochberg correction\nBenjamini and Hochberg (1995) conceived a procedure based on sorting \\(p\\)-values.\n\nSupposing \\(m\\) independent tests are performed:\n\nSort the \\(p\\)-values in increasing order \\(p_{(1)}, p_{(2)}, \\dots, p_{(m)}\\)\nReject whenever \\(p_{(i)} < \\frac{i\\alpha}{m}\\)\n\n\n\nThey proved that this controls FDR at \\(\\alpha\\)."
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#benjamini-yekutieli-correction",
    "href": "materials/slides/week3-biomarkers.html#benjamini-yekutieli-correction",
    "title": "Multiple testing corrections",
    "section": "Benjamini-Yekutieli correction",
    "text": "Benjamini-Yekutieli correction\nThe Benjamini-Hochberg assumes tests are independent, which is obviously not true in most situations. (Why?)\n\nBenjamini and Yekutieli (2001) modified the correction to hold without the independence assumption:\n\nSort the \\(p\\)-values in increasing order \\(p_{(1)}, p_{(2)}, \\dots, p_{(m)}\\)\nReject whenever \\(p_{(i)} < \\frac{i\\alpha}{m H_m}\\)\n\n\n\nAbove, \\(H_m = \\sum_{i = 1}^m \\frac{1}{i}\\) ."
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#implementing-corrections",
    "href": "materials/slides/week3-biomarkers.html#implementing-corrections",
    "title": "Multiple testing corrections",
    "section": "Implementing corrections",
    "text": "Implementing corrections\nThe easiest way to implement these corrections is to adjust the \\(p\\)-values with a multiplier:\n\n(Bonferroni) \\(p^b_i = m\\times p_i\\)\n(Benjamini-Hochberg) \\(p^{bh}_{(i)} = \\frac{m}{i} p_{(i)}\\)\n(Benjamini-Yekuteili) \\(p^{bh}_{(i)} = \\frac{m H_m}{i} p_{(i)}\\)"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#computations",
    "href": "materials/slides/week3-biomarkers.html#computations",
    "title": "Multiple testing corrections",
    "section": "Computations",
    "text": "Computations\n\nPreprocessingAssumptionsTestsCorrections\n\n\n\ntrim_fn <- function(x){\n  x[x > 3] <- 3\n  x[x < -3] <- -3\n  \n  return(x)\n}\n\nasd_clean <- asd %>% \n  select(-ados) %>%\n  # log transform\n  mutate(across(.cols = -group, log10)) %>%\n  # center and scale\n  mutate(across(.cols = -group, ~ scale(.x)[, 1])) %>%\n  # trim outliers (affects results??)\n  mutate(across(.cols = -group, trim_fn))\n\nasd_nested <- asd_clean %>%\n  pivot_longer(-group, \n               names_to = 'protein', \n               values_to = 'level') %>%\n  nest(data = c(level, group))\n\nasd_nested %>% head(4)\n\n# A tibble: 4 × 2\n  protein data              \n  <chr>   <list>            \n1 CHIP    <tibble [154 × 2]>\n2 CEBPB   <tibble [154 × 2]>\n3 NSE     <tibble [154 × 2]>\n4 PIAS4   <tibble [154 × 2]>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# compute for several groups\ntest_fn <- function(.df){\n  t_test(.df, \n         formula = level ~ group,\n         order = c('ASD', 'TD'),\n         alternative = 'two-sided',\n         var.equal = F)\n}\n\ntt_out <- asd_nested %>%\n  mutate(ttest = map(data, test_fn)) %>%\n  unnest(ttest) %>%\n  arrange(p_value)\n\ntt_out %>% head(5)\n\n# A tibble: 5 × 9\n  protein     data     statistic  t_df     p_value alternative estimate lower_ci\n  <chr>       <list>       <dbl> <dbl>       <dbl> <chr>          <dbl>    <dbl>\n1 DERM        <tibble>     -6.10  151.     8.27e-9 two.sided     -0.885    -1.17\n2 RELT        <tibble>     -5.65  152.     7.82e-8 two.sided     -0.775    -1.05\n3 FSTL1       <tibble>     -5.27  152.     4.66e-7 two.sided     -0.783    -1.08\n4 C1QR1       <tibble>     -5.26  152.     4.79e-7 two.sided     -0.782    -1.08\n5 Calcineurin <tibble>     -5.24  151.     5.37e-7 two.sided     -0.734    -1.01\n# … with 1 more variable: upper_ci <dbl>\n\n\n\n\n\n# multiple testing corrections\nm <- nrow(tt_out)\nhm <- log(m) + 1/(2*m) - digamma(1)\n  \ntt_corrected <- tt_out %>%\n  select(data, protein, p_value) %>%\n  mutate(rank = row_number()) %>%\n  mutate(p_bh = p_value*m/rank,\n         p_by = p_value*m*hm/rank,\n         p_bonf = p_value*m)\n\ntt_corrected %>% head(5)\n\n# A tibble: 5 × 7\n  data               protein           p_value  rank      p_bh      p_by  p_bonf\n  <list>             <chr>               <dbl> <int>     <dbl>     <dbl>   <dbl>\n1 <tibble [154 × 2]> DERM        0.00000000827     1 0.0000109 0.0000845 1.09e-5\n2 <tibble [154 × 2]> RELT        0.0000000782      2 0.0000515 0.000400  1.03e-4\n3 <tibble [154 × 2]> FSTL1       0.000000466       3 0.000205  0.00159   6.14e-4\n4 <tibble [154 × 2]> C1QR1       0.000000479       4 0.000158  0.00122   6.31e-4\n5 <tibble [154 × 2]> Calcineurin 0.000000537       5 0.000141  0.00110   7.07e-4"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#results",
    "href": "materials/slides/week3-biomarkers.html#results",
    "title": "Multiple testing corrections",
    "section": "Results",
    "text": "Results\n\nComparing methodsTop 10 proteins\n\n\n\n\n\n\n\nAdjusted vs. raw p-values for each multiple correction method.\n\n\n\n\n\n\n\n# top 10\ntt_corrected %>%\n  select(protein, p_by) %>%\n  slice_min(order_by = p_by, n = 10)\n\n# A tibble: 10 × 2\n   protein              p_by\n   <chr>               <dbl>\n 1 DERM            0.0000845\n 2 RELT            0.000400 \n 3 Calcineurin     0.00110  \n 4 C1QR1           0.00122  \n 5 MRC2            0.00132  \n 6 IgD             0.00136  \n 7 CXCL16, soluble 0.00149  \n 8 PTN             0.00154  \n 9 FSTL1           0.00159  \n10 Cadherin-5      0.00179"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#neat-graphic-volcano-plot",
    "href": "materials/slides/week3-biomarkers.html#neat-graphic-volcano-plot",
    "title": "Multiple testing corrections",
    "section": "Neat graphic: volcano plot",
    "text": "Neat graphic: volcano plot\n\nUpregulation and downregulation of serum levels of proteins analyzed – p-values against number of doublings (positive) or halvings (negative) of serum level in ASD group relative to TD group."
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#next-time",
    "href": "materials/slides/week3-biomarkers.html#next-time",
    "title": "Multiple testing corrections",
    "section": "Next time",
    "text": "Next time\nOther approaches to the same problem:\n\ncorrelation with ADOS (severity diagnostic score)\nvariable importance in random forest classifier"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#references",
    "href": "materials/slides/week3-biomarkers.html#references",
    "title": "Multiple testing corrections",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nBenjamini, Yoav, and Yosef Hochberg. 1995. “Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing.” Journal of the Royal Statistical Society: Series B (Methodological) 57 (1): 289–300.\n\n\nBenjamini, Yoav, and Daniel Yekutieli. 2001. “The Control of the False Discovery Rate in Multiple Testing Under Dependency.” Annals of Statistics, 1165–88.\n\n\nHewitson, Laura, Jeremy A Mathews, Morgan Devlin, Claire Schutte, Jeon Lee, and Dwight C German. 2021. “Blood Biomarker Discovery for Autism Spectrum Disorder: A Proteomic Analysis.” PLoS One 16 (2): e0246581."
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#announcementsreminders",
    "href": "materials/slides/week3-randomforest.html#announcementsreminders",
    "title": "Random forests",
    "section": "Announcements/reminders",
    "text": "Announcements/reminders\nMake your final commits for the first group assignment by Friday 10/14 11:59pm PST\n\nshould include an updated report.qmd and report.html with your write-up\n\n\nNext group assignment to be distributed Tuesday 10/18.\n\nGroups will be randomly assigned\nTask: replicate and redesign proteomic analysis"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#last-time",
    "href": "materials/slides/week3-randomforest.html#last-time",
    "title": "Random forests",
    "section": "Last time",
    "text": "Last time\n\nintroduced ASD proteomic dataset\nused multiple testing corrections to identify proteins whose serum levels differ significantly between ASD/TD groups\ndiscussed the difference between controlling familywise error vs. false discovery"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#today",
    "href": "materials/slides/week3-randomforest.html#today",
    "title": "Random forests",
    "section": "Today",
    "text": "Today\nWe’ll talk about two more approaches to identifying proteins of interest:\n\ncorrelation-based identification of proteins\nrandom forest classifier"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#ados-score",
    "href": "materials/slides/week3-randomforest.html#ados-score",
    "title": "Random forests",
    "section": "ADOS score",
    "text": "ADOS score\n\n\n\nAutism Diagnostic Observation Schedule (ADOS) scores are determined by psychological assessment and measure ASD severity.\n\nasd %>% \n  select(group, ados) %>% \n  group_by(group) %>% \n  sample_n(size = 2)\n\n# A tibble: 4 × 2\n# Groups:   group [2]\n  group  ados\n  <chr> <dbl>\n1 ASD      19\n2 ASD      12\n3 TD       NA\n4 TD       NA\n\n\n\nados is only measured for the ASD group\nnumerical score between 6 and 23 (at least in this sample)"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#correlation-approach",
    "href": "materials/slides/week3-randomforest.html#correlation-approach",
    "title": "Random forests",
    "section": "Correlation approach",
    "text": "Correlation approach\nSo here’s an idea:\n\ncompute correlations of each protein with ADOS;\npick the 10 proteins with the strongest correlation"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#computation",
    "href": "materials/slides/week3-randomforest.html#computation",
    "title": "Random forests",
    "section": "Computation",
    "text": "Computation\nThis is a simple aggregation operation and can be executed in R with summarize() :\n\n# compute correlations\nasd_clean %>%\n  pivot_longer(cols = -ados,\n               names_to = 'protein',\n               values_to = 'level') %>%\n  group_by(protein) %>%\n  summarize(correlation = cor(ados, level))\n\n# A tibble: 1,317 × 2\n   protein                   correlation\n   <chr>                           <dbl>\n 1 14-3-3                         0.0970\n 2 14-3-3 protein beta/alpha      0.0481\n 3 14-3-3 protein theta           0.122 \n 4 14-3-3 protein zeta/delta      0.0735\n 5 14-3-3E                       -0.127 \n 6 17-beta-HSD 1                  0.0969\n 7 3HAO                           0.0773\n 8 3HIDH                          0.0452\n 9 4-1BB                          0.0290\n10 4-1BB ligand                   0.0799\n# … with 1,307 more rows"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#visual-assessment",
    "href": "materials/slides/week3-randomforest.html#visual-assessment",
    "title": "Random forests",
    "section": "Visual assessment",
    "text": "Visual assessment"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#sort-and-slice",
    "href": "materials/slides/week3-randomforest.html#sort-and-slice",
    "title": "Random forests",
    "section": "Sort and slice",
    "text": "Sort and slice\n\nListVisual\n\n\n\n\n# A tibble: 10 × 4\n   protein          correlation abs.corr  rank\n   <chr>                  <dbl>    <dbl> <int>\n 1 CO8A1                  0.362    0.362  1317\n 2 C5b, 6 Complex        -0.337    0.337     1\n 3 Thrombospondin-1       0.310    0.310  1316\n 4 ILT-2                 -0.309    0.309     2\n 5 TRAIL R4               0.296    0.296  1315\n 6 HCE000414             -0.296    0.296     3\n 7 C1r                   -0.290    0.290     4\n 8 GM-CSF                 0.290    0.290  1314\n 9 Angiogenin            -0.284    0.284     5\n10 HCG                    0.278    0.278  1313"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#slr-coefficients-instead",
    "href": "materials/slides/week3-randomforest.html#slr-coefficients-instead",
    "title": "Random forests",
    "section": "SLR coefficients instead?",
    "text": "SLR coefficients instead?\nFact: the simple linear regression coefficient estimate is proportional to the correlation coefficient.\n\nSo it should give similar results to sort the SLR coefficients by significance.\n\n\n\n\n# A tibble: 1,317 × 3\n   protein          estimate p.value\n   <chr>               <dbl>   <dbl>\n 1 CO8A1               0.391 0.00131\n 2 C5b, 6 Complex     -0.401 0.00290\n 3 Thrombospondin-1    0.317 0.00635\n 4 ILT-2              -0.546 0.00664\n 5 TRAIL R4            0.353 0.00933\n 6 HCE000414          -0.278 0.00950\n 7 C1r                -0.341 0.0110 \n 8 GM-CSF              0.345 0.0110 \n 9 Angiogenin         -0.314 0.0130 \n10 HCG                 0.306 0.0149 \n# … with 1,307 more rows"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#fdr-control",
    "href": "materials/slides/week3-randomforest.html#fdr-control",
    "title": "Random forests",
    "section": "FDR control",
    "text": "FDR control\nIf we do the correlation analysis this way, do the identified proteins pass multiple testing significance thresholds?\n\n\n# A tibble: 10 × 3\n   protein          p.value  p.adj\n   <chr>              <dbl>  <dbl>\n 1 CO8A1            0.00131 0.0145\n 2 C5b, 6 Complex   0.00290 0.0342\n 3 Thrombospondin-1 0.00635 0.112 \n 4 ILT-2            0.00664 0.0641\n 5 TRAIL R4         0.00933 1.29  \n 6 HCE000414        0.00950 0.916 \n 7 C1r              0.0110  0.242 \n 8 GM-CSF           0.0110  0.124 \n 9 Angiogenin       0.0130  0.142 \n10 HCG              0.0149  0.159 \n\n\n\nprobably just introducing selection noise\nbut also, this result diverges considerably from the paper (?)"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#aside-correlation-test",
    "href": "materials/slides/week3-randomforest.html#aside-correlation-test",
    "title": "Random forests",
    "section": "Aside: correlation test",
    "text": "Aside: correlation test\nThe SLR approach is equivalent to sorting the correlations. (Take a moment to check the results.)\n\nWe could use inference on the population correlation to obtain a \\(p\\)-value associated with each sample correlation coefficient. These match the ones from SLR.\n\ncor_test <- function(x, y){\n  cor_out <- cor.test(x, y)\n  tibble(estimate = cor_out$estimate,\n         p.value = cor_out$p.value)\n}\n\nasd_clean %>%\n  pivot_longer(cols = -ados,\n               names_to = 'protein',\n               values_to = 'level') %>%\n  group_by(protein) %>%\n  summarize(correlation = cor_test(ados, level)) %>%\n  unnest(correlation) %>%\n  arrange(p.value)\n\n# A tibble: 1,317 × 3\n   protein          estimate p.value\n   <chr>               <dbl>   <dbl>\n 1 CO8A1               0.362 0.00131\n 2 C5b, 6 Complex     -0.337 0.00290\n 3 Thrombospondin-1    0.310 0.00635\n 4 ILT-2              -0.309 0.00664\n 5 TRAIL R4            0.296 0.00933\n 6 HCE000414          -0.296 0.00950\n 7 C1r                -0.290 0.0110 \n 8 GM-CSF              0.290 0.0110 \n 9 Angiogenin         -0.284 0.0130 \n10 HCG                 0.278 0.0149 \n# … with 1,307 more rows"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#background",
    "href": "materials/slides/week3-randomforest.html#background",
    "title": "Random forests",
    "section": "Background",
    "text": "Background\nA binary tree is a directed graph in which:\n\nthere is at most one path between any two nodes\neach node has at most two outward-directed edges"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#classification-tree",
    "href": "materials/slides/week3-randomforest.html#classification-tree",
    "title": "Random forests",
    "section": "Classification tree",
    "text": "Classification tree\nA classification tree is a binary tree in which the paths represent classification rules.\n\nA goofy classification tree."
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#example-classifying-high-earners",
    "href": "materials/slides/week3-randomforest.html#example-classifying-high-earners",
    "title": "Random forests",
    "section": "Example: classifying high earners",
    "text": "Example: classifying high earners\nSay we want to predict income based on captial gains and education level using census data.\n\n\n# A tibble: 6 × 3\n# Groups:   income [2]\n  income educ     capital_gain\n  <fct>  <fct>           <dbl>\n1 <=50K  hs                  0\n2 <=50K  advanced            0\n3 <=50K  college             0\n4 >50K   hs               7688\n5 >50K   hs               7688\n6 >50K   college          5178\n\n\n\nWe could construct a classification tree by ‘splitting’ based on the values of predictor variables."
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#activity-building-trees",
    "href": "materials/slides/week3-randomforest.html#activity-building-trees",
    "title": "Random forests",
    "section": "Activity: building trees",
    "text": "Activity: building trees\nTo get a sense of the process of tree construction, we’ll do an activity in groups: each group will build a tree ‘by hand’.\n\nfirst let’s look at the instructions together\nthen take about 15-20 minutes to do it"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#random-forests",
    "href": "materials/slides/week3-randomforest.html#random-forests",
    "title": "Random forests",
    "section": "Random forests",
    "text": "Random forests\nA random forest is a classifier based on many trees. It is constructed by:\n\nbuilding some large number of \\(T\\) trees using bootstrap samples and random subsets of predictors (what you just did, repeated many times)\ntaking a majority vote across all trees to determine the classification\n\n\nSo let’s take a vote using your trees!"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#variable-importance-scores",
    "href": "materials/slides/week3-randomforest.html#variable-importance-scores",
    "title": "Random forests",
    "section": "Variable importance scores",
    "text": "Variable importance scores\nIf the number of trees \\(T\\) is large (as it should be):\n\ntrees are built using lots of random subsets of predictors\ncan keep track of which ones are used most often to define splits\n\n\nVariable importance scores provide a measure of how influential each predictor is in a random forest."
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#results",
    "href": "materials/slides/week3-randomforest.html#results",
    "title": "Random forests",
    "section": "Results",
    "text": "Results\nBack to the proteomics data, the variable importance scores from a random forest provide another means of identifying proteins.\n\n\n\n\n# grow RF\nset.seed(101222)\nrf_out <- randomForest(x = asd_preds, y = asd_resp,\n                       mtry = 100, ntree = 1000, \n                       importance = T)\n\n# variable importance\nrf_out$importance %>% \n  as_tibble() %>%\n  mutate(protein = rownames(rf_out$importance)) %>%\n  slice_max(MeanDecreaseGini, n = 10) %>%\n  select(protein)\n\n# A tibble: 10 × 1\n   protein    \n   <chr>      \n 1 DERM       \n 2 IgD        \n 3 TGF-b R III\n 4 TSP4       \n 5 Notch 1    \n 6 MAPK14     \n 7 RELT       \n 8 ERBB1      \n 9 CK-MB      \n10 SOST"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#errors",
    "href": "materials/slides/week3-randomforest.html#errors",
    "title": "Random forests",
    "section": "Errors",
    "text": "Errors\nBut how accurate is the predictor?\n\n\n\n\n\n\nASD\nTD\nclass.error\n\n\n\n\nASD\n53\n23\n0.3026316\n\n\nTD\n19\n59\n0.2435897"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#next-week",
    "href": "materials/slides/week3-randomforest.html#next-week",
    "title": "Random forests",
    "section": "Next week",
    "text": "Next week\n\nlogistic regression\nvariable selection\na design view of the proteomic analysis"
  },
  {
    "objectID": "materials/slides/week4-lasso.html#announcementsreminders",
    "href": "materials/slides/week4-lasso.html#announcementsreminders",
    "title": "Variable selection with the LASSO",
    "section": "Announcements/reminders",
    "text": "Announcements/reminders\n\nOffice hours follow all class and section meetings in building 434 room 122.\n\nEnter through courtyard, not on Storke side"
  },
  {
    "objectID": "materials/slides/week4-lasso.html#minute-activity",
    "href": "materials/slides/week4-lasso.html#minute-activity",
    "title": "Variable selection with the LASSO",
    "section": "10 Minute Activity",
    "text": "10 Minute Activity\nWith your table, share your diagrams.\n\nidentify common elements/aspects\npick one or a blend that you like best and sketch it on the whiteboard"
  },
  {
    "objectID": "materials/slides/week4-lasso.html#design-assessment-1",
    "href": "materials/slides/week4-lasso.html#design-assessment-1",
    "title": "Variable selection with the LASSO",
    "section": "Design assessment",
    "text": "Design assessment\nHow would you score the proteomics analysis along the following dimensions?\n\nreproducibility: how easy is it to reproduce the results exactly?\ntransparency: how widely distributed is potential variability in results across the elements of the data analysis?\nreplicability: how likely is it that if the study were conducted with new subjects, the same panel would be obtained?"
  },
  {
    "objectID": "materials/slides/week4-lasso.html#two-steps",
    "href": "materials/slides/week4-lasso.html#two-steps",
    "title": "Variable selection with the LASSO",
    "section": "Two steps",
    "text": "Two steps\nAt a very high level, the analysis we’ve covered is a ‘two-step’ procedure:\n\n(selection step) select variables\n(estimation step) train a classifier using selected variables\n\n\nA likely reason for this approach is that one cannot fit a regression model with 1317 predictors based on only 154 data points (\\(p > n\\))."
  },
  {
    "objectID": "materials/slides/week4-lasso.html#a-schematic",
    "href": "materials/slides/week4-lasso.html#a-schematic",
    "title": "Variable selection with the LASSO",
    "section": "A schematic",
    "text": "A schematic\nHere’s a representation of the design of the analysis procedure.\n\n\n\n\n\n\n\nG\n\n \n\ncluster1\n\n statistical modeling  \n\ncluster2\n\n outputs   \n\ndata\n\n data   \n\nfit\n\n logistic   regression   \n\ndata->fit\n\n    \n\ns1\n\n multiple   testing   \n\ndata->s1\n\n    \n\ns2\n\n correlation   analysis   \n\ndata->s2\n\n    \n\ns3\n\n random   forest   \n\ndata->s3\n\n    \n\njoin\n\n   \n\ndata->join\n\n multiple partitioning   \n\nselec\n\n ensemble   selection   \n\nselec->fit\n\n    \n\nsel\n\n selected   variables   \n\nfit->sel\n\n    \n\nfit->join\n\n   \n\ns1->selec\n\n    \n\ns2->selec\n\n    \n\ns3->selec\n\n    \n\npred\n\n accuracy   quantification   \n\njoin->pred\n\n   \n\n\n\n\n\n\nlots of failure modes\nrepeated use of the same data for different sub-analyses"
  },
  {
    "objectID": "materials/slides/week4-lasso.html#what-about-one-step",
    "href": "materials/slides/week4-lasso.html#what-about-one-step",
    "title": "Variable selection with the LASSO",
    "section": "What about one step?",
    "text": "What about one step?\nIf we did simultaneous selection and estimation, our schematic might look like this:\n\n\n\n\n\n\n\nG\n\n \n\ncluster1\n\n data partitioning  \n\ncluster2\n\n statistical modeling  \n\ncluster3\n\n outputs   \n\ndata\n\n data   \n\ntrain\n\n training   set   \n\ndata->train\n\n    \n\ntest\n\n test   set   \n\ndata->test\n\n    \n\nfit\n\n selection and   estimation   \n\ntrain->fit\n\n    \n\njoin\n\n   \n\ntest->join\n\n   \n\nsel\n\n selected   variables   \n\nfit->sel\n\n    \n\nfit->join\n\n   \n\npred\n\n accuracy   quantification   \n\njoin->pred"
  },
  {
    "objectID": "materials/slides/week4-lasso.html#sparse-estimation",
    "href": "materials/slides/week4-lasso.html#sparse-estimation",
    "title": "Variable selection with the LASSO",
    "section": "Sparse estimation",
    "text": "Sparse estimation\nImagine a logistic regression model with all 1317 predictors:\n\\[\n\\log\\left(\\frac{p_i}{1 - p_i}\\right) = \\beta_0 + \\sum_{j = 1}^{1317} \\beta_j x_{ij}\n\\]\n\nSuppose we also assume sparsity constraint: a condition that most of them must be zero.\n\n\nAs long as the constraint is strong enough so that the number of nonzero coefficients is \\(n\\) or fewer (i.e., \\(\\|\\beta\\|_0 < n\\)), one can compute estimates."
  },
  {
    "objectID": "materials/slides/week4-lasso.html#the-lasso",
    "href": "materials/slides/week4-lasso.html#the-lasso",
    "title": "Variable selection with the LASSO",
    "section": "The LASSO",
    "text": "The LASSO\nThe Least Absolute Shrinkage and Selection Operator (LASSO) (Tibshirani (1996) ) is the most widely-used sparse regression estimator.\n\nMathematically, the LASSO is the \\(L_1\\)-constrained MLE, i.e., the solution to the optimization problem:\n\\[\n\\begin{aligned}\n\\text{maximize}\\quad &\\mathcal{L}(\\beta; x, y) \\\\\n\\text{subject to}\\quad &\\|\\beta\\|_1 < t\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "materials/slides/week4-lasso.html#lagrangian-form",
    "href": "materials/slides/week4-lasso.html#lagrangian-form",
    "title": "Variable selection with the LASSO",
    "section": "Lagrangian form",
    "text": "Lagrangian form\nThe solution to the constrained problem\n\\[\n\\begin{aligned}\n\\text{maximize}\\quad &\\mathcal{L}(\\beta; x, y) \\\\\n\\text{subject to}\\quad &\\|\\beta\\|_1 < t\n\\end{aligned}\n\\]\ncan be expressed as the unconstrained optimization\n\\[\n\\hat{\\beta} = \\arg\\min_\\beta \\left\\{-\\mathcal{L}(\\beta; x, y) + \\lambda\\|\\beta\\|_1\\right\\}\n\\]\nwhere \\(\\lambda\\) is a Lagrange multiplier."
  },
  {
    "objectID": "materials/slides/week4-lasso.html#toy-example",
    "href": "materials/slides/week4-lasso.html#toy-example",
    "title": "Variable selection with the LASSO",
    "section": "Toy example",
    "text": "Toy example\n\n\nI simulated 250 observations according to a logistic regression model with 13 predictors, all but 3 of which had zero coefficients:\n\\[\n\\beta = \\left[ 0\\; 3\\; 0\\; 0\\; 0\\; 0\\; 0\\; 0\\; 0\\; 0\\; 0\\; 2\\; 2\\right]^T\n\\]\nAnd computed LASSO estimates for a few values of \\(\\lambda\\).\n\n\n\n# A tibble: 14 × 4\n   term  truth lambda1 lambda2\n   <chr> <dbl>   <dbl>   <dbl>\n 1 Int    -0.5  -0.171  -0.206\n 2 V1      0     0       0    \n 3 V2      3     1.49    1.96 \n 4 V3      0     0       0    \n 5 V4      0     0       0    \n 6 V5      0     0       0    \n 7 V6      0     0       0    \n 8 V7      0     0       0    \n 9 V8      0     0       0.084\n10 V9      0     0       0    \n11 V10     0     0       0    \n12 V11     0     0      -0.069\n13 V12     2     0.892   1.29 \n14 V13     2     1.01    1.40"
  },
  {
    "objectID": "materials/slides/week4-lasso.html#constraint-strength",
    "href": "materials/slides/week4-lasso.html#constraint-strength",
    "title": "Variable selection with the LASSO",
    "section": "Constraint strength",
    "text": "Constraint strength\nIn practice, \\(\\lambda\\) controls the strength of the constraint and thus the sparsity:\n\nLarger \\(\\lambda\\) ➜ stronger constraint ➜ more sparse\nSmaller \\(\\lambda\\) ➜ weaker constraint ➜ less sparse"
  },
  {
    "objectID": "materials/slides/week4-lasso.html#selecting-lambda",
    "href": "materials/slides/week4-lasso.html#selecting-lambda",
    "title": "Variable selection with the LASSO",
    "section": "Selecting \\(\\lambda\\)",
    "text": "Selecting \\(\\lambda\\)\nHow to choose the strength of constraint?\n\nCompute estimates for a “path” \\(\\lambda_1 > \\lambda_2 > \\cdots > \\lambda_K\\)\nChoose \\(\\lambda^*\\) that minimizes an error metric.\n\n\nTypically prediction error is used as the metric, and estimated by:\n\npartitioning the data many times\naveraging test set predictive accuracy across partitions"
  },
  {
    "objectID": "materials/slides/week4-lasso.html#regularization-profile-estimates",
    "href": "materials/slides/week4-lasso.html#regularization-profile-estimates",
    "title": "Variable selection with the LASSO",
    "section": "Regularization profile: estimates",
    "text": "Regularization profile: estimates\n\nValue of coefficient estimates as a function of regularization strength for the toy example. Each path corresponds to one model coefficient. Vertical line indicates optimal strength."
  },
  {
    "objectID": "materials/slides/week4-lasso.html#regularization-profile-error",
    "href": "materials/slides/week4-lasso.html#regularization-profile-error",
    "title": "Variable selection with the LASSO",
    "section": "Regularization profile: error",
    "text": "Regularization profile: error\n\nPrediction error metric (‘deviance’, similar to RMSE) as a function of regularization strength. Two plausible choices shown – one more conservative, one less conservative."
  },
  {
    "objectID": "materials/slides/week4-lasso.html#lambda-selection",
    "href": "materials/slides/week4-lasso.html#lambda-selection",
    "title": "Variable selection with the LASSO",
    "section": "\\(\\lambda\\) selection",
    "text": "\\(\\lambda\\) selection\n\nEstimate profile with optimal choice indicated by vertical line."
  },
  {
    "objectID": "materials/slides/week4-lasso.html#coefficient-estimates",
    "href": "materials/slides/week4-lasso.html#coefficient-estimates",
    "title": "Variable selection with the LASSO",
    "section": "Coefficient estimates",
    "text": "Coefficient estimates\n\n\n\n\n\nterm\nestimate\ntruth\n\n\n\n\n(Intercept)\n-0.1712466\n-0.5\n\n\nV2\n1.4867846\n3.0\n\n\nV12\n0.8922530\n2.0\n\n\nV13\n1.0142979\n2.0"
  },
  {
    "objectID": "materials/slides/week4-lasso.html#recomputing-estimates",
    "href": "materials/slides/week4-lasso.html#recomputing-estimates",
    "title": "Variable selection with the LASSO",
    "section": "Recomputing estimates",
    "text": "Recomputing estimates\nNotice that the estimate magnitudes are off by a considerable amount. If we recompute estimates without the constraint:\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.2620012\n0.1894367\n-1.383054\n0.1666482\n\n\nV2\n2.5653900\n0.3313415\n7.742434\n0.0000000\n\n\nV12\n1.7554670\n0.2786778\n6.299271\n0.0000000\n\n\nV13\n1.8323539\n0.2632716\n6.959937\n0.0000000"
  },
  {
    "objectID": "materials/slides/week4-lasso.html#errors",
    "href": "materials/slides/week4-lasso.html#errors",
    "title": "Variable selection with the LASSO",
    "section": "Errors",
    "text": "Errors\nNow if I simulate some new observations:\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nsensitivity\nbinary\n0.8500000\n\n\nspecificity\nbinary\n0.9000000\n\n\nroc_auc\nbinary\n0.9508333"
  },
  {
    "objectID": "materials/slides/week4-lasso.html#alternative-analysis",
    "href": "materials/slides/week4-lasso.html#alternative-analysis",
    "title": "Variable selection with the LASSO",
    "section": "Alternative analysis",
    "text": "Alternative analysis\n\nPreprocessing\\(\\lambda\\) selectionRefittingAccuracy\n\n\nLog-transform, center and scale, but don’t trim outliers.\n\nlibrary(tidymodels)\n\n# read in data\nbiomarker <- read_csv('data/biomarker-clean.csv') %>%\n  select(-ados) %>%\n  mutate(across(-group, ~scale(log(.x))[,1]),\n         class = as.numeric(group == 'ASD'))\n\n# partition\nset.seed(101622)\npartitions <- biomarker %>%\n  initial_split(prop = 0.8)\n\nx_train <- training(partitions) %>%\n  select(-group, -class) %>%\n  as.matrix()\ny_train <- training(partitions) %>%\n  pull(class)\n\n\n\nEstimated error\n\n# reproducibility\nset.seed(102022)\n\n# multiple partitioning for lambda selection\ncv_out <- cv.glmnet(x_train, \n                    y_train, \n                    family = 'binomial', \n                    nfolds = 5, \n                    type.measure = 'deviance')\n\ncvout_df <- tidy(cv_out) \n\n\n\n\n\n\n\n\n\n\nCoefficient paths\n\n# LASSO estimates\nfit <- glmnet(x_train, y_train, family = 'binomial')\nfit_df <- tidy(fit)\n\n\n\n\n\n\n\n\n\n\n\n\nAfter dropping the penalty and recomputing estimates, the final model is:\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.0404465\n0.2312305\n-0.1749184\n0.8611438\n\n\nMAPK2\n-0.9896079\n0.2831955\n-3.4944340\n0.0004751\n\n\nIgD\n-0.8264133\n0.2471363\n-3.3439569\n0.0008259\n\n\nDERM\n-0.9971069\n0.2758803\n-3.6142740\n0.0003012\n\n\n\n\n\n\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nsensitivity\nbinary\n0.8125000\n\n\nspecificity\nbinary\n0.7333333\n\n\nroc_auc\nbinary\n0.8375000"
  },
  {
    "objectID": "materials/slides/week4-lasso.html#some-final-thoughts",
    "href": "materials/slides/week4-lasso.html#some-final-thoughts",
    "title": "Variable selection with the LASSO",
    "section": "Some final thoughts",
    "text": "Some final thoughts\n\nRegardless of method, classification based on serum levels seems to achieve 70-80% accuracy out of sample.\nNot specific enough to be used as a diagnostic tool, but may be helpful in early detection.\nPublished analysis seems a little over-complicated; computationally intensive methods are less transparent and more difficult to reproduce."
  },
  {
    "objectID": "materials/slides/week4-lasso.html#concepts-discussed",
    "href": "materials/slides/week4-lasso.html#concepts-discussed",
    "title": "Variable selection with the LASSO",
    "section": "Concepts discussed",
    "text": "Concepts discussed\n\nmultiple hypothesis testing, FWER and FDR control\nclassification using logistic regression, random forests\nvariable selection using LASSO regularization\ndata partitioning and accuracy quantification"
  },
  {
    "objectID": "materials/slides/week4-lasso.html#transferrable-skills",
    "href": "materials/slides/week4-lasso.html#transferrable-skills",
    "title": "Variable selection with the LASSO",
    "section": "Transferrable skills",
    "text": "Transferrable skills\n\niterative computations in R, three ways\nfitting a logistic regression model with glm()\nuse of yardstick and rsample for quantifying classification accuracy\n\n\n\n\n\n\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society: Series B (Methodological) 58 (1): 267–88."
  },
  {
    "objectID": "materials/slides/week4-logistic.html#announcementsreminders",
    "href": "materials/slides/week4-logistic.html#announcementsreminders",
    "title": "Classification with logistic regression",
    "section": "Announcements/reminders",
    "text": "Announcements/reminders\n\nNext assignment and groups posted on class website\n\nDue Friday, October 28, 11:59pm\n\nAny feedback on the first group assignment?"
  },
  {
    "objectID": "materials/slides/week4-logistic.html#from-last-time",
    "href": "materials/slides/week4-logistic.html#from-last-time",
    "title": "Classification with logistic regression",
    "section": "From last time",
    "text": "From last time\nIn groups you made \\(T = 7\\) trees ‘by hand’. To make each tree:\n\nrandomly resample observations and choose two predictors at random\nchoose a variable and a split by manual inspection, then repeat\n\nthis method of tree construction is called recursive partitioning\n\n\n\nThen each of you classified a new observation. We took a majority vote to decide on the final classification.\n\n\nThis is a random forest consisting of \\(T = 7\\) trees."
  },
  {
    "objectID": "materials/slides/week4-logistic.html#random-forests",
    "href": "materials/slides/week4-logistic.html#random-forests",
    "title": "Classification with logistic regression",
    "section": "Random forests",
    "text": "Random forests\nTo implement random forests algorithmically, one can control:\n\nnumber of trees \\(T\\)\nnumber of predictors \\(m\\) to choose at random for each tree\nbootstrap sample size and method\ntree depth as specified by…\n\nminimum number of observations per split (‘node size’)\nmaximum number of terminal nodes"
  },
  {
    "objectID": "materials/slides/week4-logistic.html#variable-importance",
    "href": "materials/slides/week4-logistic.html#variable-importance",
    "title": "Classification with logistic regression",
    "section": "Variable importance",
    "text": "Variable importance\nSuppose we had a random forest of three trees.\n\n\n\n\n\n\n\n\n\nG\n\n \n\ncluster3\n\n tree #3  \n\ncluster2\n\n tree #2  \n\ncluster1\n\n tree #1   \n\nage\n\n age   \n\nedu\n\n edu   \n\nage->edu\n\n   \n\nl2\n\n >50k   \n\nage->l2\n\n   \n\nl1\n\n >50k   \n\nedu->l1\n\n   \n\nl3\n\n <50k   \n\nedu->l3\n\n   \n\nedu2\n\n edu   \n\nl4\n\n >50k   \n\nedu2->l4\n\n   \n\nl5\n\n <50k   \n\nedu2->l5\n\n   \n\ngain\n\n gain   \n\nage2\n\n age   \n\ngain->age2\n\n   \n\nl7\n\n <50k   \n\ngain->l7\n\n   \n\nsex\n\n sex   \n\nsex->gain\n\n   \n\nl6\n\n >50k   \n\nsex->l6\n\n   \n\nl8\n\n <50k   \n\nage2->l8\n\n   \n\nl9\n\n >50k   \n\nage2->l9\n\n  \n\n\n\n\n\n\n\nWhich variables seem most important?\nHow can you tell?"
  },
  {
    "objectID": "materials/slides/week4-logistic.html#how-to-measure-importance",
    "href": "materials/slides/week4-logistic.html#how-to-measure-importance",
    "title": "Classification with logistic regression",
    "section": "How to measure importance?",
    "text": "How to measure importance?\nA natural thought is to measure importance by the use frequency of each variable.\n\nBut use frequency doesn’t capture the quality of splits. Imagine:\n\nsplitting often on education but with little improvement in classifications\nand splitting infrequently on captial gain but with dramatic improvement\ncapital gain is probably more important for classification even though it’s used less"
  },
  {
    "objectID": "materials/slides/week4-logistic.html#quality-of-splits",
    "href": "materials/slides/week4-logistic.html#quality-of-splits",
    "title": "Classification with logistic regression",
    "section": "Quality of splits",
    "text": "Quality of splits\nWhen you were building trees, you had to choose which variable to split on.\n\nHow did you pick?\nDid you have a principle or goal in mind?\nWhat would make one split better than another?"
  },
  {
    "objectID": "materials/slides/week4-logistic.html#measuring-quality-node-homogeneity",
    "href": "materials/slides/week4-logistic.html#measuring-quality-node-homogeneity",
    "title": "Classification with logistic regression",
    "section": "Measuring quality: node homogeneity",
    "text": "Measuring quality: node homogeneity\nOne approach is to tree construction is to choose splits that optimize quantitative measures of node homogeneity. If \\(p_k\\) is the proportion of observations in class \\(k\\):\n\n(Gini index) \\(1 - \\sum_{k = 1}^K p_k^2\\)\n(entropy) \\(-\\sum_{k = 1}^K p_k \\log_2 p_k\\)\n\n\nSmaller values indicate greater homogeneity."
  },
  {
    "objectID": "materials/slides/week4-logistic.html#variable-importance-scores",
    "href": "materials/slides/week4-logistic.html#variable-importance-scores",
    "title": "Classification with logistic regression",
    "section": "Variable importance scores",
    "text": "Variable importance scores\nThe change in node homogeneity can be calculated for every split:\n\\[\nh(\\text{before}) - \\underbrace{\\Big[(p_L \\times h(\\text{after}_L) - p_R \\times h(\\text{after}_R)\\Big]}_{\\text{weighted avg. of homogeneity in child nodes}}\n\\]\n\nThe average change across all nodes associated with a given predictor in all trees gives an easy measure of importance.\n\nfavors high-quality splits over splitting frequency"
  },
  {
    "objectID": "materials/slides/week4-logistic.html#proteomics-application",
    "href": "materials/slides/week4-logistic.html#proteomics-application",
    "title": "Classification with logistic regression",
    "section": "Proteomics application",
    "text": "Proteomics application\nBack to the proteomics data, the variable importance scores from a random forest provide another means of identifying proteins.\n\nfit a random forest\ncompute importance scores\nrank predictors and choose top \\(n\\)"
  },
  {
    "objectID": "materials/slides/week4-logistic.html#results",
    "href": "materials/slides/week4-logistic.html#results",
    "title": "Classification with logistic regression",
    "section": "Results",
    "text": "Results\n\nFittingImportance scoresAccuracy\n\n\n\n# reproducibility\nset.seed(101422)\n\n# fit rf\nrf_out <- randomForest(x = asd_preds, # predictors\n                       y = asd_resp, # response\n                       ntree = 1000, # number of trees\n                       importance = T) # compute importance\n\nBy default, randomForest():\n\nuses \\(\\sqrt{p}\\) predictors for each tree\ntrees grown until exact classification accuracy is achieved\nbootstrap sample size equal to number of observations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRows show true classes, columns show predicted classes.\n\n\n\n\n\n\nASD\nTD\nclass.error\n\n\n\n\nASD\n49\n27\n0.3552632\n\n\nTD\n21\n57\n0.2692308"
  },
  {
    "objectID": "materials/slides/week4-logistic.html#putting-things-together",
    "href": "materials/slides/week4-logistic.html#putting-things-together",
    "title": "Classification with logistic regression",
    "section": "Putting things together",
    "text": "Putting things together\nLet \\(\\hat{S}_j\\) indicate the set of proteins selected by method \\(j\\) . Then the final estimate is\n\\[\n\\hat{S}^* = \\bigcap_j \\hat{S}_j\n\\]\n\nIn other words, those proteins that are selected by all three methods. Remarks:\n\nprobably fairly high selection variance\nprobably pretty conservative"
  },
  {
    "objectID": "materials/slides/week4-logistic.html#core-panel",
    "href": "materials/slides/week4-logistic.html#core-panel",
    "title": "Classification with logistic regression",
    "section": "“Core” panel",
    "text": "“Core” panel\n\nMultiple testingRandom forestIntersection\n\n\n\ntt_fn <- function(.df){\n  infer::t_test(.df,\n         formula = level ~ group,\n         alternative = 'two-sided',\n         order = c('ASD', 'TD'),\n         var.equal = F)\n}\n\ns1 <- read_csv('data/biomarker-clean.csv') %>% \n  mutate(across(.cols = -c(group, ados), log10)) %>%\n  mutate(across(.cols = -c(group, ados), ~ scale(.x)[, 1])) %>%\n  mutate(across(.cols = -c(group, ados), trim_fn)) %>%\n  select(-ados) %>%\n  pivot_longer(-group,\n               names_to = \"protein\",\n               values_to = \"level\") %>%\n  nest(data = c(group, level)) %>%\n  mutate(test = map(data, tt_fn)) %>%\n  unnest(test) %>%\n  arrange(p_value) %>%\n  mutate(m = n(),\n         hm = log(m) + 1/(2*m) - digamma(1),\n         rank = row_number(),\n         p.adj = m*hm*p_value/rank) %>%\n  slice_min(p.adj, n = 10) %>%\n  pull(protein)\n\n\n\n\n# reproducibility\nset.seed(101422)\n\n# fit rf\nrf_out <- randomForest(x = asd_preds,\n                       y = asd_resp,                    \n                       ntree = 1000, \n                       importance = T) \n\n# select most important predictors\ns2 <- rf_out$importance %>% \n  as_tibble() %>%\n  mutate(protein = rownames(rf_out$importance)) %>%\n  slice_max(MeanDecreaseGini, n = 10) %>%\n  pull(protein)\n\n\n\n\ns_star <- intersect(s1, s2)\ns_star\n\n[1] \"DERM\"  \"RELT\"  \"IgD\"   \"PTN\"   \"FSTL1\""
  },
  {
    "objectID": "materials/slides/week4-logistic.html#how-accurate-is-the-panel",
    "href": "materials/slides/week4-logistic.html#how-accurate-is-the-panel",
    "title": "Classification with logistic regression",
    "section": "How accurate is the panel?",
    "text": "How accurate is the panel?\nGoal: use a statistical model to evaluate classification accuracy using the ‘core’ panel of proteins \\(\\hat{S}^*\\).\n\nThe logistic regression model is the most widely-used statistical model for binary data."
  },
  {
    "objectID": "materials/slides/week4-logistic.html#the-bernoulli-distribution",
    "href": "materials/slides/week4-logistic.html#the-bernoulli-distribution",
    "title": "Classification with logistic regression",
    "section": "The Bernoulli distribution",
    "text": "The Bernoulli distribution\nThe Bernoulli distribution describes the probability of a binary outcome (think coin toss). Mathematically:\n\\[\nY \\sim \\text{bernoulli}(p)\n\\quad\\Longleftrightarrow\\quad\nP(Y = y) = p^y (1 - p)^{1 - y}\n\\quad\\text{for}\\quad\ny \\in \\{0, 1\\}\n\\]\n\nThis just says that \\(P(Y = 1) = p\\) and \\(P(Y = 0) = 1 - p\\).\n\n\nProperties:\n\n\\(\\mathbb{E}Y = p\\)\n\\(\\text{var}Y = p(1 - p)\\)"
  },
  {
    "objectID": "materials/slides/week4-logistic.html#the-logistic-regression-model",
    "href": "materials/slides/week4-logistic.html#the-logistic-regression-model",
    "title": "Classification with logistic regression",
    "section": "The logistic regression model",
    "text": "The logistic regression model\nThe logistic regression model for a response \\(Y\\in\\{0, 1\\}\\) and covariates \\(X\\in\\mathbb{R}^p\\) is:\n\\[\n\\begin{cases}\nY_i|X_i = x_i \\stackrel{iid}{\\sim} \\text{bernoulli}(p_i)\n\\quad\\text{for}\\quad i = 1, \\dots, n\\\\\n\\log\\left(\\frac{p_i}{1 - p_i}\\right) = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip}\n\\end{cases}\n\\]\n\nThis is a generalized linear model because \\(g\\left(\\mathbb{E}Y\\right) = X\\beta\\) and \\(Y\\sim EF\\).\n\n\nParameters are estimated by maximum likelihood."
  },
  {
    "objectID": "materials/slides/week4-logistic.html#the-model-visually",
    "href": "materials/slides/week4-logistic.html#the-model-visually",
    "title": "Classification with logistic regression",
    "section": "The model, visually",
    "text": "The model, visually\n\\[\n\\log\\left(\\frac{p_i}{1 - p_i}\\right) = x_i^T\\beta\n\\quad\\Longleftrightarrow\\quad\np_i = \\frac{1}{1 + e^{-x_i^T\\beta}}\n\\]\n\nPlotting the right hand side for one predictor with \\(\\beta^T = [0 \\; 1]\\):"
  },
  {
    "objectID": "materials/slides/week4-logistic.html#assumptions",
    "href": "materials/slides/week4-logistic.html#assumptions",
    "title": "Classification with logistic regression",
    "section": "Assumptions",
    "text": "Assumptions\n\nObservations are independent\nProbability of event is monotonic in each predictor\nMean-variance relationship following Bernoulli distribution"
  },
  {
    "objectID": "materials/slides/week4-logistic.html#parameter-interpretation",
    "href": "materials/slides/week4-logistic.html#parameter-interpretation",
    "title": "Classification with logistic regression",
    "section": "Parameter interpretation",
    "text": "Parameter interpretation\nAccording to the model, the log-odds are linear in the predictors:\n\\[\n\\log\\underbrace{\\left(\\frac{p_i}{1 - p_i}\\right)}_{\\text{odds}} = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip}\n\\]\n\nSo a unit increase in the \\(j\\)th predictor \\(x_{ij} \\rightarrow x_{ij} + 1\\) is associated with a change in log-odds of \\(\\beta_j\\).\n\n\nTherefore the same unit increase is associated with a change in the odds by a factor of \\(e^\\beta_j\\)."
  },
  {
    "objectID": "materials/slides/week4-logistic.html#fitting-with-one-predictor",
    "href": "materials/slides/week4-logistic.html#fitting-with-one-predictor",
    "title": "Classification with logistic regression",
    "section": "Fitting with one predictor",
    "text": "Fitting with one predictor\n\nMLEEstimatesVisualization\n\n\nMaximum likelihood: find the parameter values for which the joint probability of the data is greatest according to the model.\n\nWritten as an optimization problem in terms of the negative log-likelihood:\n\\[\n\\hat{\\beta} = \\arg\\min_\\beta \\left\\{ -\\ell(\\beta; x, y) \\right\\}\n\\]\nComputed by iteratively re-weighted least squares (IRLS).\n\n\n\n\nasd_sub <- asd_clean %>% \n  select(group, any_of(s_star)) %>%\n  mutate(group = (group == 'ASD'))\n\nfit <- glm(group ~ DERM, family = 'binomial', data = asd_sub)\n\nfit %>% broom::tidy() %>% knitr::kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.016712\n0.1804471\n-0.0926145\n0.9262099\n\n\nDERM\n-1.116996\n0.2239268\n-4.9882173\n0.0000006\n\n\n\n\n\n\n\n\n\n\n\n\nProportion of subjects in ASD group after binning by DERM level (points) with estimated probability (curve)."
  },
  {
    "objectID": "materials/slides/week4-logistic.html#fitting-with-several-predictors",
    "href": "materials/slides/week4-logistic.html#fitting-with-several-predictors",
    "title": "Classification with logistic regression",
    "section": "Fitting with several predictors",
    "text": "Fitting with several predictors\nThe fitting procedure is identical.\n\n\nfit <- glm(group ~ ., \n           family = 'binomial', \n           data = asd_sub)\n\nfit %>%\n  broom::tidy() %>%\n  knitr::kable()\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.0933124\n0.1993486\n-0.4680863\n0.6397229\n\n\nDERM\n-0.6033605\n0.2798359\n-2.1561227\n0.0310741\n\n\nRELT\n-0.4376278\n0.2859314\n-1.5305345\n0.1258845\n\n\nIgD\n-0.6616877\n0.2129419\n-3.1073631\n0.0018876\n\n\nPTN\n-0.2339665\n0.2731089\n-0.8566785\n0.3916226\n\n\nFSTL1\n-0.3600695\n0.2592400\n-1.3889426\n0.1648502"
  },
  {
    "objectID": "materials/slides/week4-logistic.html#measuring-accuracy",
    "href": "materials/slides/week4-logistic.html#measuring-accuracy",
    "title": "Classification with logistic regression",
    "section": "Measuring accuracy",
    "text": "Measuring accuracy\nThere are two types of errors:\n\n\n\n\nPredicted 0\nPredicted 1\n\n\n\n\nClass 0\ntrue negative (TN)\nfalse positive (FP)\n\n\nClass 1\nfalse negative (FN)\ntrue positive (TP)\n\n\n\n\n\nasd_sub %>%\n  modelr::add_predictions(fit, type = 'response') %>%\n  mutate(pred_class = pred > 0.5) %>%\n  select(group, pred_class) %>%\n  mutate_all(~factor(.x, labels = c('TD', 'ASD'))) %>%\n  table()\n\n     pred_class\ngroup TD ASD\n  TD  58  20\n  ASD 20  56"
  },
  {
    "objectID": "materials/slides/week4-logistic.html#accuracy-rates",
    "href": "materials/slides/week4-logistic.html#accuracy-rates",
    "title": "Classification with logistic regression",
    "section": "Accuracy rates",
    "text": "Accuracy rates\nThe most basic accuracy rates are:\n\nSensitivity/recall: \\(\\frac{TP}{P}\\) , proportion of positives that are correctly classified\nSpecificity: \\(\\frac{TN}{N}\\) , proportion of negatives that are correctly classified\nAccuracy: proportion of observations that are correctly classified"
  },
  {
    "objectID": "materials/slides/week4-logistic.html#your-turn",
    "href": "materials/slides/week4-logistic.html#your-turn",
    "title": "Classification with logistic regression",
    "section": "Your turn",
    "text": "Your turn\nTry calculating sensitivity, specificity, and accuracy for the logistic regression using the core proteins selected.\n\n\n    TD ASD    \nTD  58  20  78\nASD 20  56  76\n    78  76 154"
  },
  {
    "objectID": "materials/slides/week4-logistic.html#using-yardstickmetric_set",
    "href": "materials/slides/week4-logistic.html#using-yardstickmetric_set",
    "title": "Classification with logistic regression",
    "section": "Using yardstick::metric_set()",
    "text": "Using yardstick::metric_set()\n\nlibrary(yardstick)\n\nclass_metrics <- metric_set(sensitivity, specificity, accuracy)\n\nasd_sub %>%\n  modelr::add_predictions(fit, type = 'response') %>%\n  class_metrics(estimate = factor(pred > 0.5),\n                truth = factor(group), \n                event_level = 'second') %>%\n  knitr::kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nsensitivity\nbinary\n0.7368421\n\n\nspecificity\nbinary\n0.7435897\n\n\naccuracy\nbinary\n0.7402597"
  },
  {
    "objectID": "materials/slides/week4-logistic.html#roc-analysis",
    "href": "materials/slides/week4-logistic.html#roc-analysis",
    "title": "Classification with logistic regression",
    "section": "ROC analysis",
    "text": "ROC analysis\nThe error rates you just calculated are based on classifying a subject as ASD whenever \\(\\hat{p}_i > 0.5\\).\n\nif we wanted a more sensitive classifier, could use \\(\\hat{p}_i > 0.4\\);\nfor a more specific classifier, use \\(\\hat{p}_i > 0.6\\).\n\n\n\n\nA receiver operating characteristic (ROC) curve shows this tradeoff between sensitivity and specificity."
  },
  {
    "objectID": "materials/slides/week4-logistic.html#other-accuracy-metrics",
    "href": "materials/slides/week4-logistic.html#other-accuracy-metrics",
    "title": "Classification with logistic regression",
    "section": "Other accuracy metrics",
    "text": "Other accuracy metrics\nSome other metrics that are useful to know:\n\nprecision \\(\\frac{TP}{TP + FP}\\), proportion of estimated positives that are correct\nfalse discovery rate \\(\\frac{FP}{TP + FP}\\), proportion of estimated positives that are incorrect\nF1 score \\(\\frac{2TP}{2TP + FP + FN}\\), harmonic mean of precision and recall\nAUROC area under ROC curve"
  },
  {
    "objectID": "materials/slides/week4-logistic.html#next-time",
    "href": "materials/slides/week4-logistic.html#next-time",
    "title": "Classification with logistic regression",
    "section": "Next time",
    "text": "Next time\n\nvariable selection via regularized estimation\ndesign assessment of Hewitson analysis\n\nmini assignment: sketch a diagram representing the data analysis in the paper; come prepared to share"
  },
  {
    "objectID": "materials/slides/week5-multinomial.html#from-last-time",
    "href": "materials/slides/week5-multinomial.html#from-last-time",
    "title": "Multinomial logistic regression",
    "section": "From last time",
    "text": "From last time\n\n\n\nLast time we ended having constructed a TF-IDF document term matrix for the claims data.\n\n\\(n = 552\\) observations\n\\(p = 15,868\\) variables (word tokens)\nbinary (rather than multiclass) labels\n\n\n\n\n# A tibble: 552 × 15,870\n   .id    bclass    adams afternoon  agent android    app arkansas arrest arrive\n   <chr>  <fct>     <dbl>     <dbl>  <dbl>   <dbl>  <dbl>    <dbl>  <dbl>  <dbl>\n 1 url1   relevant 0.0692    0.0300 0.0365  0.0450 0.0330   0.0390 0.0140 0.0305\n 2 url10  irrelev… 0         0      0       0      0        0      0      0     \n 3 url100 irrelev… 0         0      0       0      0        0      0      0     \n 4 url101 relevant 0         0      0       0      0        0      0      0     \n 5 url102 relevant 0         0      0       0      0        0      0      0     \n 6 url105 relevant 0         0      0       0      0        0      0      0     \n 7 url106 irrelev… 0         0      0       0      0        0      0      0     \n 8 url107 irrelev… 0         0      0       0      0        0      0      0     \n 9 url108 relevant 0         0      0       0      0        0      0      0     \n10 url109 irrelev… 0         0      0       0      0        0      0      0     \n# … with 542 more rows, and 15,860 more variables: assist <dbl>, attic <dbl>,\n#   barricade <dbl>, block <dbl>, blytheville <dbl>, burn <dbl>, captain <dbl>,\n#   catch <dbl>, check <dbl>, chemical <dbl>, copyright <dbl>, county <dbl>,\n#   custody <dbl>, dehydration <dbl>, demand <dbl>, department <dbl>,\n#   desktop <dbl>, device <dbl>, dispute <dbl>, division <dbl>, drug <dbl>,\n#   enter <dbl>, exit <dbl>, family <dbl>, federal <dbl>, fire <dbl>,\n#   force <dbl>, gas <dbl>, gray <dbl>, hartzell <dbl>, hold <dbl>, …"
  },
  {
    "objectID": "materials/slides/week5-multinomial.html#high-dimensionality-again",
    "href": "materials/slides/week5-multinomial.html#high-dimensionality-again",
    "title": "Multinomial logistic regression",
    "section": "High dimensionality, again",
    "text": "High dimensionality, again\nSimilar to the ASD data, we again have \\(p > n\\): more predictors than observations.\n\nBut this time, model interpretation is not important.\n\nThe goal is prediction, not explanation.\nIndividual tokens aren’t likely to be strongly associated with the labels, anyway.\n\n\n\nSo we have more options for tackling the dimensionality problem."
  },
  {
    "objectID": "materials/slides/week5-multinomial.html#sparsity",
    "href": "materials/slides/week5-multinomial.html#sparsity",
    "title": "Multinomial logistic regression",
    "section": "Sparsity",
    "text": "Sparsity\nAnother way of saying we have 15,868 predictors is that the predictor is in 15,868-dimensional space.\n\nHowever, the document term matrix is extremely sparse:\n\n# coerce DTM to sparse matrix\nclaims_dtm <- claims %>% \n  select(-.id, -bclass) %>%\n  as.matrix() %>%\n  as('sparseMatrix') \n\n# proportion of zero entries ('sparsity')\n1 - nnzero(claims_dtm)/length(claims_dtm)\n\n[1] 0.99278"
  },
  {
    "objectID": "materials/slides/week5-multinomial.html#projection",
    "href": "materials/slides/week5-multinomial.html#projection",
    "title": "Multinomial logistic regression",
    "section": "Projection",
    "text": "Projection\nSince >99% of data values are zero, there is almost certainly a low(er)-dimensional representation that well-approximates the full ~16K-dimensional predictor.\n\nSo here’s a strategy:\n\nproject the predictor onto a subspace\nfit a logistic regression model using the projected data"
  },
  {
    "objectID": "materials/slides/week5-multinomial.html#principal-components",
    "href": "materials/slides/week5-multinomial.html#principal-components",
    "title": "Multinomial logistic regression",
    "section": "Principal components",
    "text": "Principal components\nThe principal components of a data matrix \\(X\\) are an orthogonal basis (i.e., coordinate system) for its column space such that the variance of data projections is maximized along each direction.\n\nsubcollections of PC’s span subspaces\nused to find a projection that preserves variance\n\nchoose the first \\(k\\) PC’s along which the projected data retain XX% of total variance"
  },
  {
    "objectID": "materials/slides/week5-multinomial.html#illustration",
    "href": "materials/slides/week5-multinomial.html#illustration",
    "title": "Multinomial logistic regression",
    "section": "Illustration",
    "text": "Illustration\n\nImage from wikipedia."
  },
  {
    "objectID": "materials/slides/week5-multinomial.html#computation",
    "href": "materials/slides/week5-multinomial.html#computation",
    "title": "Multinomial logistic regression",
    "section": "Computation",
    "text": "Computation\nThe principal components can be computed by singular value decomposition (SVD):\n\\[\nX = UDV'\n\\]\n\ncolumns of \\(V\\) give the projections\ndiagonals of \\(D\\) give the standard deviations on each direction"
  },
  {
    "objectID": "materials/slides/week5-multinomial.html#selecting-components",
    "href": "materials/slides/week5-multinomial.html#selecting-components",
    "title": "Multinomial logistic regression",
    "section": "Selecting components",
    "text": "Selecting components\n\nFind the smallest number of components such that the proportion of variance retained exceeds a specified value:\n\\[\nn_{pc} = \\min \\left\\{i: \\frac{\\sum_{j = 1}^i d_{jj}^2}{\\sum_i d_{ii}^2} > q\\right\\}\n\\]\nSelect the corresponding projections and project the data:\n\\[\n\\tilde{X} = XV_{1:n_{pc}} \\quad\\text{where}\\quad\nV_{1:n_{pc}} = \\left( v_1 \\;\\cdots\\; v_{n_{pc}}\\right)\n\\]\n\n\nProjected data are referred to as ‘scores’."
  },
  {
    "objectID": "materials/slides/week5-multinomial.html#implementation",
    "href": "materials/slides/week5-multinomial.html#implementation",
    "title": "Multinomial logistic regression",
    "section": "Implementation",
    "text": "Implementation\nUsually prcomp() does the trick, and has a broom::tidy method available, but it’s slow for large matrices.\n\nBetter to use SVD implemented with sparse matrix computations.\n\nstart <- Sys.time()\nsvd_out <- sparsesvd(claims_dtm)\nend <- Sys.time()\ntime_ssvd <- end - start\n\nstart <- Sys.time()\nprcomp_out <- prcomp(claims_dtm, center = T)\nend <- Sys.time()\ntime_prcomp <- end - start\n\ntime_prcomp - time_ssvd\n\nTime difference of 21.19524 secs"
  },
  {
    "objectID": "materials/slides/week5-multinomial.html#obtaining-projections",
    "href": "materials/slides/week5-multinomial.html#obtaining-projections",
    "title": "Multinomial logistic regression",
    "section": "Obtaining projections",
    "text": "Obtaining projections\n\n\n\nFor today, we’ll use a function I’ve written to obtain principal components. It’s basically a wrapper around sparsesvd().\n\nThe following will return the data projected onto a subspace in which it retains at least .prop percent of the total variance.\n\nproj_out <- projection_fn(claims_dtm, .prop = 0.7)\n\nproj_out$data\n\n# A tibble: 552 × 63\n        pc1      pc2     pc3      pc4     pc5      pc6      pc7      pc8     pc9\n      <dbl>    <dbl>   <dbl>    <dbl>   <dbl>    <dbl>    <dbl>    <dbl>   <dbl>\n 1  5.24e-6 -7.68e-5 0.0135  -2.26e-4 0.00137 -0.00586  0.00255 -4.79e-3  0.0292\n 2  1.03e-5 -3.51e-4 0.00205 -2.46e-2 0.0627  -0.00335  0.00468 -1.91e-3  0.0661\n 3  1.00e-5 -6.56e-5 0.00281 -2.10e-4 0.00532 -0.00970  0.00158 -2.10e-3  0.0392\n 4  9.92e-6 -1.74e-3 0.00527 -1.32e-3 0.00296 -0.0821   0.297    1.48e-2  0.0614\n 5  3.90e-6 -7.43e-5 0.00575 -4.68e-4 0.00245 -0.00441  0.00891 -1.97e-3  0.0209\n 6  6.65e-6 -1.61e-3 0.0546  -8.38e-4 0.00198 -0.0342   0.119   -1.18e-2  0.0699\n 7  4.95e-6 -1.74e-4 0.00415 -6.60e-4 0.00170 -0.00907  0.0245  -2.75e-3  0.0600\n 8  7.46e-6 -1.41e-4 0.00101 -2.58e-4 0.00130 -0.00273  0.00336 -8.41e-4  0.0267\n 9  2.01e-5 -4.44e-4 0.00172 -5.78e-4 0.00157 -0.0133   0.0397   3.82e-4  0.0609\n10  6.76e-7 -6.03e-5 0.00288 -9.14e-4 0.00397 -2.05    -0.537    2.95e-2 -0.0998\n# … with 542 more rows, and 54 more variables: pc10 <dbl>, pc11 <dbl>,\n#   pc12 <dbl>, pc13 <dbl>, pc14 <dbl>, pc15 <dbl>, pc16 <dbl>, pc17 <dbl>,\n#   pc18 <dbl>, pc19 <dbl>, pc20 <dbl>, pc21 <dbl>, pc22 <dbl>, pc23 <dbl>,\n#   pc24 <dbl>, pc25 <dbl>, pc26 <dbl>, pc27 <dbl>, pc28 <dbl>, pc29 <dbl>,\n#   pc30 <dbl>, pc31 <dbl>, pc32 <dbl>, pc33 <dbl>, pc34 <dbl>, pc35 <dbl>,\n#   pc36 <dbl>, pc37 <dbl>, pc38 <dbl>, pc39 <dbl>, pc40 <dbl>, pc41 <dbl>,\n#   pc42 <dbl>, pc43 <dbl>, pc44 <dbl>, pc45 <dbl>, pc46 <dbl>, pc47 <dbl>, …"
  },
  {
    "objectID": "materials/slides/week5-multinomial.html#activity-1-10-min",
    "href": "materials/slides/week5-multinomial.html#activity-1-10-min",
    "title": "Multinomial logistic regression",
    "section": "Activity 1 (10 min)",
    "text": "Activity 1 (10 min)\n\nPartition the claims data into training and test sets.\nUsing the training data, find principal components that preserve at least 80% of the total variance and project the data onto those PCs.\nFit a logistic regression model to the training data with binary class labels."
  },
  {
    "objectID": "materials/slides/week5-multinomial.html#overfitting",
    "href": "materials/slides/week5-multinomial.html#overfitting",
    "title": "Multinomial logistic regression",
    "section": "Overfitting",
    "text": "Overfitting\nYou should have observed a warning that numerically 0 or 1 fitted probabilities occurred.\n\nthat means the model fit some data points exactly\n\n\nOverfitting occurs when a model is fit too closely to the training data.\n\nmeasures of fit suggest high quality\nbut predicts poorly out of sample\n\n\n\nThe curious can verify this using the model you just fit."
  },
  {
    "objectID": "materials/slides/week5-multinomial.html#another-use-of-regularization",
    "href": "materials/slides/week5-multinomial.html#another-use-of-regularization",
    "title": "Multinomial logistic regression",
    "section": "Another use of regularization",
    "text": "Another use of regularization\nLast week we spoke about using LASSO regularization for variable selection.\n\nRegularization can also be used to reduce overfitting.\n\nLASSO penalty \\(\\|\\beta\\|_1 < t\\) works\n‘ridge’ penalty \\(\\|\\beta\\|_2 < t\\) also works (but won’t shrink parameters to zero)\nor the ‘elastic net’ penalty \\(\\|\\beta\\|_1 < t\\) AND \\(\\|\\beta\\|_2 < s\\)"
  },
  {
    "objectID": "materials/slides/week5-multinomial.html#activity-2-10-min",
    "href": "materials/slides/week5-multinomial.html#activity-2-10-min",
    "title": "Multinomial logistic regression",
    "section": "Activity 2 (10 min)",
    "text": "Activity 2 (10 min)\n\nFollow activity instructions to fit a logistic regression model with an elastic net penalty to the training data.\nQuantify classification accuracy on the test data using sensitivity, specificity, and AUROC.\n\n\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 sensitivity binary         0.621\n2 specificity binary         0.830\n3 accuracy    binary         0.721\n4 roc_auc     binary         0.796"
  },
  {
    "objectID": "materials/slides/week5-multinomial.html#quick-refresher",
    "href": "materials/slides/week5-multinomial.html#quick-refresher",
    "title": "Multinomial logistic regression",
    "section": "Quick refresher",
    "text": "Quick refresher\nThe logistic regression model is\n\\[\n\\log\\left(\\frac{P(Y_i = 1)}{P(Y_i = 0)}\\right) = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip}\n\\]\n\nThis is for a binary outcome \\(Y_i \\in \\{0, 1\\}\\)."
  },
  {
    "objectID": "materials/slides/week5-multinomial.html#multinomial-response",
    "href": "materials/slides/week5-multinomial.html#multinomial-response",
    "title": "Multinomial logistic regression",
    "section": "Multinomial response",
    "text": "Multinomial response\nIf the response is instead \\(Y \\in \\{1, 2, \\dots, K\\}\\), its probability distribution can be described by the multinomial distribution (with 1 trial):\n\\[\nP(Y = k) = p_k \\quad\\text{for}\\quad k = 1, \\dots, k \\quad\\text{with}\\quad \\sum_k p_k = 1\n\\]"
  },
  {
    "objectID": "materials/slides/week5-multinomial.html#multinomial-regression-1",
    "href": "materials/slides/week5-multinomial.html#multinomial-regression-1",
    "title": "Multinomial logistic regression",
    "section": "Multinomial regression",
    "text": "Multinomial regression\nMultinomial regression fits the following model:\n\\[\n\\begin{aligned}\n\\log\\left(\\frac{p_1}{p_K}\\right) &= \\beta_0^{(1)} + x_i^T \\beta^{(1)} \\\\\n\\log\\left(\\frac{p_2}{p_K}\\right) &= \\beta_0^{(2)}  + x_i^T \\beta^{(2)} \\\\\n&\\vdots \\\\\n\\log\\left(\\frac{p_{K - 1}}{p_K}\\right) &= \\beta_0^{(K - 1)} +  x_i^T \\beta^{(K - 1)} \\\\\n\\end{aligned}\n\\]\n\nSo the number of parameters is \\((p + 1)\\times (K - 1)\\)."
  },
  {
    "objectID": "materials/slides/week5-multinomial.html#prediction",
    "href": "materials/slides/week5-multinomial.html#prediction",
    "title": "Multinomial logistic regression",
    "section": "Prediction",
    "text": "Prediction\nWith some manipulation, one can obtain expressions for each \\(p_k\\), and thus estimates of the probabilities \\(\\hat{p}_k\\) for each class \\(k\\).\n\nA natural prediction to use is whichever class is most probable:\n\\[\n\\hat{Y}_i = \\arg\\max_k \\hat{p}_k\n\\]"
  },
  {
    "objectID": "materials/slides/week5-multinomial.html#activity-3-10-min",
    "href": "materials/slides/week5-multinomial.html#activity-3-10-min",
    "title": "Multinomial logistic regression",
    "section": "Activity 3 (10 min)",
    "text": "Activity 3 (10 min)\n\nFollow instructions to fit a multinomial model to the claims data.\nCompute predictions and evaluate accuracy."
  },
  {
    "objectID": "materials/slides/week5-multinomial.html#results",
    "href": "materials/slides/week5-multinomial.html#results",
    "title": "Multinomial logistic regression",
    "section": "Results",
    "text": "Results\n\nProbabilitiesCross-tabulationSome error metrics\n\n\n\n\n# A tibble: 111 × 5\n   irrelevant physical fatality unlawful    other\n        <dbl>    <dbl>    <dbl>    <dbl>    <dbl>\n 1    0.758    0.0509   0.0639  0.0822   0.0454  \n 2    0.206    0.0254   0.728   0.0257   0.0149  \n 3    0.564    0.0680   0.244   0.0715   0.0526  \n 4    0.206    0.0254   0.728   0.0257   0.0149  \n 5    0.524    0.260    0.0303  0.137    0.0490  \n 6    0.755    0.0505   0.0652  0.0836   0.0460  \n 7    0.251    0.0304   0.669   0.0235   0.0257  \n 8    0.121    0.839    0.0184  0.00427  0.0177  \n 9    0.00946  0.00355  0.00326 0.978    0.00579 \n10    0.0107   0.00248  0.985   0.000806 0.000859\n# … with 101 more rows\n\n\n\n\n\n\n\n\n\n\nfatality\nirrelevant\nother\nphysical\nunlawful\n\n\n\n\nfatality\n17\n10\n0\n0\n0\n\n\nirrelevant\n2\n46\n1\n1\n3\n\n\nother\n0\n3\n0\n0\n0\n\n\nphysical\n0\n9\n0\n6\n0\n\n\nunlawful\n0\n6\n0\n0\n7\n\n\n\n\n\n\n\n\n# overall accuracy\nsum(diag(pred_tbl))/sum(pred_tbl)\n\n[1] 0.6846847\n\n# classwise error rates\ndiag(pred_tbl)/rowSums(pred_tbl)\n\n  fatality irrelevant      other   physical   unlawful \n 0.6296296  0.8679245  0.0000000  0.4000000  0.5384615 \n\n# predictionwise error rates\ndiag(pred_tbl)/colSums(pred_tbl)\n\n  fatality irrelevant      other   physical   unlawful \n 0.8947368  0.6216216  0.0000000  0.8571429  0.7000000"
  },
  {
    "objectID": "materials/slides/week5-text.html#announcementsreminders",
    "href": "materials/slides/week5-text.html#announcementsreminders",
    "title": "Text to data: basic NLP",
    "section": "Announcements/reminders",
    "text": "Announcements/reminders\nFeedback posted for first group assignment. Check feedback PR on repo.\n\nComments for next time:\n\ncheck your rendered html file to ensure it opens properly\nbe mindful of length, reports should be around 3-5 pages\n\nfocus on results that answer questions; no need to summarize everything attempted\n\ndon’t close the feedback pull requests\nlabel and caption figures clearly"
  },
  {
    "objectID": "materials/slides/week5-text.html#objectives",
    "href": "materials/slides/week5-text.html#objectives",
    "title": "Text to data: basic NLP",
    "section": "Objectives",
    "text": "Objectives\nThe data we’ll use for this module comes from a 2021-2022 capstone project.\n\nGoal: use a predictive model to flag webpages that may contain evidence related to fraud claims.\n\ndata are a sample of pages\nclassification problem\n\ngiven a webpage, want to predict whether contents include potential evidence"
  },
  {
    "objectID": "materials/slides/week5-text.html#about-the-data",
    "href": "materials/slides/week5-text.html#about-the-data",
    "title": "Text to data: basic NLP",
    "section": "About the data",
    "text": "About the data\nData available to develop a model are a collection of labeled webpages.\n\n~ 3K webpages\nmanually assigned labels specify type of content\n\nmultiple classes\n\nsampling method unclear/unknown\n\npredictive model fit to this data may not work well in general for an arbitrary webpage"
  },
  {
    "objectID": "materials/slides/week5-text.html#example-rows",
    "href": "materials/slides/week5-text.html#example-rows",
    "title": "Text to data: basic NLP",
    "section": "Example rows",
    "text": "Example rows\nWe will work with a random subsample of 618 observations (pages).\n\nlibrary(tidyverse)\nload('data/carpe-raw-subsample.RData')\nrawdata %>% head()\n\n# A tibble: 6 × 3\n  original_url                                         text_tmp internal_feedba…\n  <chr>                                                <chr>    <chr>           \n1 https://www.kait8.com/story/32310666/one-arrested-a… \"<!DOCT… Potentially unl…\n2 https://www.newsbreak.com/connecticut/meriden/safety \"<!DOCT… N/A: No relevan…\n3 https://www.doctor.com/Dr-Garry-Brody                \"<!DOCT… N/A: No relevan…\n4 https://thecounty.me/2020/08/13/news/motorcycle-cra… \"<!-- B… Information rel…\n5 https://obits.dallasnews.com/obituaries/dallasmorni… \"<!DOCT… Possible Fatali…\n6 https://www.athletic.net/CrossCountry/Athlete.aspx?… \"\\n\\r\\n… Physical Activi…"
  },
  {
    "objectID": "materials/slides/week5-text.html#data-semantics",
    "href": "materials/slides/week5-text.html#data-semantics",
    "title": "Text to data: basic NLP",
    "section": "Data semantics",
    "text": "Data semantics\nAt face value:\n\nThe observational units are webpages\n\none observation per page sampled\n\nThe variables are claim labels and … ???\n\n\nHow do we obtain useable data from HTML?"
  },
  {
    "objectID": "materials/slides/week5-text.html#labels",
    "href": "materials/slides/week5-text.html#labels",
    "title": "Text to data: basic NLP",
    "section": "Labels",
    "text": "Labels\nIt’ll be hard to classify labels ocurring <1% of the time.\n\n\n\n\n\ninternal_feedback\nn\nprop\n\n\n\n\nN/A: No relevant content.\n318\n0.5145631\n\n\nPossible Fatality\n116\n0.1877023\n\n\nPotentially unlawful activity\n88\n0.1423948\n\n\nPhysical Activity\n67\n0.1084142\n\n\nInformation related to the claim\n17\n0.0275081\n\n\nPotentially relevant information\n11\n0.0177994\n\n\nPain, illness, or other medical issue\n1\n0.0016181"
  },
  {
    "objectID": "materials/slides/week5-text.html#lumping",
    "href": "materials/slides/week5-text.html#lumping",
    "title": "Text to data: basic NLP",
    "section": "Lumping",
    "text": "Lumping\nWe can lump infrequent labels together (see forcats [docs]).\n\n\n\n\n\nclass\nn\nproportion\n\n\n\n\nN/A: No relevant content.\n318\n0.5145631\n\n\nPossible Fatality\n116\n0.1877023\n\n\nPotentially unlawful activity\n88\n0.1423948\n\n\nPhysical Activity\n67\n0.1084142\n\n\nOther\n29\n0.0469256\n\n\n\n\n\n\nhalf of pages contain no relevant information"
  },
  {
    "objectID": "materials/slides/week5-text.html#binary-classification",
    "href": "materials/slides/week5-text.html#binary-classification",
    "title": "Text to data: basic NLP",
    "section": "Binary classification",
    "text": "Binary classification\nThis is a multi-class classification problem.\n\nBUT if we can’t do well with binary classification, there’s not much hope for the multi-class setting. So let’s start there.\n\n\n# A tibble: 6 × 3\n  .id   bclass     text_tmp                                                     \n  <chr> <fct>      <chr>                                                        \n1 url1  relevant   \"<!DOCTYPE html> <html lang=\\\"en-US\\\"> <head> <script>var pb…\n2 url2  irrelevant \"<!DOCTYPE html><html lang=\\\"en\\\"><head><meta name=\\\"ahrefs-…\n3 url3  irrelevant \"<!DOCTYPE html>\\n<!--[if lt IE 7]>      <html class=\\\"no-js…\n4 url4  relevant   \"<!-- Base Template: base.twig -->\\n<!-- This Template: sing…\n5 url5  relevant   \"<!DOCTYPE html>\\n<html amp lang=\\\"en\\\">\\n\\n<head>\\n  <meta …\n6 url6  relevant   \"\\n\\r\\n<!DOCTYPE html>\\r\\n<html id=\\\"html\\\" lang=\\\"en\\\" xmln…\n\n\n\n\nFirst task: HTML ➜ data."
  },
  {
    "objectID": "materials/slides/week5-text.html#raw-html",
    "href": "materials/slides/week5-text.html#raw-html",
    "title": "Text to data: basic NLP",
    "section": "Raw HTML",
    "text": "Raw HTML\nHere’s what a page looks like.\n\n\n[1] \"<!DOCTYPE html> <html lang=\\\"en-US\\\"> <head> <script>var pb_global={pageName:\\\"article-template\\\",pageId:\\\"rHJ9vP1Xz0ktKr\\\",contextPath:\\\"/pb\\\",isAdmin:false,layoutEngineName:\\\"\\\",environment:\\\"prod\\\",resourceToken:\\\"\\\",_website:\\\"kait\\\"};</script> <script>window.serviceCallbacks=[];window.pageBuilder=window.pageBuilder||{};window.pageBuilder.featureLoaded=function(el){if(window.services.asyncFeatureCallback)window.services.asyncFeatureCallback(el);else window.serviceCallbacks.push(function(){window.services.asyncFeatureCallback(el)})};</script> <script>(function(){if(window.BOOMR&&window.BOOMR.version)return;var dom,doc,where,iframe=document.createElement(\\\"iframe\\\"),win=window;function boomerangSaveLoadTime(e){win.BOOMR_onload=e&&e.timeStamp||(new Date).getTime()}if(win.addEventListener)win.addEventListener(\\\"load\\\",boomerangSaveLoadTime,false);else if(win.attachEvent)win.attachEvent(\\\"onload\\\",boomerangSaveLoadTime);iframe.src=\\\"javascript:false\\\";iframe.title=\\\"\\\";iframe.role=\\\"presentation\\\";(iframe.frameElement||iframe).style.cssText=\\\"width:0;height:0;border:0;display:none;\\\";\\nwhere=document.getElementsByTagName(\\\"script\\\")[0];where.parentNode.insertBefore(iframe,where);try{doc=iframe.contentWindow.document}catch(e){dom=document.domain;iframe.src=\\\"javascript:var d\\\\x3ddocument.open();d.domain\\\\x3d'\\\"+dom+\\\"';void(0);\\\";doc=iframe.contentWindow.document}doc.open()._l=function(){var js=this.createElement(\\\"script\\\");if(dom)this.domain=dom;js.id=\\\"boomr-if-as\\\";js.src=\\\"https://c.go-mpulse.net/boomerang/\\\"+\\\"XPXCW-DAMJV-5VCNY-NSGAA-ZZ4G2\\\";BOOMR_lstart=(new Date).getTime();this.body.appendChild(js)};\\ndoc.write('\\\\x3cbody onload\\\\x3d\\\"document._l();\\\"\\\\x3e');doc.close()})();</script> <script>\\n  const addZeroPad = function(num) {\\n    num = Math.abs(num)\\n    num = num.toString()\\n    if (num.length === 1) {\\n      num = '0' + num\\n    }\\n    return num\\n  }\\n\\n  const getTimezoneOffset = function() {\\n    const offset = -new Date().getTimezoneOffset() / 60\\n    const sign = offset < 0 ? '-' : '+'\\n    return sign + addZeroPad(offset) + ':00'\\n  }\\n</script> <script>\\n  var RCdataLayer = [{\\n    \\\"adTarget\\\": \\\"kait/web/news\\\",\\n    \\\"authors\\\": [\\n                  \\n                    {\\n                      \\\"_id\\\": \\\"\\\",\\n                      \\\"name\\\": \\\"Region 8 Newsdesk\\\",\\n                      \\\"type\\\": \\\"author\\\"\\n                    },\\n                  \\n                ],\\n    \\\"canonicalUrl\\\": \\\"/story/32310666/one-arrested-after-police-standoff-in-blytheville\\\",\\n    \\\"contentId\\\": \\\"QK3SPA45DZEFPGOOKT6HIFCSGQ\\\",\\n    \\\"contentName\\\": \\\"One arrested after police standoff in Blytheville\\\",\\n    \\\"contentOwner\\\": \\\"kait\\\",\\n    \\\"contentTags\\\":  [\\n                  \\n                ],\\n    \\\"contentType\\\": \\\"story\\\",\\n    \\\"datePublished\\\": \\\"2016-06-27T01:53:12Z\\\",\\n    \\\"description\\\": \\\"One man is in custody after a 4-hour police standoff in Blytheville Sunday afternoon.\\\",\\n    \\\"distributorCategory\\\": \\\"\\\",\\n    \\\"distributorMode\\\": \\\"\\\",\\n    \\\"distributorName\\\": \\\"\\\",\\n    \\\"distributorReferenceId\\\": \\\"\\\",\\n    \\\"distributorSubCategory\\\": \\\"\\\",\\n    \\\"hasVideo\\\": false,\\n    \\\"metaDescription\\\": \\\"One man is in custody after a 4-hour police standoff in Blytheville Sunday afternoon.\\\",\\n    \\\"platformName\\\": \\\"Arc Feature Pack\\\",\\n    \\\"primarySection\\\": \\n                        \\n                        \\n                          \\n                          {\\n                            \\\"name\\\": \\\"News\\\",\\n                            \\\"_id\\\": \\\"/news\\\",\\n                            \\\"path\\\": \\\"/news\\\",\\n                          }\\n                        \\n                      ,\\n    \\\"publishedDay\\\": \\\"26\\\",\\n    \\\"publishedMonth\\\": \\\"06\\\",\\n    \\\"publishedYear\\\": \\\"2016\\\",\\n    \\\"stationName\\\": \\\"kait\\\",\\n    \\\"timezoneOffset\\\": getTimezoneOffset(),\\n    \\\"userAgent\\\": navigator.userAgent,\\n    \\\"userId\\\": \\\"\\\",\\n    \\\"userTimezone\\\": Intl.DateTimeFormat().resolvedOptions().timeZone,\\n    \\\"userTimezoneTimestamp\\\": new Date()\\n  }]\\n\\n</script> <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({\\\"gtm.start\\\":(new Date).getTime(),event:\\\"gtm.js\\\"});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!=\\\"dataLayer\\\"?\\\"\\\\x26l\\\\x3d\\\"+l:\\\"\\\";j.async=true;j.src=\\\"https://www.googletagmanager.com/gtm.js?id\\\\x3d\\\"+i+dl;f.parentNode.insertBefore(j,f)})(window,document,\\\"script\\\",\\\"RCdataLayer\\\",\\\"GTM-M5QQ3JP\\\");</script> <meta http-equiv=\\\"X-UA-Compatible\\\" content=\\\"IE=edge\\\"/> <meta name=\\\"distribution\\\" content=\\\"global\\\"/> <meta name=\\\"rating\\\" content=\\\"general\\\"/> <meta name=\\\"viewport\\\" content=\\\"width=device-width, minimum-scale=1, initial-scale=1\\\"> <meta charset=\\\"UTF-8\\\"/> <meta name=\\\"copyright\\\" content=\\\"Copyright (c) 2017 \\\"/> <meta name=\\\"google-site-verification\\\" content=\\\"sKiWVpF1B8xon1FyiyR9SfMIyuoRn0fwtKIcSdB3_f4\\\"/> <title>One arrested after police standoff in Blytheville</title> <meta name=\\\"date\\\" content=\\\"2016-06-27T16:58:11Z\\\"/> <meta itemprop=\\\"description\\\" name=\\\"description\\\" content=\\\"One man is in custody after a 4-hour police standoff in Blytheville Sunday afternoon.\\\"/> <meta name=\\\"robots\\\" content=\\\"index,follow,noarchive\\\"/> <meta name=\\\"googlebot\\\" content=\\\"index\\\"/> <meta name=\\\"googlebot-news\\\" content=\\\"index\\\"/> <link rel=\\\"canonical\\\" href=\\\"https://www.kait8.com/story/32310666/one-arrested-after-police-standoff-in-blytheville\\\"> <meta name=\\\"twitter:title\\\" content=\\\"One arrested after police standoff in Blytheville\\\"/> <meta name=\\\"twitter:image\\\" content=\\\"https://www.kait8.com/resizer/b4Wsx4ZkmQ1VxgznD1ZncCCCvOY=/1200x0/arc-anglerfish-arc2-prod-raycom.s3.amazonaws.com/public/DB2NJARH4FCNVB7KWHKJGBNQ7Y.jpg\\\"/> <meta name=\\\"twitter:card\\\" content=\\\"summary_large_image\\\"> <meta name=\\\"twitter:description\\\" content=\\\"One man is in custody after a 4-hour police standoff in Blytheville Sunday afternoon.\\\"/> <meta name=\\\"twitter:site\\\" content=\\\"@Region8News\\\"/> <meta name=\\\"twitter:creator\\\" content=\\\"@Region8News\\\"> <meta property=\\\"og:url\\\" content=\\\"https://www.kait8.com/story/32310666/one-arrested-after-police-standoff-in-blytheville\\\"/> <meta property=\\\"og:title\\\" content=\\\"One arrested after police standoff in Blytheville\\\"/> <meta itemprop=\\\"image\\\" property=\\\"og:image\\\" content=\\\"https://www.kait8.com/resizer/b4Wsx4ZkmQ1VxgznD1ZncCCCvOY=/1200x0/arc-anglerfish-arc2-prod-raycom.s3.amazonaws.com/public/DB2NJARH4FCNVB7KWHKJGBNQ7Y.jpg\\\"/> <meta property=\\\"og:description\\\" content=\\\"One man is in custody after a 4-hour police standoff in Blytheville Sunday afternoon.\\\"/> <meta property=\\\"og:locale\\\" content=\\\"en_US\\\"/> <meta property=\\\"og:site_name\\\" content=\\\"https://www.kait8.com\\\"/> <meta property=\\\"og:type\\\" content=\\\"article\\\"/> <meta property=\\\"fb:app_id\\\" content=\\\"213753052169397\\\"/> <meta property=\\\"fb:pages\\\" content=\\\"87689783147\\\"/> <meta property=\\\"fb:admins\\\" content=\\\"644426401\\\"/> <meta name=\\\"burst\\\" content=\\\"kait\\\"/> <meta property=\\\"article:publisher\\\" content=\\\"https://www.facebook.com/Region8News\\\"> <meta name=\\\"author\\\" content=\\\"Region 8 Newsdesk\\\"> <script data-schema=\\\"NewsArticle\\\" type=\\\"application/ld+json\\\">\\n      {\\n        \\\"@context\\\": \\\"http://schema.org\\\",\\n        \\\"@type\\\": \\\"NewsArticle\\\",\\n        \\\"mainEntityOfPage\\\": {\\n          \\\"@type\\\": \\\"WebPage\\\",\\n          \\\"@id\\\": \\\"\\\"\\n        },\\n        \\\"headline\\\": \\\"One arrested after police standoff in Blytheville\\\",\\n        \\n        \\\"image\\\": {\\n          \\\"@type\\\": \\\"ImageObject\\\",\\n          \\\"url\\\": \\\"https://www.kait8.com/resizer/b4Wsx4ZkmQ1VxgznD1ZncCCCvOY=/1200x0/arc-anglerfish-arc2-prod-raycom.s3.amazonaws.com/public/DB2NJARH4FCNVB7KWHKJGBNQ7Y.jpg\\\",\\n          \\\"height\\\": 900.0,\\n          \\\"width\\\": 1200\\n        },\\n        \\n        \\\"datePublished\\\": \\\"2016-06-27T01:53:12Z\\\",\\n        \\\"dateModified\\\": \\\"2016-06-27T16:58:11Z\\\",\\n        \\\"articleBody\\\": \\\"(Source: KAIT)Hartzell Watson (Source: Blytheville Police Dept.)BLYTHEVILLE, AR (KAIT) - One man is in custody after a 4-hour police standoff in Blytheville Sunday afternoon.According to Captain Scott Adams with the Blytheville Police Department, Hartzell Watson, 44, was arrested after being forced out of a home in the 500-block of North Division.Captain Adams said police originally responded to the house after a family dispute.When officers arrived, Watson barricaded himself in the house and refused to exit peacefully.After several hours, police fired smoke and a chemical agent through a window of the home. The gas caught on fire within minutes.Officers entered the burning house to find Watson, but he jumped out of an attic window.He was checked by paramedics on the scene and was taken into custody.The Blytheville Fire Department was on scene and quickly put the fire out.Watson was then taken to the Mississippi County Jail. He is being held on state and federal warrants.The Arkansas State Police, Mississippi County Sheriff&#039;s Office, and the Second Judicial Drug Task Force assisted with the standoff.One officer on the scene was treated for dehydration.Copyright 2016 KAIT. All rights reserved.Watch Region 8 News On Demand: On your Desktop | On your Mobile deviceRegion 8 News App - Install or update on your: iPhone | Android\\\",\\n        \\\"author\\\": [\\n          \\n            \\n              \\n                {\\n                  \\\"@type\\\": \\\"Person\\\",\\n                  \\\"name\\\": \\\"Region 8 Newsdesk\\\"\\n                }\\n              \\n            \\n            \\n          \\n        ],\\n         \\\"publisher\\\": {\\n        \\\"@type\\\": \\\"Organization\\\",\\n        \\\"name\\\": \\\"KAIT\\\",\\n        \\\"logo\\\": {\\n          \\\"@type\\\": \\\"ImageObject\\\",\\n          \\\"url\\\": \\\"https://www.kait8.com/pb/resources/images/rm_icons/kait-icon.png?token=123\\\",\\n          \\\"width\\\": 467,\\n          \\\"height\\\": 60\\n        }\\n      }\\n        },\\n        // will be empty for branded publishing\\n        \\\"description\\\": \\\"One man is in custody after a 4-hour police standoff in Blytheville Sunday afternoon.\\\"\\n      }\\n    </script> <script data-schema=\\\"Organization\\\" type=\\\"application/ld+json\\\">\\n      {\\n        \\\"name\\\": \\\"KAIT\\\",\\n        \\\"url\\\": \\\"https://www.kait8.com\\\",\\n        \\\"logo\\\": \\\"https://www.kait8.com/pb/resources/images/rm_icons/kait-icon.png?token=123\\\",\\n        \\\"@type\\\": \\\"Organization\\\",\\n        \\n          \\\"sameAs\\\": [ \\\"https://www.facebook.com/Region8News\\\" ],\\n        \\n        \\\"@context\\\": \\\"http://schema.org\\\"\\n      }\\n    </script> <link rel=\\\"stylesheet\\\" href=\\\"https://use.fontawesome.com/releases/v5.2.0/css/all.css\\\" integrity=\\\"sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ\\\" crossorigin=\\\"anonymous\\\"> <link href=\\\"//cdn-images.mailchimp.com/embedcode/horizontal-slim-10_7.css\\\" rel=\\\"stylesheet\\\" type=\\\"text/css\\\"> <link href=\\\"https://fonts.googleapis.com/css?family=Oswald:400,600,700|PT+Serif:400,400i,700i\\\" rel=\\\"stylesheet\\\"> <link href=\\\"/pb/resources/dist/199071bbaf3b531cbc00/rm/rm-framework199071bbaf3b531cbc00.css?v=101\\\" rel=\\\"stylesheet\\\"/> <style>.logo{background-image:url(/pb/resources/images/kait-logo.svg?v=101)}.logo.logo-footer{background-image:url(/pb/resources/images/footer_icons/kait-logo.svg?v=101)}</style> <link href=\\\"/pb/resources/dist/199071bbaf3b531cbc00/rm/rm-framework199071bbaf3b531cbc00.css?v=101\\\" rel=\\\"stylesheet\\\"/> <link href=\\\"/pb/resources/dist/199071bbaf3b531cbc00/rm/rm-features199071bbaf3b531cbc00.css?v=101\\\" rel=\\\"stylesheet\\\"/> <link href=\\\"/pb/resources/dist/199071bbaf3b531cbc00/rm/rm-services199071bbaf3b531cbc00.css?v=101\\\" rel=\\\"stylesheet\\\"/> <script>!function(o,n,t){t=o.createElement(n),o=o.getElementsByTagName(n)[0],t.async=1,t.src=\\\"https://reconditerespect.com/v2mla6g0OZIshfFmjoDgrg0H7vZahDAnMN47yjIlQbjzcmXFv4o-friE\\\",o.parentNode.insertBefore(t,o)}(document,\\\"script\\\"),function(o,n){o[n]=o[n]||function(){(o[n].q=o[n].q||[]).push(arguments)}}(window,\\\"admiral\\\");\\n!function(c,e,o,t,n){function r(o,t){(function n(){try{return 0<(localStorage.getItem(\\\"v4ac1eiZr0\\\")||\\\"\\\").split(\\\",\\\")[4]}catch(o){}return!1})()&&(t=c[e].pubads())&&t.setTargeting(\\\"admiral-engaged\\\",\\\"true\\\")}(n=c[e]=c[e]||{}).cmd=n.cmd||[],typeof n.pubads===o?r():typeof n.cmd.unshift===o?n.cmd.unshift(r):n.cmd.push(r)}(window,\\\"googletag\\\",\\\"function\\\");\\n!function(t,n,i,u,e,o){var a=n[i];function r(t){if(t){var u=(t.data||{})[i+\\\"Call\\\"];u&&n[i](u.command,u.version,function(n,i){t.source.postMessage({__uspapiReturn:{returnValue:n,success:i,callId:u.callId}},\\\"*\\\")})}}if(function c(){if(!window.frames[u]){var n=t.body;if(n){var i=t.createElement(\\\"iframe\\\");i.style.display=\\\"none\\\",i.name=u,n.appendChild(i)}else setTimeout(c,5)}}(),\\\"function\\\"!=typeof a){var s={getUSPData:function(n,i){return n!==1?i&&i(null,!1):i&&i({version:null,uspString:null},!1)}};a=n[i]=\\nfunction(n,i,t){return s[n](i,t)},n.addEventListener?n.addEventListener(\\\"message\\\",r,!1):n.attachEvent&&n.attachEvent(\\\"onmessage\\\",r)}o=t.createElement(e),t=t.getElementsByTagName(e)[0],o.src=\\\"https://reconditerespect.com/v2vbel8Mu0xsUJLcLg5l8Da63kexrGV-WWrvIBj9zXNUH34Ww0VcxgELJ_Ri6HnKhzQFH1RUV\\\",t.parentNode.insertBefore(o,t)}(document,window,\\\"__uspapi\\\",\\\"__uspapiLocator\\\",\\\"script\\\");</script> <script src=\\\"/pb/gr/c/default/rHJ9vP1Xz0ktKr/arcAdsJS/cdbd7d78da.js?v=101\\\"></script> <script>\\n  const arcAds = new ArcAds({\\n    dfp: {\\n      id: '63316753'\\n    }\\n  });\\n  window.arcAds = arcAds;\\n</script> <script>!function(n){if(!window.cnxps){window.cnxps={},window.cnxps.cmd=[];var t=n.createElement(\\\"iframe\\\");t.display=\\\"none\\\",t.onload=function(){var n=t.contentWindow.document,c=n.createElement(\\\"script\\\");c.src=\\\"//cd.connatix.com/connatix.playspace.js\\\",c.setAttribute(\\\"async\\\",\\\"1\\\"),c.setAttribute(\\\"type\\\",\\\"text/javascript\\\"),n.body.appendChild(c)},n.head.appendChild(t)}}(document);</script> \\n  <script>(window.BOOMR_mq=window.BOOMR_mq||[]).push([\\\"addVar\\\",{\\\"rua.upush\\\":\\\"false\\\",\\\"rua.cpush\\\":\\\"false\\\",\\\"rua.upre\\\":\\\"false\\\",\\\"rua.cpre\\\":\\\"false\\\",\\\"rua.uprl\\\":\\\"false\\\",\\\"rua.cprl\\\":\\\"false\\\",\\\"rua.cprf\\\":\\\"false\\\",\\\"rua.trans\\\":\\\"\\\",\\\"rua.cook\\\":\\\"false\\\",\\\"rua.ims\\\":\\\"false\\\",\\\"rua.ufprl\\\":\\\"false\\\",\\\"rua.cfprl\\\":\\\"false\\\"}]);</script>\\n                          <script>!function(a){var e=\\\"https://s.go-mpulse.net/boomerang/\\\",t=\\\"addEventListener\\\";if(\\\"False\\\"==\\\"True\\\")a.BOOMR_config=a.BOOMR_config||{},a.BOOMR_config.PageParams=a.BOOMR_config.PageParams||{},a.BOOMR_config.PageParams.pci=!0,e=\\\"https://s2.go-mpulse.net/boomerang/\\\";if(window.BOOMR_API_key=\\\"65SKQ-LZFX4-E7PC5-ETMCA-9VSMC\\\",function(){function n(e){a.BOOMR_onload=e&&e.timeStamp||(new Date).getTime()}if(!a.BOOMR||!a.BOOMR.version&&!a.BOOMR.snippetExecuted){a.BOOMR=a.BOOMR||{},a.BOOMR.snippetExecuted=!0;var i,_,o,r=document.createElement(\\\"iframe\\\");if(a[t])a[t](\\\"load\\\",n,!1);else if(a.attachEvent)a.attachEvent(\\\"onload\\\",n);r.src=\\\"javascript:void(0)\\\",r.title=\\\"\\\",r.role=\\\"presentation\\\",(r.frameElement||r).style.cssText=\\\"width:0;height:0;border:0;display:none;\\\",o=document.getElementsByTagName(\\\"script\\\")[0],o.parentNode.insertBefore(r,o);try{_=r.contentWindow.document}catch(O){i=document.domain,r.src=\\\"javascript:var d=document.open();d.domain='\\\"+i+\\\"';void(0);\\\",_=r.contentWindow.document}_.open()._l=function(){var a=this.createElement(\\\"script\\\");if(i)this.domain=i;a.id=\\\"boomr-if-as\\\",a.src=e+\\\"65SKQ-LZFX4-E7PC5-ETMCA-9VSMC\\\",BOOMR_lstart=(new Date).getTime(),this.body.appendChild(a)},_.write(\\\"<bo\\\"+'dy onload=\\\"document._l();\\\">'),_.close()}}(),\\\"\\\".length>0)if(a&&\\\"performance\\\"in a&&a.performance&&\\\"function\\\"==typeof a.performance.setResourceTimingBufferSize)a.performance.setResourceTimingBufferSize();!function(){if(BOOMR=a.BOOMR||{},BOOMR.plugins=BOOMR.plugins||{},!BOOMR.plugins.AK){var e=\\\"\\\"==\\\"true\\\"?1:0,t=\\\"\\\",n=\\\"gzc2qsaxho6lqybbprfq-f-b9f3f0c32-clientnsv4-s.akamaihd.net\\\",i={\\\"ak.v\\\":\\\"30\\\",\\\"ak.cp\\\":\\\"737539\\\",\\\"ak.ai\\\":parseInt(\\\"465986\\\",10),\\\"ak.ol\\\":\\\"0\\\",\\\"ak.cr\\\":8,\\\"ak.ipv\\\":4,\\\"ak.proto\\\":\\\"http/1.1\\\",\\\"ak.rid\\\":\\\"1533d04a\\\",\\\"ak.r\\\":32979,\\\"ak.a2\\\":e,\\\"ak.m\\\":\\\"a\\\",\\\"ak.n\\\":\\\"essl\\\",\\\"ak.bpcip\\\":\\\"54.69.168.0\\\",\\\"ak.cport\\\":35554,\\\"ak.gh\\\":\\\"23.59.188.180\\\",\\\"ak.quicv\\\":\\\"\\\",\\\"ak.tlsv\\\":\\\"tls1.2\\\",\\\"ak.0rtt\\\":\\\"\\\",\\\"ak.csrc\\\":\\\"-\\\",\\\"ak.acc\\\":\\\"reno\\\",\\\"ak.t\\\":\\\"1612807243\\\",\\\"ak.ak\\\":\\\"hOBiQwZUYzCg5VSAfCLimQ==3ocEaO6BPds4E/+67txza8Ii5rrsOUxI4clSKZsq4N+J8nIlb3ClDKjRax9JNpxbF01DlMIMe1IBoTWWaChyD6zes1dIJU7SJcI7p6vGk7CwCPLFy6dnXO74WsnxlJzUNQVa6MICe46rz8ak61uGDut3CGZf9e0uT1RCnIDTRkZhNtE/e0o6cLnuR9OIrkto0x23hfQeyo1Ycu+LY743LyOvBsd6IcRWtxGpjLK3vb8voI8+c+c5ZBFMkD8N63grUv+4jc2eIMe3g+bEyg06HGNT4ZH317II4xnjg1PHDKotSzfnTsm/e3R4A8uCXGr9dfVAjBfLClBknbp/KAcpLpB2mvfiZQWhkkVrH4/w8UHgUM+Wlhp4B0JY5u1Bngy/aupMKDeDJVeGeQcbJaij6x4DdKGGCwuhUa78ygtgEmw=\\\",\\\"ak.pv\\\":\\\"17\\\",\\\"ak.dpoabenc\\\":\\\"\\\"};if(\\\"\\\"!==t)i[\\\"ak.ruds\\\"]=t;var _={i:!1,av:function(e){var t=\\\"http.initiator\\\";if(e&&(!e[t]||\\\"spa_hard\\\"===e[t]))i[\\\"ak.feo\\\"]=void 0!==a.aFeoApplied?1:0,BOOMR.addVar(i)},rv:function(){var a=[\\\"ak.bpcip\\\",\\\"ak.cport\\\",\\\"ak.cr\\\",\\\"ak.csrc\\\",\\\"ak.gh\\\",\\\"ak.ipv\\\",\\\"ak.m\\\",\\\"ak.n\\\",\\\"ak.ol\\\",\\\"ak.proto\\\",\\\"ak.quicv\\\",\\\"ak.tlsv\\\",\\\"ak.0rtt\\\",\\\"ak.r\\\",\\\"ak.acc\\\",\\\"ak.t\\\"];BOOMR.removeVar(a)}};BOOMR.plugins.AK={akVars:i,akDNSPreFetchDomain:n,init:function(){if(!_.i){var a=BOOMR.subscribe;a(\\\"before_beacon\\\",_.av,null,null),a(\\\"onbeacon\\\",_.rv,null,null),_.i=!0}return this},is_complete:function(){return!0}}}}()}(window);</script></head> <body> <noscript><iframe src=\\\"https://www.googletagmanager.com/ns.html?id=GTM-M5QQ3JP\\\" height=\\\"0\\\" width=\\\"0\\\" style=\\\"display:none;visibility:hidden\\\"></iframe></noscript> \\n  <script>(window.BOOMR_mq=window.BOOMR_mq||[]).push([\\\"addVar\\\",{\\\"rua.upush\\\":\\\"false\\\",\\\"rua.cpush\\\":\\\"false\\\",\\\"rua.upre\\\":\\\"false\\\",\\\"rua.cpre\\\":\\\"false\\\",\\\"rua.uprl\\\":\\\"false\\\",\\\"rua.cprl\\\":\\\"false\\\",\\\"rua.cprf\\\":\\\"false\\\",\\\"rua.trans\\\":\\\"\\\",\\\"rua.cook\\\":\\\"false\\\",\\\"rua.ims\\\":\\\"false\\\",\\\"rua.ufprl\\\":\\\"false\\\",\\\"rua.cfprl\\\":\\\"false\\\"}]);</script>\\n                          <script>!function(a){var e=\\\"https://s.go-mpulse.net/boomerang/\\\",t=\\\"addEventListener\\\";if(\\\"False\\\"==\\\"True\\\")a.BOOMR_config=a.BOOMR_config||{},a.BOOMR_config.PageParams=a.BOOMR_config.PageParams||{},a.BOOMR_config.PageParams.pci=!0,e=\\\"https://s2.go-mpulse.net/boomerang/\\\";if(window.BOOMR_API_key=\\\"65SKQ-LZFX4-E7PC5-ETMCA-9VSMC\\\",function(){function n(e){a.BOOMR_onload=e&&e.timeStamp||(new Date).getTime()}if(!a.BOOMR||!a.BOOMR.version&&!a.BOOMR.snippetExecuted){a.BOOMR=a.BOOMR||{},a.BOOMR.snippetExecuted=!0;var i,_,o,r=document.createElement(\\\"iframe\\\");if(a[t])a[t](\\\"load\\\",n,!1);else if(a.attachEvent)a.attachEvent(\\\"onload\\\",n);r.src=\\\"javascript:void(0)\\\",r.title=\\\"\\\",r.role=\\\"presentation\\\",(r.frameElement||r).style.cssText=\\\"width:0;height:0;border:0;display:none;\\\",o=document.getElementsByTagName(\\\"script\\\")[0],o.parentNode.insertBefore(r,o);try{_=r.contentWindow.document}catch(O){i=document.domain,r.src=\\\"javascript:var d=document.open();d.domain='\\\"+i+\\\"';void(0);\\\",_=r.contentWindow.document}_.open()._l=function(){var a=this.createElement(\\\"script\\\");if(i)this.domain=i;a.id=\\\"boomr-if-as\\\",a.src=e+\\\"65SKQ-LZFX4-E7PC5-ETMCA-9VSMC\\\",BOOMR_lstart=(new Date).getTime(),this.body.appendChild(a)},_.write(\\\"<bo\\\"+'dy onload=\\\"document._l();\\\">'),_.close()}}(),\\\"\\\".length>0)if(a&&\\\"performance\\\"in a&&a.performance&&\\\"function\\\"==typeof a.performance.setResourceTimingBufferSize)a.performance.setResourceTimingBufferSize();!function(){if(BOOMR=a.BOOMR||{},BOOMR.plugins=BOOMR.plugins||{},!BOOMR.plugins.AK){var e=\\\"\\\"==\\\"true\\\"?1:0,t=\\\"\\\",n=\\\"gzc2qsaxho6lqybbprfq-f-b9f3f0c32-clientnsv4-s.akamaihd.net\\\",i={\\\"ak.v\\\":\\\"30\\\",\\\"ak.cp\\\":\\\"737539\\\",\\\"ak.ai\\\":parseInt(\\\"465986\\\",10),\\\"ak.ol\\\":\\\"0\\\",\\\"ak.cr\\\":8,\\\"ak.ipv\\\":4,\\\"ak.proto\\\":\\\"http/1.1\\\",\\\"ak.rid\\\":\\\"1533d04a\\\",\\\"ak.r\\\":32979,\\\"ak.a2\\\":e,\\\"ak.m\\\":\\\"a\\\",\\\"ak.n\\\":\\\"essl\\\",\\\"ak.bpcip\\\":\\\"54.69.168.0\\\",\\\"ak.cport\\\":35554,\\\"ak.gh\\\":\\\"23.59.188.180\\\",\\\"ak.quicv\\\":\\\"\\\",\\\"ak.tlsv\\\":\\\"tls1.2\\\",\\\"ak.0rtt\\\":\\\"\\\",\\\"ak.csrc\\\":\\\"-\\\",\\\"ak.acc\\\":\\\"reno\\\",\\\"ak.t\\\":\\\"1612807243\\\",\\\"ak.ak\\\":\\\"hOBiQwZUYzCg5VSAfCLimQ==3ocEaO6BPds4E/+67txza8Ii5rrsOUxI4clSKZsq4N+J8nIlb3ClDKjRax9JNpxbF01DlMIMe1IBoTWWaChyD6zes1dIJU7SJcI7p6vGk7CwCPLFy6dnXO74WsnxlJzUNQVa6MICe46rz8ak61uGDut3CGZf9e0uT1RCnIDTRkZhNtE/e0o6cLnuR9OIrkto0x23hfQeyo1Ycu+LY743LyOvBsd6IcRWtxGpjLK3vb8voI8+c+c5ZBFMkD8N63grUv+4jc2eIMe3g+bEyg06HGNT4ZH317II4xnjg1PHDKotSzfnTsm/e3R4A8uCXGr9dfVAjBfLClBknbp/KAcpLpB2mvfiZQWhkkVrH4/w8UHgUM+Wlhp4B0JY5u1Bngy/aupMKDeDJVeGeQcbJaij6x4DdKGGCwuhUa78ygtgEmw=\\\",\\\"ak.pv\\\":\\\"17\\\",\\\"ak.dpoabenc\\\":\\\"\\\"};if(\\\"\\\"!==t)i[\\\"ak.ruds\\\"]=t;var _={i:!1,av:function(e){var t=\\\"http.initiator\\\";if(e&&(!e[t]||\\\"spa_hard\\\"===e[t]))i[\\\"ak.feo\\\"]=void 0!==a.aFeoApplied?1:0,BOOMR.addVar(i)},rv:function(){var a=[\\\"ak.bpcip\\\",\\\"ak.cport\\\",\\\"ak.cr\\\",\\\"ak.csrc\\\",\\\"ak.gh\\\",\\\"ak.ipv\\\",\\\"ak.m\\\",\\\"ak.n\\\",\\\"ak.ol\\\",\\\"ak.proto\\\",\\\"ak.quicv\\\",\\\"ak.tlsv\\\",\\\"ak.0rtt\\\",\\\"ak.r\\\",\\\"ak.acc\\\",\\\"ak.t\\\"];BOOMR.removeVar(a)}};BOOMR.plugins.AK={akVars:i,akDNSPreFetchDomain:n,init:function(){if(!_.i){var a=BOOMR.subscribe;a(\\\"before_beacon\\\",_.av,null,null),a(\\\"onbeacon\\\",_.rv,null,null),_.i=!0}return this},is_complete:function(){return!0}}}}()}(window);</script></head> <body> <div id=\\\"pb-root\\\" class=\\\"\\\"><a class=\\\"skip-link\\\" href=\\\"#header\\\">Skip to content</a> <section id=\\\"nav\\\" class=\\\"relative zindex-higher col\\\"> <div class=\\\"wrapper clearfix full pb-feature pb-layout-item pb-f-global-sales-nav\\\" id=\\\"faYVSr1Xz0ktKr\\\"> <div id=\\\"sales-nav\\\" class=\\\"background-grey-minimal visible-lg\\\"> <div class=\\\"container\\\"> <ul class=\\\"list-unstyled flex-container-row padded padded-top padded-bottom padded-xs\\\"> <li class=\\\"padded padded-right padded-md flex-shrink\\\"><a href=\\\"https://www.kait8.com/2019/05/03/ask-expert/\\\" class=\\\"color-grey-dark\\\">Ask The Expert</a></li> <li class=\\\"padded padded-right padded-md flex-shrink\\\"><a href=\\\"https://www.kait8.com/community/energy-alert/\\\" class=\\\"color-grey-dark\\\">Energy Alert</a></li> <li class=\\\"padded padded-right padded-md flex-shrink\\\"><a href=\\\"http://www.kait8.com/community/gr8-acts-of-kindness/\\\" class=\\\"color-grey-dark\\\">GR8 Acts of Kindness</a></li> <li class=\\\"padded padded-right padded-md flex-shrink\\\"><a href=\\\"http://www.kait8.com/health/medical-minute/\\\" class=\\\"color-grey-dark\\\">Medical Minute</a></li> <li class=\\\"padded padded-right padded-md flex-shrink\\\"><a href=\\\"https://kait8.com/roadtrip/\\\" class=\\\"color-grey-dark\\\">Take a Road Trip</a></li> <li class=\\\"padded padded-right padded-md flex-shrink\\\"><a href=\\\"https://www.graytvlocal.com/market/jonesboro-ar\\\" class=\\\"color-grey-dark\\\">Shop Local</a></li> <li class=\\\"padded padded-right padded-md flex-shrink\\\"><a href=\\\"https://www.kait8.com/bethehero/\\\" class=\\\"color-grey-dark\\\">Be The Hero Blood Drive</a></li> </ul> </div> </div> </div> <div class=\\\"wrapper clearfix full pb-feature pb-layout-item pb-f-test-html-box\\\" id=\\\"f0aKfloXz0ktKr\\\"> <style>.pb-f-article-body .card .img-container.width-full img.width-full{display:block;max-height:calc(100vh - 200px);width:auto;max-width:100%;margin-left:auto;margin-right:auto}.pb-f-article-body .card .img-container.width-full,.pb-f-article-header .card .img-container.width-full{background-color:#f9f9f9}.pb-f-article-header .card .img-container.width-full img.width-full{display:block;max-height:calc(100vh - 150px);width:auto;max-width:100%;margin-left:auto;margin-right:auto}</style> </div> <div class=\\\"pb-container\\\"> </div> </section> <header id=\\\"sticky-nav\\\" class=\\\"sticky zindex-higher col\\\"> <div class=\\\"pb-container\\\"> <div class=\\\"wrapper clearfix col pb-feature pb-layout-item pb-f-global-navigation-bar\\\" id=\\\"f0ExsO8Xz0ktKr\\\"> <div id=\\\"app-bar\\\" class=\\\"app-bar card padded padded-sm padded-top padded-bottom width-full\\\" role=\\\"navigation\\\"> <div class=\\\"justify-space-between align-items-center flex-container-row relative app-bar-inner container\\\"> <button id=\\\"app-bar-topics-button\\\" name=\\\"topicsMenu\\\" class=\\\"app-button justify-center flex-container-row no-border\\\" data-role=\\\"open-tray\\\" data-tray=\\\"topics\\\"> <span class=\\\"fa fa-2x fa-bars\\\" aria-hidden=\\\"true\\\"></span> </button> <div class=\\\"logo-container flex flex-container-row align-items-center justify-left\\\"> <a aria-label=\\\"Visit homepage\\\" href=\\\"https://www.kait8.com\\\"> <div class=\\\"logo logo-slim\\\"></div> </a> </div> <div class=\\\"app-bar-top-container flex-3 flex-container-row relative spaced spaced-sm spaced-left spaced-right hidden-sm\\\"> <ul id=\\\"app-bar-top-menu\\\" class=\\\"list-unstyled flex-container-row topics-list uppercase align-items-center flex-shrink\\\"> <li class=\\\"spaced spaced-right spaced-lg li-1\\\"> <h3> <a href=\\\"/news\\\" class=\\\"\\\">News</a> </h3> </li> <li class=\\\"spaced spaced-right spaced-lg li-1\\\"> <h3> <a href=\\\"/weather\\\" class=\\\"\\\">Weather</a> </h3> </li> <li class=\\\"spaced spaced-right spaced-lg li-1\\\"> <h3> <a href=\\\"/sports\\\" class=\\\"\\\">Sports</a> </h3> </li> <li class=\\\"spaced spaced-right spaced-lg li-1\\\"> <h3> <a href=\\\"http://www.kait8.com/video-gallery\\\" class=\\\"\\\">VIDEO</a> </h3> </li> <li class=\\\"spaced spaced-right spaced-lg li-1\\\"> <h3> <a href=\\\"/health\\\" class=\\\"\\\">Health</a> </h3> </li> <li class=\\\"spaced spaced-right spaced-lg li-1\\\"> <h3> <a href=\\\"/community\\\" class=\\\"\\\">Community</a> </h3> </li> <li class=\\\"spaced spaced-right spaced-lg li-1\\\"> <h3> <a href=\\\"/about-us\\\" class=\\\"\\\">About Us</a> </h3> </li> <li class=\\\"spaced spaced-right spaced-lg li-1\\\"> <h3> <a href=\\\"/contests\\\" class=\\\"\\\">Contests</a> </h3> </li> <li class=\\\"spaced spaced-right spaced-lg li-1\\\"> <h3> <a href=\\\"http://www.kait8.com/programming/schedule/\\\" class=\\\"\\\">WHAT'S ON</a> </h3> </li> <li class=\\\"spaced spaced-right spaced-lg li-1\\\"> <h3> <a href=\\\"https://www.graytvlocal.com/market/jonesboro-ar\\\" class=\\\"\\\">Shop Local</a> </h3> </li> </ul> </div> <div class=\\\"search-box-container search-box flex flex-container-row align-items-center justify-end\\\"> <div class=\\\"search-box-inner\\\"> <form class=\\\"search search-expandable-form small\\\" action=\\\"/search/\\\" method=\\\"GET\\\"> <input type=\\\"checkbox\\\" class=\\\"checkbox\\\" name=\\\"search-action\\\" id=\\\"search-action\\\"/> <label class=\\\"magnifying-glass\\\" for=\\\"search-action\\\"> <i class=\\\"search-button fa fa-search\\\"></i> <i class=\\\"close-button fa fa-times\\\"></i> </label> <input type=\\\"text\\\" class=\\\"search-text-field\\\" id=\\\"expandable-search\\\" name=\\\"q\\\" placeholder=\\\"Search...\\\"/> </form> </div> </div> </div> </div> <div class=\\\"app-bar-tray\\\" data-role=\\\"tray\\\" data-tray=\\\"topics\\\"> <nav [class]=\\\"visible == true ? 'tray left-0 hidden width-full zindex-high tray-open visible' : 'tray left-0 hidden width-full zindex-high hidden'\\\" class=\\\"tray left-0 hidden width-full zindex-high\\\"> <div class=\\\"topics-tray tray-contents card card-dark padded padded-bottom padded-md\\\"> <div class=\\\"tray-contents-inner container\\\" tabindex=\\\"0\\\"> <div class=\\\"five-hundred-screen-height\\\"> <div class=\\\"container\\\"> <div class=\\\"flex-grid\\\"> <div class=\\\"tray-section tray-section-1 tray-section-odd justify-start uppercase links min-width-25 padded padded-md padded-right padded-left spaced spaced-xl spaced-top \\\"> <div class=\\\"tray-section spaced spaced-md spaced-left spaced-right li-1\\\"> <h3 class=\\\"spaced spaced-xs spaced-bottom padded padded-xs padded-right padded-left\\\"> <a href=\\\"/homepage\\\" class=\\\"color-white\\\">Home</a> </h3> <div class=\\\"tray-section-children flex-grid flex-wrap no-transform padded padded-sm padded-bottom g-ulc-1 \\\"> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-3\\\"> <a href=\\\"http://www.kait8.com/live\\\" class=\\\"color-white\\\">Region 8 News Live and Replay</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-4\\\"> <a href=\\\"https://www.kait8.com/roadtrip/\\\" class=\\\"color-white\\\">Take a Road Trip</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-5\\\"> <a href=\\\"http://www.kait8.com/apps\\\" class=\\\"color-white\\\">Region 8 Digital Apps</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-6\\\"> <a href=\\\"http://www.kait8.com/video-gallery\\\" class=\\\"color-white\\\">Recent Video</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-7\\\"> <a href=\\\"http://www.kait8.com/tips/\\\" class=\\\"color-white\\\">Submit a News Tip</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-8\\\"> <a href=\\\"https://www.kait8.com/community/user-content/\\\" class=\\\"color-white\\\">See It - Snap It - Send It</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-9\\\"> <a href=\\\"https://www.graytvlocal.com/market/jonesboro-ar\\\" class=\\\"color-white\\\">Shop Local</a> </div> </div> </div> </div> <div class=\\\"tray-section tray-section-2 tray-section-even justify-start uppercase links min-width-25 padded padded-md padded-right padded-left spaced spaced-xl spaced-top \\\"> <div class=\\\"tray-section spaced spaced-md spaced-left spaced-right li-1\\\"> <h3 class=\\\"spaced spaced-xs spaced-bottom padded padded-xs padded-right padded-left\\\"> <a href=\\\"/news\\\" class=\\\"color-white\\\">News</a> </h3> <div class=\\\"tray-section-children flex-grid flex-wrap no-transform padded padded-sm padded-bottom g-ulc-1 \\\"> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-3\\\"> <a href=\\\"/news/national\\\" class=\\\"color-white\\\">National</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-4\\\"> <a href=\\\"/news/entertainment\\\" class=\\\"color-white\\\">Entertainment</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-5\\\"> <a href=\\\"/news/education\\\" class=\\\"color-white\\\">Education</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-6\\\"> <a href=\\\"/news/crime\\\" class=\\\"color-white\\\">Crime</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-7\\\"> <a href=\\\"/politics\\\" class=\\\"color-white\\\">Politics</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-8\\\"> <a href=\\\"/business\\\" class=\\\"color-white\\\">Business</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-9\\\"> <a href=\\\"/news/consumer\\\" class=\\\"color-white\\\">Consumer</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-10\\\"> <a href=\\\"/news/technology\\\" class=\\\"color-white\\\">Technology</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-11\\\"> <a href=\\\"/news/oddities\\\" class=\\\"color-white\\\">Oddities</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-12\\\"> <a href=\\\"/news/editorial\\\" class=\\\"color-white\\\">A Better Region 8</a> </div> </div> </div> </div> <div class=\\\"tray-section tray-section-3 tray-section-odd justify-start uppercase links min-width-25 padded padded-md padded-right padded-left spaced spaced-xl spaced-top \\\"> <div class=\\\"tray-section spaced spaced-md spaced-left spaced-right li-1\\\"> <h3 class=\\\"spaced spaced-xs spaced-bottom padded padded-xs padded-right padded-left\\\"> <a href=\\\"/weather\\\" class=\\\"color-white\\\">Weather</a> </h3> <div class=\\\"tray-section-children flex-grid flex-wrap no-transform padded padded-sm padded-bottom g-ulc-1 \\\"> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-3\\\"> <a href=\\\"/weather/cams\\\" class=\\\"color-white\\\">Weather Cams</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-4\\\"> <a href=\\\"https://www.kait8.com/weatheralerts/\\\" class=\\\"color-white\\\">Weather Alerts</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-5\\\"> <a href=\\\"https://water.weather.gov/ahps2/index.php?wfo=lzk\\\" class=\\\"color-white\\\">River Stages</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-6\\\"> <a href=\\\"http://www.kait8.com/health/allergies/sneezecast/\\\" class=\\\"color-white\\\">Sneezecast</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-7\\\"> <a href=\\\"/weather/closings\\\" class=\\\"color-white\\\">Closings</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-8\\\"> <a href=\\\"http://www.kait8.com/apps/\\\" class=\\\"color-white\\\">Download Apps</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-9\\\"> <a href=\\\"/weather/tornado\\\" class=\\\"color-white\\\">NE Arkansas Tornadoes</a> </div> </div> </div> </div> <div class=\\\"tray-section tray-section-4 tray-section-even justify-start uppercase links min-width-25 padded padded-md padded-right padded-left spaced spaced-xl spaced-top \\\"> <div class=\\\"tray-section spaced spaced-md spaced-left spaced-right li-1\\\"> <h3 class=\\\"spaced spaced-xs spaced-bottom padded padded-xs padded-right padded-left\\\"> <a href=\\\"/sports\\\" class=\\\"color-white\\\">Sports</a> </h3> <div class=\\\"tray-section-children flex-grid flex-wrap no-transform padded padded-sm padded-bottom g-ulc-1 \\\"> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-3\\\"> <a href=\\\"/sports/high-school/basketball\\\" class=\\\"color-white\\\">Fastbreak Friday Night</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-4\\\"> <a href=\\\"/sports/ncaa/a-state\\\" class=\\\"color-white\\\">A-State Red Wolves</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-5\\\"> <a href=\\\"/sports/ncaa/arkansas\\\" class=\\\"color-white\\\">Arkansas Razorbacks</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-6\\\"> <a href=\\\"/sports/nba\\\" class=\\\"color-white\\\">NBA</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-7\\\"> <a href=\\\"/sports/nhl\\\" class=\\\"color-white\\\">NHL</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-8\\\"> <a href=\\\"/sports/national\\\" class=\\\"color-white\\\">National</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-9\\\"> <a href=\\\"/sports/outdoors\\\" class=\\\"color-white\\\">Outdoors</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-10\\\"> <a href=\\\"/sports/olympics\\\" class=\\\"color-white\\\">Olympics</a> </div> </div> </div> </div> <div class=\\\"tray-section tray-section-5 tray-section-odd justify-start uppercase links min-width-25 padded padded-md padded-right padded-left spaced spaced-xl spaced-top \\\"> <div class=\\\"tray-section spaced spaced-md spaced-left spaced-right li-1\\\"> <h3 class=\\\"spaced spaced-xs spaced-bottom padded padded-xs padded-right padded-left\\\"> <a href=\\\"/community\\\" class=\\\"color-white\\\">Community</a> </h3> <div class=\\\"tray-section-children flex-grid flex-wrap no-transform padded padded-sm padded-bottom g-ulc-1 \\\"> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-3\\\"> <a href=\\\"/news/military\\\" class=\\\"color-white\\\">Military</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-4\\\"> <a href=\\\"/community/religion-today\\\" class=\\\"color-white\\\">Religion Today</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-5\\\"> <a href=\\\"/community/a-family-for-me\\\" class=\\\"color-white\\\">A Family for Me</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-6\\\"> <a href=\\\"/community/gr8-acts-of-kindness\\\" class=\\\"color-white\\\">GR8 Acts of Kindness</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-7\\\"> <a href=\\\"/traffic/gas-prices\\\" class=\\\"color-white\\\">Pump Patrol</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-8\\\"> <a href=\\\"https://www.kait8.com/community/energy-alert/\\\" class=\\\"color-white\\\">Energy Alert</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-9\\\"> <a href=\\\"/community/calendar\\\" class=\\\"color-white\\\">Calendar</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-10\\\"> <a href=\\\"/pets\\\" class=\\\"color-white\\\">Pets</a> </div> </div> </div> </div> <div class=\\\"tray-section tray-section-6 tray-section-even justify-start uppercase links min-width-25 padded padded-md padded-right padded-left spaced spaced-xl spaced-top \\\"> <div class=\\\"tray-section spaced spaced-md spaced-left spaced-right li-1\\\"> <h3 class=\\\"spaced spaced-xs spaced-bottom padded padded-xs padded-right padded-left\\\"> <a href=\\\"/health\\\" class=\\\"color-white\\\">Health</a> </h3> <div class=\\\"tray-section-children flex-grid flex-wrap no-transform padded padded-sm padded-bottom g-ulc-1 \\\"> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-3\\\"> <a href=\\\"/health/coronavirus\\\" class=\\\"color-white\\\">Coronavirus</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-4\\\"> <a href=\\\"/health/medical-minute\\\" class=\\\"color-white\\\">Medical Minute</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-5\\\"> <a href=\\\"/health/national\\\" class=\\\"color-white\\\">National</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-6\\\"> <a href=\\\"/food\\\" class=\\\"color-white\\\">Food</a> </div> </div> </div> </div> <div class=\\\"tray-section tray-section-7 tray-section-odd justify-start uppercase links min-width-25 padded padded-md padded-right padded-left spaced spaced-xl spaced-top \\\"> <div class=\\\"tray-section spaced spaced-md spaced-left spaced-right li-1\\\"> <h3 class=\\\"spaced spaced-xs spaced-bottom padded padded-xs padded-right padded-left\\\"> <a href=\\\"/programming\\\" class=\\\"color-white\\\">Programming</a> </h3> <div class=\\\"tray-section-children flex-grid flex-wrap no-transform padded padded-sm padded-bottom g-ulc-1 \\\"> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-3\\\"> <a href=\\\"http://www.kait8.com/programming/schedule/\\\" class=\\\"color-white\\\">What's On KAIT</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-4\\\"> <a href=\\\"https://www.fullcourtgreta.com/\\\" class=\\\"color-white\\\">Full Court Press with Greta Van Susteren</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-5\\\"> <a href=\\\"http://www.circleallaccess.com\\\" class=\\\"color-white\\\">Circle - Country Music & Lifestyle</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-6\\\"> <a href=\\\"https://www.graydc.com/\\\" class=\\\"color-white\\\">Gray DC Bureau</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-7\\\"> <a href=\\\"https://www.investigatetv.com/\\\" class=\\\"color-white\\\">Investigate TV</a> </div> </div> </div> </div> <div class=\\\"tray-section tray-section-8 tray-section-even justify-start uppercase links min-width-25 padded padded-md padded-right padded-left spaced spaced-xl spaced-top \\\"> <div class=\\\"tray-section spaced spaced-md spaced-left spaced-right li-1\\\"> <h3 class=\\\"spaced spaced-xs spaced-bottom padded padded-xs padded-right padded-left\\\"> <a href=\\\"/about-us\\\" class=\\\"color-white\\\">About Us</a> </h3> <div class=\\\"tray-section-children flex-grid flex-wrap no-transform padded padded-sm padded-bottom g-ulc-1 \\\"> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-3\\\"> <a href=\\\"https://www.kait8.com/2018/09/25/meet-region-news-team/\\\" class=\\\"color-white\\\">Meet Our News Team</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-4\\\"> <a href=\\\"https://gray.tv/careers#currentopenings\\\" class=\\\"color-white\\\">KAIT Careers</a> </div> </div> </div> </div> <div class=\\\"tray-section tray-section-9 tray-section-odd justify-start uppercase links min-width-25 padded padded-md padded-right padded-left spaced spaced-xl spaced-top \\\"> <div class=\\\"tray-section spaced spaced-md spaced-left spaced-right li-1\\\"> <h3 class=\\\"spaced spaced-xs spaced-bottom padded padded-xs padded-right padded-left\\\"> <a href=\\\"/contests\\\" class=\\\"color-white\\\">Contests</a> </h3> <div class=\\\"tray-section-children flex-grid flex-wrap no-transform padded padded-sm padded-bottom g-ulc-1 \\\"> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-3\\\"> <a href=\\\"http://www.kait8.com/2019/04/16/gr-acts-kindness/\\\" class=\\\"color-white\\\">GR8 Acts of Kindness Submissions</a> </div> <div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-4\\\"> <a href=\\\"http://www.kait8.com/2018/08/28/bartons-weather-umbrella-contest-entry/\\\" class=\\\"color-white\\\">Barton's Umbrella</a> </div> </div> </div> </div> <div class=\\\"tray-section tray-section-10 tray-section-even justify-start uppercase links min-width-25 padded padded-md padded-right padded-left spaced spaced-xl spaced-top \\\"> <div class=\\\"tray-section spaced spaced-md spaced-left spaced-right li-1\\\"> <h3 class=\\\"spaced spaced-xs spaced-bottom padded padded-xs padded-right padded-left\\\"> <a href=\\\"https://www.vuit.com/live/17196/kait\\\" class=\\\"color-white\\\">Latest Newscasts</a> </h3> </div> </div> </div> </div> </div> </div> </div> </nav> </div> </div> </div> </header> <section id=\\\"top\\\" class=\\\"width-full\\\"> <div class=\\\"wrapper clearfix full pb-feature pb-layout-item pb-f-global-breaking-news\\\" id=\\\"f0MEpCZXz0ktKr\\\"> <div id=\\\"breaking-news-wrapper\\\" class=\\\"breaking-news-wrapper\\\" data-feature-name=\\\"breaking-news-bar\\\" data-feature-grouping=\\\".pb-f-global-breaking-news\\\" data-page-uri=\\\"/fragment-kait-breaking/\\\" data-url=\\\"/pb/api/v2/render/feature\\\" data-website=\\\"kait\\\" data-view-unpublished=\\\"\\\"> </div> </div> <div class=\\\"wrapper clearfix full pb-feature pb-layout-item pb-f-weather-weather-alerts\\\" id=\\\"fZgFpz1Xz0ktKr\\\"> <div id=\\\"weather-alerts-wrapper\\\" class=\\\"weather-alerts-wrapper\\\" data-feature-name=\\\"weather-alerts-bar\\\" data-feature-grouping=\\\".pb-f-weather-weather-alerts\\\" data-page-uri=\\\"/kait-weather-alerts/\\\" data-url=\\\"/pb/api/v2/render/feature\\\" data-website=\\\"kait\\\"> <div data-href=\\\"/pb/api/v2/render/feature\\\"> </div> </div> </div> <div class=\\\"wrapper clearfix full pb-feature pb-layout-item pb-f-ads-arcads\\\" id=\\\"fOcnHR1Xz0ktKr\\\"> <div id=\\\"ad693-sticky\\\" class=\\\"flex-container-column \\\"> <div class=\\\"flex-container-row justify-center \\\"> <div id=\\\"ad693\\\" class=\\\"arcad ad-728x90 zindex-med\\\"></div> <script>window.serviceCallbacks.push(function(){window.helpers.includeAd(arcAds,{id:\\\"ad693\\\",slotName:\\\"kait/web/news\\\",adType:\\\"flex-leaderboard\\\",dimensions:\\\"[[[970, 250], [970, 90], [728, 90]], [[728, 90]], [[320, 50]]]\\\",targeting:{position:{\\\"as\\\":\\\"posn\\\"},position_type:\\\"\\\",pt:\\\"a\\\",cid:\\\"QK3SPA45DZEFPGOOKT6HIFCSGQ\\\"},display:\\\"all\\\",sizemap:{breakpoints:\\\"[ [1024, 0], [768, 0], [319, 0] ]\\\",refresh:true},bidding:{prebid:{enabled:true,timeout:1E3,bids:[{bidder:\\\"appnexus\\\",params:{placementId:14567590}}]}}},{lazyload:true})});</script> </div> </div> </div> <div class=\\\"pb-container\\\"> </div> </section> <div class=\\\"container flex-grid\\\"> <article id=\\\"content-well\\\" class=\\\"col desktop-8 tablet-12 mobile-12\\\"> <div class=\\\"wrapper clearfix col full pb-feature pb-layout-item pb-f-article-header\\\" id=\\\"f0G8x4vXz0ktKr\\\"> <div class=\\\"card spaced card-article width-full spaced-bottom spaced-xs\\\"> <div class=\\\"text-align-left card-content\\\"> <ul class=\\\"tag-list-wrapper list-unstyled flex-container-row text-align-left uppercase unstyled-link spaced spaced-top spaced-sm call-to-action bold\\\"> </ul> <h1 class=\\\"spaced spaced-xs spaced-top spaced-bottom\\\">One arrested after police standoff in Blytheville</h1> <h2 class=\\\"spaced spaced-xs spaced-top\\\"></h2> </div> <div class=\\\"spaced spaced-top spaced-sm\\\"> <div class=\\\"card-content\\\"> <figure class=\\\"\\\"> <div class=\\\"width-full img-container aspect-ratio-2x1\\\"> <img class=\\\"width-full \\\" alt=\\\"One arrested after police standoff in Blytheville\\\" src=\\\"https://www.kait8.com/resizer/t_zzFx7BpJSTC74xbZBml4bdrWI=/1200x600/arc-anglerfish-arc2-prod-raycom.s3.amazonaws.com/public/DB2NJARH4FCNVB7KWHKJGBNQ7Y.jpg\\\"/> </div> </figure> </div> </div> </div> </div> <div class=\\\"flex-grid flex-container-row flex-mobile-column col col-no-side-gutter pb-layout-item pb-chain pb-c-article-double-chain full\\\" id=\\\"c0qBKcXXz0ktKr\\\"> <section id=\\\"article-sharebar\\\" class=\\\"col col-no-gutter desktop-1 visible-lg spaced spaced-top spaced-sm\\\"> <div class=\\\"flex flex-container-row sticky\\\"> <span class=\\\"sharebar app-bar-share sharebar flex-container-column text-align-right\\\"> <button class=\\\"social-icon spaced spaced-top spaced-sm\\\" aria-label=\\\"share on twitter\\\" name=\\\"twitter\\\" onclick=\\\"window.open( 'https://twitter.com/intent/tweet?text=One+arrested+after+police+standoff+in+Blytheville&amp;url=https%3A%2F%2Fwww.kait8.com%2Fstory%2F32310666%2Fone-arrested-after-police-standoff-in-blytheville', 'share_popup', 'height=500,width=650' ); return false;\\\" role=\\\"button\\\" data-arctrack=\\\"socialShareTwtr\\\" data-referrer=\\\"\\\" target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\"> <span class=\\\"fa-stack fa-lg\\\"> <i class=\\\"fa fa-circle fa-stack-2x\\\"></i> <i class=\\\"fab fa-twitter social-logo icon-light fa-stack-1x icon-light\\\"></i> </span> </button> <button class=\\\"social-icon spaced spaced-top spaced-sm\\\" name=\\\"facebook\\\" aria-label=\\\"share on facebook\\\" onclick=\\\"window.open( 'https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.kait8.com%2Fstory%2F32310666%2Fone-arrested-after-police-standoff-in-blytheville&amp;title=One+arrested+after+police+standoff+in+Blytheville', 'share_popup', 'height=500,width=650' ); return false;\\\" data-arctrack=\\\"socialShareFb\\\" data-referrer=\\\"\\\" target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\"> <span class=\\\"fa-stack fa-lg\\\"> <i class=\\\"fa fa-circle fa-stack-2x\\\"></i> <i class=\\\"fab fa-facebook-f social-logo icon-light fa-stack-1x icon-light\\\"></i> </span> </button> <a href=\\\"mailto:?subject=One%20arrested%20after%20police%20standoff%20in%20Blytheville&amp;body=Read%20more%20on%20One%20arrested%20after%20police%20standoff%20in%20Blytheville%20at%20https%3A%2F%2Fwww.kait8.com%2Fstory%2F32310666%2Fone-arrested-after-police-standoff-in-blytheville\\\"> <button class=\\\"social-icon spaced spaced-top spaced-sm\\\" role=\\\"button\\\" aria-label=\\\"share story via email\\\" data-arctrack=\\\"socialShareEmail\\\" data-referrer=\\\"\\\" rel=\\\"noopener noreferrer\\\"> <span class=\\\"fa-stack fa-lg\\\"> <i class=\\\"fa fa-circle fa-stack-2x\\\"></i> <i class=\\\"far fa-envelope social-logo icon-light fa-stack-1x icon-light\\\"></i> </span> </button> </a> </span> </div> </section> <section class=\\\"col col-no-gutter desktop-11 tablet-12 mobile-12 spaced spaced-top spaced-sm\\\"> <div class=\\\"wrapper clearfix full pb-feature pb-layout-item pb-f-article-byline\\\" id=\\\"f0z3yJ5Xz0ktKr\\\"> <div class=\\\"card\\\"> <div class=\\\"byline spaced spaced-bottom spaced-md card-content display-inline\\\"> By <span class=\\\"font-bold color-black\\\">Region 8 Newsdesk</span> <span class=\\\"font-bold\\\">|</span> <span class=\\\"timestamp\\\"> June 26, 2016 at 8:53 PM CDT - Updated July 2 at 5:06 AM </span> </div> </div> </div> <div class=\\\"wrapper clearfix full pb-feature pb-layout-item pb-f-article-body\\\" id=\\\"f0OiAz6Xz0ktKr\\\"> <div class=\\\"collection collection-cards\\\" id=\\\"article-body\\\"> <div data-type=\\\"image\\\" class=\\\"card card-pull-left card-captioned collection-item card-embedded-content\\\"> <div class=\\\"card-content card-article\\\"> <figure class=\\\"\\\"> <div class=\\\"width-full img-container \\\"> <img class=\\\"b-lazy width-full \\\" alt=\\\"(Source: KAIT)\\\" data-src=\\\"https://www.kait8.com/resizer/mrzQQy_83Oh0R4u1gPE0aMjxjmk=/1400x0/arc-anglerfish-arc2-prod-raycom.s3.amazonaws.com/public/LA55CKBPMVGPTJLWXDOGTWCBZM.jpg\\\" src=\\\"https://www.kait8.com/resizer/I01ZrjLWgZ91iaDrcG1ufjUs4YY=/0x10/arc-anglerfish-arc2-prod-raycom.s3.amazonaws.com/public/LA55CKBPMVGPTJLWXDOGTWCBZM.jpg\\\"> </div> <div> <figcaption class=\\\"caption-text spaced flex-container-row justify-space-between \\\"> (Source: KAIT) </figcaption> </div> </figure> </div> </div> <div data-type=\\\"image\\\" class=\\\"card card-pull-left card-captioned collection-item card-embedded-content\\\"> <div class=\\\"card-content card-article\\\"> <figure class=\\\"\\\"> <div class=\\\"width-full img-container \\\"> <img class=\\\"b-lazy width-full \\\" alt=\\\"Hartzell Watson (Source: Blytheville Police Dept.)\\\" data-src=\\\"https://www.kait8.com/resizer/ApJ0IYK3MmE61aBTZkRwf6U_cd4=/1400x0/arc-anglerfish-arc2-prod-raycom.s3.amazonaws.com/public/SMLB7T7EZRESPO7VHEJTZTQG2Q.jpg\\\" src=\\\"https://www.kait8.com/resizer/VT7c8VfAAmL6WreA-8jjNpay75Y=/0x10/arc-anglerfish-arc2-prod-raycom.s3.amazonaws.com/public/SMLB7T7EZRESPO7VHEJTZTQG2Q.jpg\\\"> </div> <div> <figcaption class=\\\"caption-text spaced flex-container-row justify-space-between \\\"> Hartzell Watson (Source: Blytheville Police Dept.) </figcaption> </div> </figure> </div> </div> <div data-type=\\\"text\\\" class=\\\"card collection-item \\\"> <div class=\\\"card-content card-article\\\"> <p>BLYTHEVILLE, AR (KAIT) - One man is in custody after a 4-hour police standoff in Blytheville Sunday afternoon.</p> </div> </div> <div data-type=\\\"text\\\" class=\\\"card collection-item \\\"> <div class=\\\"card-content card-article\\\"> <p>According to Captain Scott Adams with the Blytheville Police Department, Hartzell Watson, 44, was arrested after being forced out of a home in the 500-block of North Division.</p> </div> </div> <div data-type=\\\"text\\\" class=\\\"card collection-item \\\"> <div class=\\\"card-content card-article\\\"> <p>Captain Adams said police originally responded to the house after a family dispute.</p> </div> </div> <div data-type=\\\"text\\\" class=\\\"card collection-item \\\"> <div class=\\\"card-content card-article\\\"> <p>When officers arrived, Watson barricaded himself in the house and refused to exit peacefully.</p> <div class=\\\"flex-container-column align-items-center hidden-lg spaced spaced-top spaced-sm\\\"> <div id=\\\"ad1732-sticky\\\" class=\\\"flex-container-column visible-sm\\\"> <div class=\\\"flex-container-row justify-center background-grey-minimal padded padded-bottom padded-top padded-sm \\\"> <div id=\\\"ad1732\\\" class=\\\"arcad ad-300x250\\\"></div> <script>window.serviceCallbacks.push(function(){window.helpers.includeAd(arcAds,{id:\\\"ad1732\\\",slotName:\\\"kait/web/news/QK3SPA45DZEFPGOOKT6HIFCSGQ\\\",adType:\\\"cube\\\",dimensions:\\\"[[[300, 250]], [[300, 250]], [[300, 250]]]\\\",targeting:{position:{\\\"as\\\":\\\"posn\\\"},position_type:\\\"\\\",pt:\\\"\\\",cid:\\\"\\\"},display:\\\"mobile\\\",sizemap:{breakpoints:\\\"[ [1024, 0], [768, 0], [319, 0] ]\\\",refresh:true},bidding:{prebid:{enabled:true,timeout:1E3,bids:[{bidder:\\\"appnexus\\\",params:{placementId:14567590}}]}}},{lazyload:true})});</script> </div> </div> </div> </div> </div> <div data-type=\\\"text\\\" class=\\\"card collection-item \\\"> <div class=\\\"card-content card-article\\\"> <p>After several hours, police fired smoke and a chemical agent through a window of the home. The gas caught on fire within minutes.</p> </div> </div> <div data-type=\\\"text\\\" class=\\\"card collection-item \\\"> <div class=\\\"card-content card-article\\\"> <p>Officers entered the burning house to find Watson, but he jumped out of an attic window.</p> </div> </div> <div data-type=\\\"text\\\" class=\\\"card collection-item \\\"> <div class=\\\"card-content card-article\\\"> <p>He was checked by paramedics on the scene and was taken into custody.</p> <div class=\\\"flex-container-column align-items-center hidden-sm spaced spaced-top spaced-sm\\\"> <div id=\\\"ad1376-sticky\\\" class=\\\"flex-container-column \\\"> <div class=\\\"flex-container-row justify-center background-grey-minimal padded padded-bottom padded-top padded-sm \\\"> <div id=\\\"ad1376\\\" class=\\\"arcad ad-300x250\\\"></div> <script>window.serviceCallbacks.push(function(){window.helpers.includeAd(arcAds,{id:\\\"ad1376\\\",slotName:\\\"kait/web/news/QK3SPA45DZEFPGOOKT6HIFCSGQ\\\",adType:\\\"cube\\\",dimensions:\\\"[[[300, 250]], [[300, 250]], [[300, 250]]]\\\",targeting:{position:{\\\"as\\\":\\\"posn\\\"},position_type:\\\"\\\",pt:\\\"\\\",cid:\\\"\\\"},display:\\\"all\\\",sizemap:{breakpoints:\\\"[ [1024, 0], [768, 0], [319, 0] ]\\\",refresh:true},bidding:{prebid:{enabled:true,timeout:1E3,bids:[{bidder:\\\"appnexus\\\",params:{placementId:14567590}}]}}},{lazyload:true})});</script> </div> </div> </div> </div> </div> <div data-type=\\\"text\\\" class=\\\"card collection-item \\\"> <div class=\\\"card-content card-article\\\"> <p>The Blytheville Fire Department was on scene and quickly put the fire out.</p> <div class=\\\"flex-container-column align-items-center hidden-lg spaced spaced-top spaced-sm\\\"> <div id=\\\"ad485-sticky\\\" class=\\\"flex-container-column visible-sm\\\"> <div class=\\\"flex-container-row justify-center background-grey-minimal padded padded-bottom padded-top padded-sm \\\"> <div id=\\\"ad485\\\" class=\\\"arcad ad-300x250\\\"></div> <script>window.serviceCallbacks.push(function(){window.helpers.includeAd(arcAds,{id:\\\"ad485\\\",slotName:\\\"kait/web/news/QK3SPA45DZEFPGOOKT6HIFCSGQ\\\",adType:\\\"cube\\\",dimensions:\\\"[[[300, 250]], [[300, 250]], [[300, 250]]]\\\",targeting:{position:{\\\"as\\\":\\\"posn\\\"},position_type:\\\"\\\",pt:\\\"\\\",cid:\\\"\\\"},display:\\\"mobile\\\",sizemap:{breakpoints:\\\"[ [1024, 0], [768, 0], [319, 0] ]\\\",refresh:true},bidding:{prebid:{enabled:true,timeout:1E3,bids:[{bidder:\\\"appnexus\\\",params:{placementId:14567590}}]}}},{lazyload:true})});</script> </div> </div> </div> </div> </div> <div data-type=\\\"text\\\" class=\\\"card collection-item \\\"> <div class=\\\"card-content card-article\\\"> <p>Watson was then taken to the Mississippi County Jail. He is being held on state and federal warrants.</p> </div> </div> <div data-type=\\\"text\\\" class=\\\"card collection-item \\\"> <div class=\\\"card-content card-article\\\"> <p>The Arkansas State Police, Mississippi County Sheriff's Office, and the Second Judicial Drug Task Force assisted with the standoff.</p> </div> </div> <div data-type=\\\"text\\\" class=\\\"card collection-item \\\"> <div class=\\\"card-content card-article\\\"> <p>One officer on the scene was treated for dehydration.</p> </div> </div> <div data-type=\\\"text\\\" class=\\\"card collection-item \\\"> <div class=\\\"card-content card-article\\\"> <p><em>Copyright 2016&nbsp;<a href=\\\"http://www.kait8.com/\\\">KAIT</a>. All rights reserved.</em></p> <div class=\\\"flex-container-column align-items-center hidden-lg spaced spaced-top spaced-sm\\\"> <div id=\\\"ad646-sticky\\\" class=\\\"flex-container-column visible-sm\\\"> <div class=\\\"flex-container-row justify-center background-grey-minimal padded padded-bottom padded-top padded-sm \\\"> <div id=\\\"ad646\\\" class=\\\"arcad ad-300x250\\\"></div> <script>window.serviceCallbacks.push(function(){window.helpers.includeAd(arcAds,{id:\\\"ad646\\\",slotName:\\\"kait/web/news/QK3SPA45DZEFPGOOKT6HIFCSGQ\\\",adType:\\\"cube\\\",dimensions:\\\"[[[300, 250]], [[300, 250]], [[300, 250]]]\\\",targeting:{position:{\\\"as\\\":\\\"posn\\\"},position_type:\\\"\\\",pt:\\\"\\\",cid:\\\"\\\"},display:\\\"mobile\\\",sizemap:{breakpoints:\\\"[ [1024, 0], [768, 0], [319, 0] ]\\\",refresh:true},bidding:{prebid:{enabled:true,timeout:1E3,bids:[{bidder:\\\"appnexus\\\",params:{placementId:14567590}}]}}},{lazyload:true})});</script> </div> </div> </div> </div> </div> <div data-type=\\\"text\\\" class=\\\"card collection-item \\\"> <div class=\\\"card-content card-article\\\"> <p><strong>Watch Region 8 News&nbsp;On Demand:&nbsp;<a href=\\\"http://bit.ly/Region8live\\\">On your Desktop</a>&nbsp;|&nbsp;<a href=\\\"http://ftpcontent4.worldnow.com/raycom/mobile/liveplayer/kait.html#vrvextbrowser=yes\\\">On your Mobile device</a></strong></p> </div> </div> <div data-type=\\\"text\\\" class=\\\"card collection-item card-border-bottom card-border-bottom-thick card-border-bottom-dark\\\"> <div class=\\\"card-content card-article\\\"> <p><strong>Region 8 News App - Install or update on your:&nbsp;<a href=\\\"http://itunes.apple.com/us/app/kait-region-8-news/id449621749?mt=8\\\">iPhone</a>&nbsp;|&nbsp;<a href=\\\"https://market.android.com/details?id=com.raycom.kait&amp;feature=search_result\\\">Android</a></strong></p> </div> </div> </div> </div> <div class=\\\"wrapper clearfix full pb-feature pb-layout-item pb-f-ads-connatix\\\" id=\\\"fwaAKx1Xz0ktKr\\\"> <div class=\\\"flex-grid ad-connatix spaced spaced-lg spaced-top spaced-bottom \\\"> <div class=\\\"col desktop-12 tablet-12 mobile-12\\\"> <div id=\\\"connatix-highlights-embed\\\" class=\\\"connatix-container box-shadow-light\\\"> <script id=\\\"a1c1c57f95064f50bba9396a0b142ced\\\">cnxps.cmd.push(function(){cnxps({playerId:\\\"cec234f2-82ca-435a-a30a-271018f43d08\\\"}).render(\\\"a1c1c57f95064f50bba9396a0b142ced\\\")});</script> </div> </div> </div> </div> <div class=\\\"wrapper clearfix full pb-feature pb-layout-item pb-f-ads-outbrain\\\" id=\\\"f06efnJXz0ktKr\\\"> <div id=\\\"outbrain-article-ad\\\" class=\\\"\\\"> <div class=\\\"OUTBRAIN\\\" data-widget-id=\\\"AR_8\\\" data-src=\\\"https://www.kait8.com/story/32310666/one-arrested-after-police-standoff-in-blytheville\\\" data-ob-template=\\\"raycom\\\"> </div> <div class=\\\"OUTBRAIN\\\" data-widget-id=\\\"AR_9\\\" data-src=\\\"https://www.kait8.com/story/32310666/one-arrested-after-police-standoff-in-blytheville\\\" data-ob-template=\\\"raycom\\\"> </div> </div> </div> </section> </div> <div class=\\\"pb-container\\\"> </div> </article> <aside id=\\\"right-rail\\\" class=\\\"col desktop-4 tablet-12 mobile-12 right-rail\\\"> <div class=\\\"pb-container\\\"> <div class=\\\"wrapper clearfix col pb-feature pb-layout-item pb-f-ads-arcads tablet-6 mobile-12 desktop-12\\\" id=\\\"fzMDJM1Xz0ktKr\\\"> <div id=\\\"ad1105-sticky\\\" class=\\\"flex-container-column \\\"> <div class=\\\"flex-container-row justify-center \\\"> <div id=\\\"ad1105\\\" class=\\\"arcad ad-300x250\\\"></div> <script>window.serviceCallbacks.push(function(){window.helpers.includeAd(arcAds,{id:\\\"ad1105\\\",slotName:\\\"kait/web/news\\\",adType:\\\"flex-cube\\\",dimensions:\\\"[[[300, 600], [300, 250]], [[300, 250]], [[300, 250]]]\\\",targeting:{position:{\\\"as\\\":\\\"posn\\\"},position_type:\\\"\\\",pt:\\\"a\\\",cid:\\\"QK3SPA45DZEFPGOOKT6HIFCSGQ\\\"},display:\\\"all\\\",sizemap:{breakpoints:\\\"[ [1024, 0], [768, 0], [319, 0] ]\\\",refresh:true},bidding:{prebid:{enabled:true,timeout:1E3,bids:[{bidder:\\\"appnexus\\\",params:{placementId:14567590}}]}}},{lazyload:true})});</script> </div> </div> </div> <div class=\\\"wrapper clearfix col desktop-12 tablet-6 mobile-12 pb-feature pb-layout-item pb-f-weather-forecast-box\\\" id=\\\"f47aum1Xz0ktKr\\\"> <div class=\\\"invisible weather-gradient flex-grid color-white pb-weather day\\\" id=\\\"weather-forecast-box\\\" data-valid-content=\\\"true\\\"> <div id=\\\"forecast-box-bg-wrapper\\\" data-current-bg-image=\\\"day-cloudy\\\" data-default-location-code=\\\"USAR0304\\\" class=\\\"weather-item flex pb-weather pb-forecast-box padded padded-md padded-top padded-bottom padded-right padded-left\\\"> <div id=\\\"forecast-box-content\\\"> <input type=\\\"hidden\\\" id=\\\"forecast-box-zip-code\\\" data-url=\\\"/pb/api/v2/render/feature/weather/forecast-box\\\" data-feature-grouping=\\\".pb-f-weather-forecast-box\\\" data-website=\\\"kait\\\" data-location-code=\\\"USAR0304\\\" data-language=\\\"en\\\" data-content-config-keys='Code,Language' data-content-config-values='USAR0304,en' data-custom-field-keys='spanish' data-custom-field-values=''/> <div class=\\\"col spaced spaced-top spaced-sm\\\"> <div class=\\\"flex-container-row justify-center\\\"> <div class=\\\"spaced spaced-right spaced-lg flex-container-row align-items-center\\\"> <a href=\\\"https://www.kait8.com/weather\\\"> <img class=\\\"radar-image\\\" src=\\\"//webpubcontent.raycommedia.com/kait/wximages/SatRad_DMA_640.jpg\\\" width=\\\"100%\\\"/> </a> </div> <div class=\\\"flex-container-column\\\"> <div class=\\\"flex-container-row\\\"> <div class=\\\"font-bold temp temp-lg weather-temp temp-lg-forecast-box\\\" data-temp-f=\\\"36\\\" data-temp-c=\\\"2\\\"> 36</div> <div class=\\\"icon spaced spaced-left spaced-md flex-container-row align-items-center\\\"> <img width=\\\"40px\\\" height=\\\"40px\\\" src=\\\"/pb/resources/images/weather/weather-condition-icons/400x400/67_daily_forecast.png\\\" alt=\\\"weather icon\\\"/> </div> </div> <div class=\\\"spaced spaced-sm spaced-top text-align-center\\\"> <div class=\\\"flex-container-column relative\\\"> <div class=\\\"text-align-center\\\"> <span class=\\\"location font-bold\\\"> Currently in<br/>Jonesboro, AR</span> </div> </div> </div> <div class=\\\"flex-container-row spaced spaced-top spaced-md justify-center\\\"> <a href=\\\"https://www.kait8.com/weather\\\"> <button class=\\\"weather uppercase button button-mini-radius button-transparent button-white-border color-white bold font-sm padded padded-xs padded-left padded-right padded-top padded-bottom\\\"> Full Forecast</button> </a> </div> </div> </div> </div> </div> <div class=\\\"flex-container-row justify-center spaced spaced-md spaced-bottom spaced-top font-semi-bold\\\"> <span class=\\\"spaced spaced-right spaced-sm\\\">Sponsored By</span> <div id=\\\"ad1264-sticky\\\" class=\\\"flex-container-column \\\"> <div class=\\\"flex-container-row justify-center \\\"> <div id=\\\"ad1264\\\" class=\\\"arcad ad-120x30\\\"></div> <script>window.serviceCallbacks.push(function(){window.helpers.includeAd(arcAds,{id:\\\"ad1264\\\",slotName:\\\"kait/web/weather-widget\\\",adType:\\\"weather-sponsorship\\\",dimensions:\\\"[[[120, 30]], [[120, 30]], [[120, 30]]]\\\",targeting:{position:{\\\"as\\\":\\\"posn\\\"},position_type:\\\"\\\",pt:\\\"weather\\\",cid:\\\"\\\"},display:\\\"all\\\",sizemap:{breakpoints:\\\"[ [1024, 0], [768, 0], [319, 0] ]\\\",refresh:true},bidding:{prebid:{enabled:false,timeout:1E3,bids:[{bidder:\\\"appnexus\\\",params:{placementId:14567590}}]}}},{lazyload:true})});</script> </div> </div> </div> </div> </div> </div> <div class=\\\"wrapper clearfix col desktop-12 tablet-12 mobile-12 pb-feature pb-layout-item pb-f-article-author-bio\\\" id=\\\"frRRQ42Xz0ktKr\\\"> </div> </div> <div class=\\\"wrapper clearfix full pb-feature pb-layout-item pb-f-global-mailchimp-signup\\\" id=\\\"fxwHfN1Xz0ktKr\\\"> </div> <div class=\\\"col pb-layout-item pb-chain pb-c-single-chain full\\\" id=\\\"c0NRdx1Xz0ktKr\\\"> <div class=\\\"flex-grid\\\"> <section class=\\\"col desktop-12 tablet-12 mobile-12\\\"> <div class=\\\"flex-grid\\\"> <div class=\\\"wrapper clearfix col full pb-feature pb-layout-item pb-f-homepage-story-feed\\\" id=\\\"fhr3sb2Xz0ktKr\\\"> <div class=\\\"flex-feature \\\"> <h3 class=\\\"bold\\\"><span class=\\\"rectangle\\\"></span> RECENT CONTENT</h3> <div class=\\\"flex-feature height-full flex-grid api-returned-stories\\\" id=\\\"load-more-wrapper-111303\\\" data-content-service=\\\"content-feed\\\"> <div class=\\\"col desktop-12 tablet-6 mobile-12\\\"> <div class=\\\"card skin\\\"> <div class=\\\"card-media width-full\\\"> <div class=\\\"align-items-center\\\"> <div class=\\\"width-full spaced card-media\\\"> <figure class=\\\"\\\"> <div class=\\\"width-full img-container aspect-ratio-2x1\\\"> <a href=\\\"/2021/02/08/city-councilman-arrested-sodomy-kidnapping-charges/\\\"> <img class=\\\"width-full \\\" alt=\\\"City councilman arrested on sodomy, kidnapping charges\\\" src=\\\"https://www.kait8.com/resizer/wg3Y3JODCJ2nJzchyq_R3eHUqZY=/1200x600/cloudfront-us-east-1.images.arcpublishing.com/raycom/L73F4RLYERGQ3ORWPPS7HHYTUA.png\\\"> </a> </div> </figure> </div> <div class=\\\"card card-article-mobile-only width-full flex-container-column\\\"> <h4 class=\\\"promo-header spaced spaced-xs spaced-bottom spaced-top font-normal\\\"><a class=\\\"unstyled-link \\\" href=\\\"/2021/02/08/city-councilman-arrested-sodomy-kidnapping-charges/\\\"> City councilman arrested on sodomy, kidnapping charges</a></h4> <div class=\\\"spaced color-grey-dark font-md\\\" data-pb-field=\\\"custom.blurbText_\\\" data-pb-url-field=\\\"custom.blurbUrl_\\\" data-pb-field-type=\\\"text\\\" data-pb-placeholder=\\\"#\\\">The councilman is in the Dunklin County Jail with no bond and is set to be arraigned on Tuesday.</div> <div class=\\\"byline-timestamp-container flex-container-column flex-desktop-row spaced spaced-top spaced-xs align-items-start\\\"> <div class=\\\"byline-wrapper flex-container-row \\\"> <span class=\\\"byline\\\">By&nbsp;</span> <h3 class=\\\"byline bold\\\"> <span><span>Region 8 Newsdesk</span></span> </h3> <span class=\\\"spaced spaced-right spaced-md\\\"></span> </div> </div> </div> </div> </div> </div> </div> <div class=\\\"col desktop-12 tablet-6 mobile-12\\\"> <div class=\\\"card skin\\\"> <div class=\\\"card-media width-full\\\"> <div class=\\\"align-items-center\\\"> <div class=\\\"width-full spaced card-media\\\"> <figure class=\\\"\\\"> <div class=\\\"width-full img-container aspect-ratio-2x1\\\"> <a href=\\\"/2021/02/08/osceola-police-investigate-friday-shooting/\\\"> <img class=\\\"width-full \\\" alt=\\\"Osceola police investigate Friday shooting\\\" src=\\\"https://www.kait8.com/resizer/MwxZfroPz_VTamfctCMR99zL37Q=/1200x600/cloudfront-us-east-1.images.arcpublishing.com/raycom/HPPQUSEVK5GANPXSANIHAFZXAA.jpg\\\"> </a> </div> </figure> </div> <div class=\\\"card card-article-mobile-only width-full flex-container-column\\\"> <h4 class=\\\"promo-header spaced spaced-xs spaced-bottom spaced-top font-normal\\\"><a class=\\\"unstyled-link \\\" href=\\\"/2021/02/08/osceola-police-investigate-friday-shooting/\\\"> Osceola police investigate Friday shooting</a></h4> <div class=\\\"spaced color-grey-dark font-md\\\" data-pb-field=\\\"custom.blurbText_\\\" data-pb-url-field=\\\"custom.blurbUrl_\\\" data-pb-field-type=\\\"text\\\" data-pb-placeholder=\\\"#\\\">The Osceola Police Department is investigating a Friday evening shooting that took place at the local Express Mart.</div> <div class=\\\"byline-timestamp-container flex-container-column flex-desktop-row spaced spaced-top spaced-xs align-items-start\\\"> <div class=\\\"byline-wrapper flex-container-row \\\"> <span class=\\\"byline\\\">By&nbsp;</span> <h3 class=\\\"byline bold\\\"> <span><span>Region 8 Newsdesk</span></span> </h3> <span class=\\\"spaced spaced-right spaced-md\\\"></span> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </section> </div> </div> <div class=\\\"wrapper clearfix full pb-feature pb-layout-item pb-f-ads-arcads\\\" id=\\\"f09JMK9Xz0ktKr\\\"> <div id=\\\"ad100-sticky\\\" class=\\\"flex-container-column \\\"> <div class=\\\"flex-container-row justify-center \\\"> <div id=\\\"ad100\\\" class=\\\"arcad ad-300x250\\\"></div> <script>window.serviceCallbacks.push(function(){window.helpers.includeAd(arcAds,{id:\\\"ad100\\\",slotName:\\\"kait/web/news\\\",adType:\\\"cube\\\",dimensions:\\\"[[[300, 250]], [[300, 250]], [[300, 250]]]\\\",targeting:{position:{\\\"as\\\":\\\"posn\\\"},position_type:\\\"\\\",pt:\\\"a\\\",cid:\\\"QK3SPA45DZEFPGOOKT6HIFCSGQ\\\"},display:\\\"all\\\",sizemap:{breakpoints:\\\"[ [1024, 0], [768, 0], [319, 0] ]\\\",refresh:true},bidding:{prebid:{enabled:true,timeout:1E3,bids:[{bidder:\\\"appnexus\\\",params:{placementId:14567590}}]}}},{lazyload:true})});</script> </div> </div> </div> <div class=\\\"col pb-layout-item pb-chain pb-c-single-chain full\\\" id=\\\"c008fu9Xz0ktKr\\\"> <div class=\\\"flex-grid\\\"> <section class=\\\"col desktop-12 tablet-12 mobile-12\\\"> <div class=\\\"flex-grid\\\"> <div class=\\\"wrapper clearfix col full pb-feature pb-layout-item pb-f-homepage-story-feed\\\" id=\\\"fmrOMh1Xz0ktKr\\\"> <div class=\\\"flex-feature \\\"> <div class=\\\"flex-feature height-full flex-grid api-returned-stories\\\" id=\\\"load-more-wrapper-10996\\\" data-content-service=\\\"content-feed\\\"> <div class=\\\"col desktop-12 tablet-6 mobile-12\\\"> <div class=\\\"card skin\\\"> <div class=\\\"card-media width-full\\\"> <div class=\\\"align-items-center flex-container-row justify-space-between\\\"> <div class=\\\"width-full spaced card-media flex-reverse spaced-left spaced-lg flex\\\"> <figure class=\\\"\\\"> <div class=\\\"width-full img-container aspect-ratio-4x3\\\"> <a href=\\\"/2021/02/08/report-terry-mohajir-target-ucf-athletic-director-search/\\\"> <img class=\\\"width-full \\\" alt=\\\"Report: Terry Mohajir a target in UCF athletic director search\\\" src=\\\"https://www.kait8.com/resizer/OhQ0t9syUaQKZTKiI4W49pt8yMw=/1200x900/cloudfront-us-east-1.images.arcpublishing.com/raycom/YK3QUVJ5LZDY3EXMWCRPUP6TVM.jpg\\\"> </a> </div> </figure> </div> <div class=\\\"card card-article-mobile-only width-full flex-container-column flex-2\\\"> <h4 class=\\\"promo-header spaced spaced-xs spaced-bottom spaced-top font-normal\\\"><a class=\\\"unstyled-link \\\" href=\\\"/2021/02/08/report-terry-mohajir-target-ucf-athletic-director-search/\\\"> Report: Terry Mohajir a target in UCF athletic director search</a></h4> <div class=\\\"byline-timestamp-container flex-container-column flex-desktop-row spaced spaced-top spaced-xs align-items-start\\\"> <div class=\\\"byline-wrapper flex-container-row hidden\\\"> <span class=\\\"byline\\\">By&nbsp;</span> <h3 class=\\\"byline bold\\\"> <span><span>Chris Hudgison</span></span> </h3> <span class=\\\"spaced spaced-right spaced-md\\\"></span> </div> <div class=\\\"font-sm color-grey byline-wrapper flex-container-row \\\"> Published 1h at 10:45 AM </div> </div> </div> </div> </div> </div> </div> <div class=\\\"col desktop-12 tablet-6 mobile-12\\\"> <div class=\\\"card skin\\\"> <div class=\\\"card-media width-full\\\"> <div class=\\\"align-items-center flex-container-row justify-space-between\\\"> <div class=\\\"width-full spaced card-media flex-reverse spaced-left spaced-lg flex\\\"> <figure class=\\\"\\\"> <div class=\\\"width-full img-container aspect-ratio-4x3\\\"> <a href=\\\"/2021/02/08/harding-university-honor-murdered-alumnus/\\\"> <img class=\\\"width-full \\\" alt=\\\"Harding University to honor murdered alumnus\\\" src=\\\"https://www.kait8.com/resizer/aqxTbm7wchoZHFCmoB5F3uv-SvU=/1200x900/cloudfront-us-east-1.images.arcpublishing.com/raycom/4KYQ6HNEIZEKNG2QUHSE4SRBRI.jpg\\\"> </a> </div> </figure> </div> <div class=\\\"card card-article-mobile-only width-full flex-container-column flex-2\\\"> <h4 class=\\\"promo-header spaced spaced-xs spaced-bottom spaced-top font-normal\\\"><a class=\\\"unstyled-link \\\" href=\\\"/2021/02/08/harding-university-honor-murdered-alumnus/\\\"> Harding University to honor murdered alumnus</a></h4> <div class=\\\"byline-timestamp-container flex-container-column flex-desktop-row spaced spaced-top spaced-xs align-items-start\\\"> <div class=\\\"byline-wrapper flex-container-row hidden\\\"> <span class=\\\"byline\\\">By&nbsp;</span> <h3 class=\\\"byline bold\\\"> <span><span>Region 8 Newsdesk</span></span> </h3> <span class=\\\"spaced spaced-right spaced-md\\\"></span> </div> <div class=\\\"font-sm color-grey byline-wrapper flex-container-row \\\"> Published 1h at 10:34 AM </div> </div> </div> </div> </div> </div> </div> <div class=\\\"col desktop-12 tablet-6 mobile-12\\\"> <div class=\\\"card skin\\\"> <div class=\\\"card-media width-full\\\"> <div class=\\\"align-items-center flex-container-row justify-space-between\\\"> <div class=\\\"width-full spaced card-media flex-reverse spaced-left spaced-lg flex\\\"> <figure class=\\\"\\\"> <div class=\\\"width-full img-container aspect-ratio-4x3\\\"> <a href=\\\"/2021/02/08/lagging-production-driving-up-gas-prices-arkansas/\\\"> <img class=\\\"width-full \\\" alt=\\\"Lagging production driving up gas prices in Arkansas\\\" src=\\\"https://www.kait8.com/resizer/7OLbSYpfhs-xAK7abU8dfrZrfSU=/1200x900/cloudfront-us-east-1.images.arcpublishing.com/raycom/RUYNYT6THBF4FDLIZJCYPUXPBE.png\\\"> </a> </div> </figure> </div> <div class=\\\"card card-article-mobile-only width-full flex-container-column flex-2\\\"> <h4 class=\\\"promo-header spaced spaced-xs spaced-bottom spaced-top font-normal\\\"><a class=\\\"unstyled-link \\\" href=\\\"/2021/02/08/lagging-production-driving-up-gas-prices-arkansas/\\\"> Lagging production driving up gas prices in Arkansas</a></h4> <div class=\\\"byline-timestamp-container flex-container-column flex-desktop-row spaced spaced-top spaced-xs align-items-start\\\"> <div class=\\\"byline-wrapper flex-container-row hidden\\\"> <span class=\\\"byline\\\">By&nbsp;</span> <h3 class=\\\"byline bold\\\"> <span><span>Region 8 Newsdesk</span></span> </h3> <span class=\\\"spaced spaced-right spaced-md\\\"></span> </div> <div class=\\\"font-sm color-grey byline-wrapper flex-container-row \\\"> Published 3h at 8:26 AM </div> </div> </div> </div> </div> </div> </div> <div class=\\\"col desktop-12 tablet-6 mobile-12\\\"> <div class=\\\"card skin\\\"> <div class=\\\"card-media width-full\\\"> <div class=\\\"align-items-center flex-container-row justify-space-between\\\"> <div class=\\\"width-full spaced card-media flex-reverse spaced-left spaced-lg flex\\\"> <figure class=\\\"\\\"> <div class=\\\"width-full img-container aspect-ratio-4x3\\\"> <a href=\\\"/2021/02/08/police-say-missing-year-old-has-been-found-safe/\\\"> <img class=\\\"width-full \\\" alt=\\\"Police say missing 10-year-old has been found safe\\\" src=\\\"https://www.kait8.com/resizer/e177Sz6cFkOH2rm-RA-DCnARN-Y=/1200x900/cloudfront-us-east-1.images.arcpublishing.com/raycom/MTKXIGEMNVF5BHM2HT3P5Y7J5I.jpg\\\"> </a> </div> </figure> </div> <div class=\\\"card card-article-mobile-only width-full flex-container-column flex-2\\\"> <h4 class=\\\"promo-header spaced spaced-xs spaced-bottom spaced-top font-normal\\\"><a class=\\\"unstyled-link \\\" href=\\\"/2021/02/08/police-say-missing-year-old-has-been-found-safe/\\\"> Police say missing 10-year-old has been found safe</a></h4> <div class=\\\"byline-timestamp-container flex-container-column flex-desktop-row spaced spaced-top spaced-xs align-items-start\\\"> <div class=\\\"byline-wrapper flex-container-row hidden\\\"> <span class=\\\"byline\\\">By&nbsp;</span> <h3 class=\\\"byline bold\\\"> <span>WAFB Staff</span> </h3> <span class=\\\"spaced spaced-right spaced-md\\\"></span> </div> <div class=\\\"font-sm color-grey byline-wrapper flex-container-row \\\"> Published 3h at 8:24 AM </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </section> </div> </div> <div class=\\\"pb-container\\\"> </div> </aside> <div id=\\\"recirc\\\" class=\\\"col desktop-12 tablet-12 mobile-12\\\"> </div> </div> <section id=\\\"bottom\\\" class=\\\"col desktop-12 tablet-12 mobile-12\\\"> <div class=\\\"wrapper clearfix full pb-feature pb-layout-item pb-f-ads-arcads\\\" id=\\\"f0XShVkXz0ktKr\\\"> <div id=\\\"ad554-sticky\\\" class=\\\"flex-container-column \\\"> <div class=\\\"flex-container-row justify-center \\\"> <div id=\\\"ad554\\\" class=\\\"arcad ad-728x90 zindex-med\\\"></div> <script>window.serviceCallbacks.push(function(){window.helpers.includeAd(arcAds,{id:\\\"ad554\\\",slotName:\\\"kait/web/news\\\",adType:\\\"flex-leaderboard\\\",dimensions:\\\"[[[970, 250], [970, 90], [728, 90]], [[728, 90]], [[320, 50]]]\\\",targeting:{position:{\\\"as\\\":\\\"posn\\\"},position_type:\\\"\\\",pt:\\\"a\\\",cid:\\\"QK3SPA45DZEFPGOOKT6HIFCSGQ\\\"},display:\\\"all\\\",sizemap:{breakpoints:\\\"[ [1024, 0], [768, 0], [319, 0] ]\\\",refresh:true},bidding:{prebid:{enabled:true,timeout:1E3,bids:[{bidder:\\\"appnexus\\\",params:{placementId:14567590}}]}}},{lazyload:true})});</script> </div> </div> </div> <div class=\\\"wrapper clearfix full pb-feature pb-layout-item pb-f-global-footer\\\" id=\\\"fU2oFF1Xz0ktKr\\\"> <footer class=\\\"card card-dark padded padded-bottom padded-0\\\"> <div class=\\\"container\\\"> <div class=\\\"container spaced spaced-sm spaced-left\\\"> <div class=\\\"flex flex-container-column flex-tablet-row flex-desktop-row width-full color-white\\\"> <div class=\\\"spaced spaced-right spaced-bottom spaced-xl\\\"> <div class=\\\"logo-container flex flex-container-row align-items-center justify-left\\\"> <a aria-label=\\\"Visit homepage\\\" href=\\\"https://www.kait8.com\\\"> <div class=\\\"logo logo-large logo-footer\\\"></div> </a> </div> </div> <div class=\\\"spaced spaced-right spaced-xl footer-mission\\\"> <p class=\\\"spaced spaced-bottom spaced-xs\\\"> 472 CR 766<br/>Jonesboro, AR 72401<br/>(870) 931-8888 </p> <a class=\\\"call-to-action uppercase color-white\\\" href=\\\"http://www.kait8.com/about-us\\\">Contact Us <i class=\\\"fa fa-caret-right\\\" aria-hidden=\\\"true\\\"></i></a> <div id=\\\"ccpa-link\\\" class=\\\"call-to-action ccpa-dynamic-insertion-container uppercase color-white\\\"></div> <div class=\\\"spaced spaced-top spaced-xl hidden-lg\\\"> <ul class=\\\"list-unstyled call-to-action uppercase\\\"> <li><a href=\\\"http://publicfiles.fcc.gov/tv-profile/kait\\\" class=\\\"color-white\\\">FCC Public File</a></li> <li><a href=\\\"mailto:publicfile@kait8.com\\\" class=\\\"color-white\\\">publicfile@kait8.com <br/> (870) 336-1817</a></li> <li><a href=\\\"http://webpubcontent.raycommedia.com/kait/PDF/latestKAITEEO.pdf\\\" class=\\\"color-white\\\">EEO</a></li> <li><a href=\\\"https://webpubcontent.gray.tv/gdm/fcc/kait-fcc_applications.pdf\\\" class=\\\"color-white\\\">FCC Applications</a></li> <li><a href=\\\"http://www.kait8.com/2018/08/28/kait-closed-captioning-service-support/\\\" class=\\\"color-white\\\">Closed Captioning</a></li> <li><a href=\\\"https://gray.tv/careers#currentopenings\\\" class=\\\"color-white\\\">KAIT 8 Careers</a></li> <li><a href=\\\"/privacy-policy/\\\" class=\\\"color-white\\\">Privacy Policy</a></li> <li><a href=\\\"/terms-of-service/\\\" class=\\\"color-white\\\">Terms of Service</a></li> </ul> </div> <div class=\\\"spaced spaced-top spaced-xl hidden-lg\\\"> <span class=\\\"buttonbar app-bar-share flex-container-grid width-full text-align-right\\\"> <a class=\\\"button button-icon button-icon-hover-accent button-lg footer-social-icon\\\" href=\\\"https://twitter.com/Region8News\\\" role=\\\"button\\\" aria-label=\\\"Visit Twitter\\\" target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\"> <i class=\\\"fab fa-twitter \\\"></i> </a> <a class=\\\"button button-icon button-icon-hover-accent button-lg footer-social-icon\\\" href=\\\"https://www.facebook.com/Region8News\\\" role=\\\"button\\\" aria-label=\\\"Visit Facebook\\\" target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\"> <i class=\\\"fab fa-facebook-square \\\"></i> </a> </span> </div> </div> <div class=\\\"spaced spaced-right spaced-xl visible-lg\\\"> <ul class=\\\"list-unstyled call-to-action uppercase\\\"> <li><a href=\\\"http://publicfiles.fcc.gov/tv-profile/kait\\\" class=\\\"color-white\\\">FCC Public File</a></li> <li><a href=\\\"mailto:publicfile@kait8.com\\\" class=\\\"color-white\\\">publicfile@kait8.com <br/> (870) 336-1817</a></li> <li><a href=\\\"http://webpubcontent.raycommedia.com/kait/PDF/latestKAITEEO.pdf\\\" class=\\\"color-white\\\">EEO</a></li> <li><a href=\\\"https://webpubcontent.gray.tv/gdm/fcc/kait-fcc_applications.pdf\\\" class=\\\"color-white\\\">FCC Applications</a></li> <li><a href=\\\"http://www.kait8.com/2018/08/28/kait-closed-captioning-service-support/\\\" class=\\\"color-white\\\">Closed Captioning</a></li> <li><a href=\\\"https://gray.tv/careers#currentopenings\\\" class=\\\"color-white\\\">KAIT 8 Careers</a></li> <li><a href=\\\"/privacy-policy/\\\" class=\\\"color-white\\\">Privacy Policy</a></li> <li><a href=\\\"/terms-of-service/\\\" class=\\\"color-white\\\">Terms of Service</a></li> </ul> </div> <div class=\\\"visible-lg\\\"> <span class=\\\"buttonbar app-bar-share flex-container-grid width-full text-align-right\\\"> <a class=\\\"button button-icon button-icon-hover-accent button-lg footer-social-icon\\\" href=\\\"https://twitter.com/Region8News\\\" role=\\\"button\\\" aria-label=\\\"Visit Twitter\\\" target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\"> <i class=\\\"fab fa-twitter \\\"></i> </a> <a class=\\\"button button-icon button-icon-hover-accent button-lg footer-social-icon\\\" href=\\\"https://www.facebook.com/Region8News\\\" role=\\\"button\\\" aria-label=\\\"Visit Facebook\\\" target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\"> <i class=\\\"fab fa-facebook-square \\\"></i> </a> </span> </div> </div> </div> </div> <div class=\\\"footer-copyright width-full bordered bordered-top-darken spaced spaced-top spaced-sm\\\"> <div class=\\\"container\\\"> <p class=\\\"font-sm color-white text-align-center spaced spaced-sm spaced-top spaced-bottom\\\"> <span>A Gray Media Group, Inc. Station - &copy; 2002-2021 Gray Television, Inc.</span> </p> <div class=\\\"footer-corp-logo width-full spaced spaced-sm spaced-top spaced-bottom\\\"></div> </div> </div> </footer> </div> <div class=\\\"pb-container\\\"> </div> </section> </div> <script async=\\\"async\\\" src=\\\"https://widgets.outbrain.com/outbrain.js\\\"></script> <script async src='/pb/gr/p/default/rHJ9vP1Xz0ktKr/render.js?v=101'></script> <script>(function(){window.pageBuilder=window.pageBuilder||{};var testIdToFeatureId={};var status={};var TIMEOUT=1500;var TIME_NOW=Date.now();var BANDITO_IS_ADMIN=null;var BANDITO_PERF_API=null;var BANDITO_TESTS_API=null;var BANDITO_EVENTS_API=null;var supportsPassive=false;try{var opts=Object.defineProperty({},\\\"passive\\\",{get:function(){supportsPassive=true}});window.addEventListener(\\\"test\\\",null,opts)}catch(e){}var visibilityCallbacks={};function visibilityCallback(){var config=null;var element=null;var callback=\\nnull;var body=document.body;var html=document.documentElement;var height=html.clientHeight||body.clientHeight;for(var key in visibilityCallbacks)if(visibilityCallbacks.hasOwnProperty(key)){config=visibilityCallbacks[key];if(config){element=config[0];callback=config[1];var boundaries=element.getBoundingClientRect();if(boundaries.top>0&&boundaries.top<height){callback();visibilityCallbacks[key]=null}}}}window.addEventListener(\\\"resize\\\",visibilityCallback,supportsPassive?{passive:true}:false);document.addEventListener(\\\"scroll\\\",\\nvisibilityCallback,supportsPassive?{passive:true}:false);function ajax(url,success,failure,timeout){var xhr=new XMLHttpRequest;xhr.open(\\\"GET\\\",url);xhr.onreadystatechange=function(){if(xhr.readyState==4)if(xhr.status===200)try{var response=JSON.parse(xhr.responseText);success(response)}catch(e){failure()}else failure()};if(timeout){xhr.ontimeout=failure;xhr.timeout=timeout}xhr.send()}function perf(status){var elapsed=Date.now()-TIME_NOW;var url=BANDITO_PERF_API+\\\"?status\\\\x3d\\\"+status+\\\"\\\\x26time\\\\x3d\\\"+\\nelapsed;ajax(url,function(){},function(){})}function showDefaultVariants(){for(var id in window._variantFeatureIds)if(window._variantFeatureIds.hasOwnProperty(id)){perf(\\\"failure\\\");showDefaultVariant(window._variantFeatureIds[id],false)}}function iterateFeatures(testId,callback){var id,feature;for(id in window._variantFeatureIds)if(window._variantFeatureIds.hasOwnProperty(id))if(window._variantFeatureIds[id]==testId){feature=document.getElementById(id);if(feature)callback(feature)}}function showDefaultVariant(testId,\\nsuccess){iterateFeatures(testId,function(feature){variantLoaded(testId,testId,feature,success)})}function showOtherVariant(testId,winner,timeLeft){var uri=window._context+\\\"/api/v2/render/feature/variant/\\\"+winner+\\\"?rid\\\\x3d\\\"+window._rid+\\\"\\\\x26uri\\\\x3d\\\"+window._uri+\\\"\\\\x26outputType\\\\x3d\\\"+window._outputType;ajax(uri,function(response){iterateFeatures(testId,function(feature){var resources=response.pageResources||{};var parent=feature.parentNode;var div=document.createElement(\\\"div\\\");div.innerHTML=response.rendering.trim();\\nvar newFeature=div.childNodes[0];parent.replaceChild(newFeature,feature);var scripts=Array.prototype.slice.call(newFeature.getElementsByTagName(\\\"script\\\"));for(var i=0;i<scripts.length;i++)if(scripts[i][\\\"type\\\"]==\\\"\\\"||scripts[i][\\\"type\\\"]==\\\"text/javascript\\\")if(scripts[i].src!=\\\"\\\"){var s=document.createElement(\\\"script\\\");s.setAttribute(\\\"src\\\",scripts[i].src);document.body.appendChild(s)}else eval(scripts[i].innerHTML);for(var file in resources)if(resources.hasOwnProperty(file)&&file.endsWith(\\\".js\\\")){var script=\\ndocument.createElement(\\\"script\\\");script.setAttribute(\\\"src\\\",resources[file]);document.body.appendChild(script)}perf(\\\"success-variant\\\");variantLoaded(testId,winner,newFeature,true)})},function(){iterateFeatures(testId,function(feature){perf(\\\"failure-step2\\\");variantLoaded(testId,testId,feature,false)})},timeLeft)}function variantLoaded(testId,variantId,feature,success){var tag=testId+\\\"-\\\"+feature.id;if(!status[tag]){status[tag]=true;feature.style.visibility=\\\"visible\\\";if(success){feature.addEventListener(\\\"click\\\",\\nfunction(event){var url=null;var target=event.target;while(target&&!(target.tagName.toUpperCase()==\\\"A\\\")&&!(target==feature))target=target.parentNode;if(target.tagName.toUpperCase()==\\\"A\\\"){url=target.href;if(url){var request=new XMLHttpRequest;request.open(\\\"POST\\\",BANDITO_EVENTS_API,true);request.setRequestHeader(\\\"Content-Type\\\",\\\"application/json\\\");request.send(JSON.stringify({event:\\\"clicked\\\",test_id:testId,variant_id:variantId}));if(event.button>0||event.ctrlKey||event.metaKey||event.shiftKey);else{event.preventDefault();\\nsetTimeout(function(){document.location=url},250)}}}});visibilityCallbacks[tag]=[feature,function(){var request=new XMLHttpRequest;request.open(\\\"POST\\\",BANDITO_EVENTS_API,true);request.setRequestHeader(\\\"Content-Type\\\",\\\"application/json\\\");request.send(JSON.stringify({event:\\\"served\\\",test_id:testId,variant_id:variantId}))}];visibilityCallback()}}}function loadVariants(tests){var testsFound={},testId,winner,timeLeft;if(tests&&tests[\\\"length\\\"]){for(var i=0;i<tests.length;i++){winner=tests[i][\\\"winner\\\"];testId=\\ntests[i][\\\"_id\\\"];testsFound[testId]=true;if(testId==winner){perf(\\\"success-default\\\");showDefaultVariant(testId,true)}else{timeLeft=TIMEOUT-(Date.now()-TIME_NOW);if(timeLeft>50)showOtherVariant(testId,winner,timeLeft);else{perf(\\\"timeout-step1\\\");showDefaultVariant(testId,false)}}}for(var id in window._variantFeatureIds)if(window._variantFeatureIds.hasOwnProperty(id)){testId=window._variantFeatureIds[id];if(!testsFound[testId]){perf(\\\"not-found\\\");showDefaultVariant(testId,false)}}}else showDefaultVariants()}\\nwindow.pageBuilder.initVariants=function(opts){opts=opts||{};TIMEOUT=opts[\\\"TIMEOUT\\\"];BANDITO_IS_ADMIN=opts[\\\"IS_ADMIN\\\"];BANDITO_PERF_API=opts[\\\"PERF_API\\\"];BANDITO_TESTS_API=opts[\\\"TESTS_API\\\"];BANDITO_EVENTS_API=opts[\\\"EVENTS_API\\\"]};window.pageBuilder.showVariants=function(){if(BANDITO_IS_ADMIN)return;var tests=[];for(var id in window._variantFeatureIds)if(window._variantFeatureIds.hasOwnProperty(id)){var testId=window._variantFeatureIds[id];testIdToFeatureId[testId]=id;tests.push(testId)}if(tests.length>\\n0){var url=BANDITO_TESTS_API+\\\"?ids\\\\x3d\\\"+tests.join(\\\",\\\");ajax(url,loadVariants,showDefaultVariants)}}})();</script> <script>window.banditoEnv={TIMEOUT:1500,PERF_API:\\\"https://bandito-events.perso.aws.arc.pub/api/save-perf\\\",TESTS_API:\\\"https://bandito.perso.aws.arc.pub/api/variants\\\",EVENTS_API:\\\"https://bandito-events.perso.aws.arc.pub/api/save-event\\\"};if(window[\\\"pageBuilder\\\"]&&window[\\\"pageBuilder\\\"][\\\"initVariants\\\"])window.pageBuilder.initVariants({IS_ADMIN:false,TIMEOUT:window.banditoEnv.TIMEOUT,PERF_API:window.banditoEnv.PERF_API,TESTS_API:window.banditoEnv.TESTS_API,EVENTS_API:window.banditoEnv.EVENTS_API});</script> <script async src=\\\"/pb/resources/dist/199071bbaf3b531cbc00/rm/rm-index199071bbaf3b531cbc00.js\\\"></script>\"\n\n\n\nWe’ll try to extract the words from the page."
  },
  {
    "objectID": "materials/slides/week5-text.html#scraping-tools",
    "href": "materials/slides/week5-text.html#scraping-tools",
    "title": "Text to data: basic NLP",
    "section": "Scraping tools",
    "text": "Scraping tools\nFirst step: strip HTML and extract text (see rvest [docs]).\n\nlibrary(rvest)\n\n# parse\npage_text <- read_html(page) %>%\n  # extract paragraph elements\n  html_elements('p') %>%\n  # strip html and extract text\n  html_text2()\n\n# print result\npage_text\n\n [1] \"BLYTHEVILLE, AR (KAIT) - One man is in custody after a 4-hour police standoff in Blytheville Sunday afternoon.\"                                                                 \n [2] \"According to Captain Scott Adams with the Blytheville Police Department, Hartzell Watson, 44, was arrested after being forced out of a home in the 500-block of North Division.\"\n [3] \"Captain Adams said police originally responded to the house after a family dispute.\"                                                                                            \n [4] \"When officers arrived, Watson barricaded himself in the house and refused to exit peacefully.\"                                                                                  \n [5] \"After several hours, police fired smoke and a chemical agent through a window of the home. The gas caught on fire within minutes.\"                                              \n [6] \"Officers entered the burning house to find Watson, but he jumped out of an attic window.\"                                                                                       \n [7] \"He was checked by paramedics on the scene and was taken into custody.\"                                                                                                          \n [8] \"The Blytheville Fire Department was on scene and quickly put the fire out.\"                                                                                                     \n [9] \"Watson was then taken to the Mississippi County Jail. He is being held on state and federal warrants.\"                                                                          \n[10] \"The Arkansas State Police, Mississippi County Sheriff's Office, and the Second Judicial Drug Task Force assisted with the standoff.\"                                            \n[11] \"One officer on the scene was treated for dehydration.\"                                                                                                                          \n[12] \"Copyright 2016 KAIT. All rights reserved.\"                                                                                                                                      \n[13] \"Watch Region 8 News On Demand: On your Desktop | On your Mobile device\"                                                                                                         \n[14] \"Region 8 News App - Install or update on your: iPhone | Android\"                                                                                                                \n[15] \"472 CR 766\\nJonesboro, AR 72401\\n(870) 931-8888\"                                                                                                                                \n[16] \"A Gray Media Group, Inc. Station - © 2002-2021 Gray Television, Inc.\""
  },
  {
    "objectID": "materials/slides/week5-text.html#one-long-string",
    "href": "materials/slides/week5-text.html#one-long-string",
    "title": "Text to data: basic NLP",
    "section": "One long string",
    "text": "One long string\nWe can collapse the list into one long character string containing all the paragraph text.\n\nstring <- page_text %>% str_c(collapse = ' ')\n\nstring\n\n[1] \"BLYTHEVILLE, AR (KAIT) - One man is in custody after a 4-hour police standoff in Blytheville Sunday afternoon. According to Captain Scott Adams with the Blytheville Police Department, Hartzell Watson, 44, was arrested after being forced out of a home in the 500-block of North Division. Captain Adams said police originally responded to the house after a family dispute. When officers arrived, Watson barricaded himself in the house and refused to exit peacefully. After several hours, police fired smoke and a chemical agent through a window of the home. The gas caught on fire within minutes. Officers entered the burning house to find Watson, but he jumped out of an attic window. He was checked by paramedics on the scene and was taken into custody. The Blytheville Fire Department was on scene and quickly put the fire out. Watson was then taken to the Mississippi County Jail. He is being held on state and federal warrants. The Arkansas State Police, Mississippi County Sheriff's Office, and the Second Judicial Drug Task Force assisted with the standoff. One officer on the scene was treated for dehydration. Copyright 2016 KAIT. All rights reserved. Watch Region 8 News On Demand: On your Desktop | On your Mobile device Region 8 News App - Install or update on your: iPhone | Android 472 CR 766\\nJonesboro, AR 72401\\n(870) 931-8888 A Gray Media Group, Inc. Station - © 2002-2021 Gray Television, Inc.\""
  },
  {
    "objectID": "materials/slides/week5-text.html#words-neq-strings",
    "href": "materials/slides/week5-text.html#words-neq-strings",
    "title": "Text to data: basic NLP",
    "section": "Words \\(\\neq\\) strings",
    "text": "Words \\(\\neq\\) strings\nnathan , Nathan, and Nathan! are identical words but distinct strings.\n\n'nathan' == 'Nathan'\n\n[1] FALSE\n\n\n\nBut text analysis requires that strings ⟺ words.\n\n\nQuestion. What are the steps to get from [1] to [2] ?\n\n\n[1] \"For more information, call @Alfred | (201) 744 5050\"\n[2] \"for more information call alfred\""
  },
  {
    "objectID": "materials/slides/week5-text.html#string-manipulation",
    "href": "materials/slides/week5-text.html#string-manipulation",
    "title": "Text to data: basic NLP",
    "section": "String manipulation",
    "text": "String manipulation\nSee stringr [docs] for string manipulation via pattern matching.\n\nlibrary(stringr)\n\nc('example-string') %>% str_replace('[[:punct:]]', ' ')\n\n[1] \"example string\"\n\n\n\nSee qdapRegex [docs] for shorthand wrappers tor removing common but complex patterns.\n\nlibrary(qdapRegex)\n\nc('email Mildred mildred@mildred.info') %>% rm_email() \n\n[1] \"email Mildred\""
  },
  {
    "objectID": "materials/slides/week5-text.html#page-text-processing",
    "href": "materials/slides/week5-text.html#page-text-processing",
    "title": "Text to data: basic NLP",
    "section": "Page text processing",
    "text": "Page text processing\nOur strategy will be:\n\nRemove URLs and email addresses\nRemove non-letters:\n\nline breaks \\n and &nbsp\npunctuation, numbers, and special characters\n\nAdd spaces before capital letters then remove extra whitespace\nReplace all capital letters with lower case letters"
  },
  {
    "objectID": "materials/slides/week5-text.html#example",
    "href": "materials/slides/week5-text.html#example",
    "title": "Text to data: basic NLP",
    "section": "Example",
    "text": "Example\nHere’s what that looks like for one page.\n\nremove <- c('\\n', \n            '[[:punct:]]', \n            'nbsp', \n            '[[:digit:]]', \n            '[[:symbol:]]') %>%\n  paste(collapse = '|')\n\nstring %>%\n  rm_url() %>%\n  rm_email() %>%\n  str_remove_all('\\'') %>%\n  str_replace_all(remove, ' ') %>%\n  str_replace_all(\"([a-z])([A-Z])\", \"\\\\1 \\\\2\") %>%\n  tolower() %>%\n  str_replace_all(\"\\\\s+\", \" \")\n\n[1] \"blytheville ar kait one man is in custody after a hour police standoff in blytheville sunday afternoon according to captain scott adams with the blytheville police department hartzell watson was arrested after being forced out of a home in the block of north division captain adams said police originally responded to the house after a family dispute when officers arrived watson barricaded himself in the house and refused to exit peacefully after several hours police fired smoke and a chemical agent through a window of the home the gas caught on fire within minutes officers entered the burning house to find watson but he jumped out of an attic window he was checked by paramedics on the scene and was taken into custody the blytheville fire department was on scene and quickly put the fire out watson was then taken to the mississippi county jail he is being held on state and federal warrants the arkansas state police mississippi county sheriffs office and the second judicial drug task force assisted with the standoff one officer on the scene was treated for dehydration copyright kait all rights reserved watch region news on demand on your desktop on your mobile device region news app install or update on your i phone android cr jonesboro ar a gray media group inc station gray television inc \""
  },
  {
    "objectID": "materials/slides/week5-text.html#output-quality",
    "href": "materials/slides/week5-text.html#output-quality",
    "title": "Text to data: basic NLP",
    "section": "Output quality",
    "text": "Output quality\nComments:\n\nconsistent input format (i.e. sampling and collection) is really important for consistent scraping and text processing\n\ntricky with HTML because webpages may use different elements to display content\n\nthorough quality tests are recommended: inspect random subsamples for errors in processing"
  },
  {
    "objectID": "materials/slides/week5-text.html#quality-checks",
    "href": "materials/slides/week5-text.html#quality-checks",
    "title": "Text to data: basic NLP",
    "section": "Quality checks",
    "text": "Quality checks\n\n\n[[1]]\n[1] \"a melrose woman has been sentenced to seven years in prison for exploiting an elderly neighbor while duping her own live in girlfriend a melrose woman has been sentenced to seven years in prison for exploiting an elderly neighbor while duping her own live in girlfriend rhonda jo lay pleaded no contest late friday to exploitation of the elderly in connection with thefts from a year old woman circuit court judge james p nilon sentenced lay to seven years in prison to be followed by eight years of probation nilon also ordered lay to pay the victim in restitution the case against lay began as a case against her roommate lisa a feindt had been charged in the case but assistant state attorney deborah hunt said all charges against feindt were dropped dec when it became clear that lay had been lying to feindt when the women were arrested feindt was a physician s assistant at the university of florida feindt s attorney gilbert schaffnit said feindt had to resign from her job while waiting for the charges against her to be resolved my client was another victim schaffnit said she had to resign or be terminated over the criminal charges and getting her job back looks unlikely the case began when the elderly neighbor s daughter came for an extended visit and discovered that her mother was not getting any mail investigators said the daughter discovered that lay had arranged for the mail to be held at the post office presumably so that the daughter would not see any of the bank statements or other financial documents once the daughter collected the mail and financial information she called the alachua county sheriff s office to report money was missing from the woman s bank account sgt tom wetherington determined that more than two dozen checks worth more than had been fraudulently written on the woman s account wetherington noted that most of the money was taken from the account in checks made out to lay or feindt the rest of the money was taken from the account by writing checks to others who apparently were unaware they may have been written fraudulently during the investigation which ultimately cleared feindt investigators discovered that lay had been convicted of similar crimes in south carolina and michigan lay was being held at the alachua county jail on monday awaiting transfer to the florida department of corrections gannett co inc all rights reserved original content available for non commercial use under a creative commons license except where noted gainesville sun sw th st gainesville fl do not sell my personal information cookie policy do not sell my personal information privacy policy terms of service your california privacy rights privacy policy gannett usa today network choose the plan that s right for you digital access or digital and print delivery \"\n\n[[2]]\n[1] \"neighbors please be mindful of social distancing guidelines while you do your part to slow the spread of the new coronavirus see the latest guidance from the cdc here this post was contributed by a community member the views expressed here are the authors own small joys art exhibit at artworks gallery on the green st paul s on the green in norwalk will host a retrospective art exhibit small joys thursday december through saturday december in the artworks gallery in the historic chittim howell house next to the church at east avenue in norwalk the exhibit features small original artworks including mixed media painting and photography from regional artists whose work has previously appeared at the gallery they include merion frolich marcy juran elisa keogh kendall klingbeil tom kretsch scott kuykendall susan leggitt kerwin kipp mayers wendy moore gregg welz and nancy woodward the public is invited to the opening reception which includes light refreshments on thursday december from p m visitors can also shop for unique holiday gifts decorations and collectibles at the st paul s holiday market on the green boutique in the chittim howell house for more information please visit the st paul s website at thursday pm bedford katonah ny friday am new canaan ct friday pm new york city ny friday pm weston redding easton ct saturday am norwalk ct saturday am new canaan ct saturday am paramus nj\"\n\n[[3]]\n[1] \"order flowers for the family show your sympathy to the family november september robert irwin bob worzalla died peacefully at life path hospice care on sep in tampa florida at the age of he was born on nov in stevens point to the late irwin and hildegard worzalla and he was married to dorothy kiefer in at sacred heart catholic church in nekoosa presided by his uncle father dennis worzalla bob started a career in television repair in wisconsin rapids at miller tv over the next years the couple welcomed two children into their home robert michael worzalla and david john worzalla and set about teaching them life s lesson his children would remember him as a driven father who encouraged them to pursue their goals in he started a potato farm golden boy farms followed by a career with n e isaacson associates a lake developer in various positions with the company in he started a seafood distributorship central wisconsin seafoods with his youngest son david in he opened a health food store in brandon florida and ran this business until he retired in he was a generous witty and dedicated individual who loved the health care field and who was passionate about helping people improve their health by using natural herbal remedies bob is survived by son david j karen worzalla of pawleys island south carolina sisters patricia david olson of stevens point yvonne david kopperud of stevens point and bonnie bill wright of madison grandchildren eric worzalla of misawa japan curtis worzalla of colorado springs colorado sophie worzalla of camp douglas and nash worzalla of pawleys island south carolina and great granddaughter laney worzalla of paden city virginia he is preceded in death by son robert m tammy worzalla of camp douglas and brother john whitey worzalla of stevens point no memorial service will be held we ask instead that those of you who knew him to take a moment to remember him and celebrate his life for online condolences to the family please visit to send a flower arrangement or to plant trees in memory of robert irwin worzalla please click here to visit our sympathy store copyright pisarski funeral homes all rights reserved funeral home website by batesville inc funeral planning and grief resources terms of use privacy\""
  },
  {
    "objectID": "materials/slides/week5-text.html#processed-data",
    "href": "materials/slides/week5-text.html#processed-data",
    "title": "Text to data: basic NLP",
    "section": "Processed data",
    "text": "Processed data\n\nclean <- rawdata_relabeled %>%\n  filter(str_detect(text_tmp, '<!')) %>%\n  rowwise() %>%\n  mutate(text_clean = parse_fn(text_tmp)) %>%\n  select(-text_tmp) %>%\n  unnest(text_clean)\n\nclean %>% head()\n\n# A tibble: 6 × 3\n  .id   bclass     text_clean                                                   \n  <chr> <fct>      <chr>                                                        \n1 url1  relevant   \"blytheville ar kait one man is in custody after a hour poli…\n2 url2  irrelevant \"\"                                                           \n3 url3  irrelevant \"email password dont have an account sign up now forgot your…\n4 url4  relevant   \"username or email address password remember me presque isle…\n5 url5  relevant   \" this may take a moment this may take a moment \"            \n6 url6  relevant   \"\""
  },
  {
    "objectID": "materials/slides/week5-text.html#about-nlp",
    "href": "materials/slides/week5-text.html#about-nlp",
    "title": "Text to data: basic NLP",
    "section": "About NLP",
    "text": "About NLP\nNatural language processing(NLP) refers to techniques for processing and analyzing speech and text. Although a specialized subfield, it comprises a broad range of problems and methods, including:\n\ntext and speech processing and representation\nautomated summarization\nspeech recognition\nmachine translation\nsentiment analysis"
  },
  {
    "objectID": "materials/slides/week5-text.html#text-processing-techniques",
    "href": "materials/slides/week5-text.html#text-processing-techniques",
    "title": "Text to data: basic NLP",
    "section": "Text processing techniques",
    "text": "Text processing techniques\nWe will focus here on NLP techniques for processing text, i.e., converting text into data.\n\ntokenization: breaking a string of text into smaller units\nlemmatization: converting tokens into common forms\ncalculating frequency measures"
  },
  {
    "objectID": "materials/slides/week5-text.html#tokenization",
    "href": "materials/slides/week5-text.html#tokenization",
    "title": "Text to data: basic NLP",
    "section": "Tokenization",
    "text": "Tokenization\nBreaking a string of text into subunits is called tokenization.\n\nConsider this string:\n\n\n[1] \"if you are a dreamer come in if you are a dreamer a wisher a liar a hope er a pray er a magic bean buyer if youre a pretender come sit by my fire for we have some flax golden tales to spin come in come in\""
  },
  {
    "objectID": "materials/slides/week5-text.html#word-tokenization",
    "href": "materials/slides/week5-text.html#word-tokenization",
    "title": "Text to data: basic NLP",
    "section": "Word tokenization",
    "text": "Word tokenization\nThe tokenizers package [docs] contains various tokenization functions. The most elementary method is to treat each word as a token.\n\nlibrary(tokenizers)\n\ninvitation_text %>% tokenize_words()\n\n[[1]]\n [1] \"if\"        \"you\"       \"are\"       \"a\"         \"dreamer\"   \"come\"     \n [7] \"in\"        \"if\"        \"you\"       \"are\"       \"a\"         \"dreamer\"  \n[13] \"a\"         \"wisher\"    \"a\"         \"liar\"      \"a\"         \"hope\"     \n[19] \"er\"        \"a\"         \"pray\"      \"er\"        \"a\"         \"magic\"    \n[25] \"bean\"      \"buyer\"     \"if\"        \"youre\"     \"a\"         \"pretender\"\n[31] \"come\"      \"sit\"       \"by\"        \"my\"        \"fire\"      \"for\"      \n[37] \"we\"        \"have\"      \"some\"      \"flax\"      \"golden\"    \"tales\"    \n[43] \"to\"        \"spin\"      \"come\"      \"in\"        \"come\"      \"in\""
  },
  {
    "objectID": "materials/slides/week5-text.html#n-gram-tokens",
    "href": "materials/slides/week5-text.html#n-gram-tokens",
    "title": "Text to data: basic NLP",
    "section": "N-gram tokens",
    "text": "N-gram tokens\nAlternatively, one could tokenize by n-grams: unique combinations of \\(n\\) adjacent words.\n\ninvitation_text %>% tokenize_ngrams(n = 2)\n\n[[1]]\n [1] \"if you\"         \"you are\"        \"are a\"          \"a dreamer\"     \n [5] \"dreamer come\"   \"come in\"        \"in if\"          \"if you\"        \n [9] \"you are\"        \"are a\"          \"a dreamer\"      \"dreamer a\"     \n[13] \"a wisher\"       \"wisher a\"       \"a liar\"         \"liar a\"        \n[17] \"a hope\"         \"hope er\"        \"er a\"           \"a pray\"        \n[21] \"pray er\"        \"er a\"           \"a magic\"        \"magic bean\"    \n[25] \"bean buyer\"     \"buyer if\"       \"if youre\"       \"youre a\"       \n[29] \"a pretender\"    \"pretender come\" \"come sit\"       \"sit by\"        \n[33] \"by my\"          \"my fire\"        \"fire for\"       \"for we\"        \n[37] \"we have\"        \"have some\"      \"some flax\"      \"flax golden\"   \n[41] \"golden tales\"   \"tales to\"       \"to spin\"        \"spin come\"     \n[45] \"come in\"        \"in come\"        \"come in\""
  },
  {
    "objectID": "materials/slides/week5-text.html#stopwords",
    "href": "materials/slides/week5-text.html#stopwords",
    "title": "Text to data: basic NLP",
    "section": "Stopwords",
    "text": "Stopwords\nSome tokens are thought to contain little semantic information, such as logical connectives, pronouns, and the like.\n\nIn NLP these are treated as stopwords: words that are stopped in text processing.\n\nlibrary(stopwords)\n\n# display 10 random stopwords from the 'snowball' dictionary\nset.seed(102422)\nstopwords(language = 'en', source = 'snowball') %>% \n  sample(size = 10)\n\n [1] \"but\"      \"off\"      \"didn't\"   \"nor\"      \"yourself\" \"they\"    \n [7] \"ought\"    \"when's\"   \"wouldn't\" \"they'll\""
  },
  {
    "objectID": "materials/slides/week5-text.html#stopword-removal",
    "href": "materials/slides/week5-text.html#stopword-removal",
    "title": "Text to data: basic NLP",
    "section": "Stopword removal",
    "text": "Stopword removal\nIn the default stopword list, some stopwords include punctuation. Since this was removed from our string, it should also be removed from the stopword list for effective string matching.\n\nstopwords_nopunct <- stopwords() %>% \n  str_remove_all('[[:punct:]]')\n\ninvitation_text %>%\n  tokenize_words(stopwords = stopwords_nopunct)\n\n[[1]]\n [1] \"dreamer\"   \"come\"      \"dreamer\"   \"wisher\"    \"liar\"      \"hope\"     \n [7] \"er\"        \"pray\"      \"er\"        \"magic\"     \"bean\"      \"buyer\"    \n[13] \"pretender\" \"come\"      \"sit\"       \"fire\"      \"flax\"      \"golden\"   \n[19] \"tales\"     \"spin\"      \"come\"      \"come\""
  },
  {
    "objectID": "materials/slides/week5-text.html#lemmatization",
    "href": "materials/slides/week5-text.html#lemmatization",
    "title": "Text to data: basic NLP",
    "section": "Lemmatization",
    "text": "Lemmatization\nLemmatization refers to grouping word inflections into a single form. See textstem [docs].\n\nlibrary(textstem)\n\nlocomotion <- c('run', 'running', 'ran', \n                'boating', 'boat', \n                'swim', 'swam', 'swimming', 'swum') \n\nlocomotion %>% lemmatize_words()\n\n[1] \"run\"  \"run\"  \"run\"  \"boat\" \"boat\" \"swim\" \"swim\" \"swim\" \"swim\"\n\n\n\nAlso works by pattern matching and replacement using a source dictionary."
  },
  {
    "objectID": "materials/slides/week5-text.html#tidytextunnest_tokens",
    "href": "materials/slides/week5-text.html#tidytextunnest_tokens",
    "title": "Text to data: basic NLP",
    "section": "tidytext::unnest_tokens()",
    "text": "tidytext::unnest_tokens()\n\n\n\nThe tidytext package [docs] contains wrappers around tokenizers and other functions for use in tidyverse-style programming.\n\n\n# A tibble: 36 × 3\n   doc        token   token.lem\n   <chr>      <chr>   <chr>    \n 1 invitation dreamer dreamer  \n 2 invitation come    come     \n 3 invitation dreamer dreamer  \n 4 invitation wisher  wisher   \n 5 invitation liar    liar     \n 6 invitation hope    hope     \n 7 invitation er      er       \n 8 invitation pray    pray     \n 9 invitation er      er       \n10 invitation magic   magic    \n# … with 26 more rows"
  },
  {
    "objectID": "materials/slides/week5-text.html#quality-check",
    "href": "materials/slides/week5-text.html#quality-check",
    "title": "Text to data: basic NLP",
    "section": "Quality check",
    "text": "Quality check\nText processing is error-prone. The functions shown here are handy but imperfect. You should always perform quality checks to identify bugs!\n\nSee any problems?\n\n\n [1] \"dreamer\"   \"come\"      \"wisher\"    \"liar\"      \"hope\"      \"er\"       \n [7] \"pray\"      \"magic\"     \"bean\"      \"buyer\"     \"pretender\" \"sit\"      \n[13] \"fire\"      \"flax\"      \"golden\"    \"tale\"      \"spin\"      \"boat\"     \n[19] \"just\"      \"build\"     \"fine\"      \"try\"       \"tell\"      \"us\"       \n[25] \"side\"      \"back\"      \"divine\"    \"bottom\"    \"guess\"     \"forget\""
  },
  {
    "objectID": "materials/slides/week5-text.html#corpora",
    "href": "materials/slides/week5-text.html#corpora",
    "title": "Text to data: basic NLP",
    "section": "Corpora",
    "text": "Corpora\nWe can summarize a text corpus (collection of texts) as:\n\n\\(T = \\{t_1, \\dots, t_p\\}\\) set of \\(p\\) unique tokens\n\\(D = \\{d_1, \\dots, d_n\\}\\) set of \\(n\\) documents comprising some corpus\n\n\\(d_i = \\{t_{i1}, \\dots, t_{iL_i}: t_{ij} \\in T\\}\\) is the \\(i\\)th document"
  },
  {
    "objectID": "materials/slides/week5-text.html#frequency-measures",
    "href": "materials/slides/week5-text.html#frequency-measures",
    "title": "Text to data: basic NLP",
    "section": "Frequency measures",
    "text": "Frequency measures\nThen we can define the following:\n\ndocumentwise token counts \\(n_{ti} = \\sum_j \\mathbf{1}\\{t_{ij} = t\\}\\)\n\nnumber of times token \\(t\\) appears in document \\(i\\)\n\ncorpuswise token counts \\(n_t = \\sum_i \\mathbf{1}\\{t \\in d_i\\}\\)\n\nnumber of documents containing token \\(t\\)\n\nterm frequency: \\(\\text{tf}(t, i) = n_{ti}/n_i\\)\ndocument frequency: \\(\\text{df}(t, i) = n_t/n\\)\ninverse document frequency: \\(-\\log\\left(\\text{df}(t, i)\\right)\\)"
  },
  {
    "objectID": "materials/slides/week5-text.html#computing-tf-idf",
    "href": "materials/slides/week5-text.html#computing-tf-idf",
    "title": "Text to data: basic NLP",
    "section": "Computing TF-IDF",
    "text": "Computing TF-IDF\nContinuing with our toy example:\n\n\n# A tibble: 4 × 6\n  doc        token.lem  n_ti     tf   idf tf_idf\n  <chr>      <chr>     <int>  <dbl> <dbl>  <dbl>\n1 invitation come          4 0.182  0.693 0.126 \n2 boat       just          2 0.143  0.693 0.0990\n3 invitation dreamer       2 0.0909 0.693 0.0630\n4 invitation er            2 0.0909 0.693 0.0630\n\n\n\nQuestion: what does it mean that IDF is the same for all terms?"
  },
  {
    "objectID": "materials/slides/week5-text.html#tf-idf",
    "href": "materials/slides/week5-text.html#tf-idf",
    "title": "Text to data: basic NLP",
    "section": "TF-IDF",
    "text": "TF-IDF\nThe last column was the product of term frequency and inverse document frequency, known as TF-IDF:\n\\[\n\\text{tf-idf}(t, i) = \\text{tf}(t, i) \\times \\text{idf}(t, i)\n\\]\n\nInterpretation:\n\nhigher values indicate rare words used often in a document\nlower values indicate common words used infrequently in a document"
  },
  {
    "objectID": "materials/slides/week5-text.html#document-term-matrix",
    "href": "materials/slides/week5-text.html#document-term-matrix",
    "title": "Text to data: basic NLP",
    "section": "Document term matrix",
    "text": "Document term matrix\nFinally, we can pivot a selected frequency measure into a data frame in which:\n\neach row is a document\neach column is a token\neach value is a frequency measure\n\n\n\n\n# A tibble: 2 × 31\n  doc        back   boat bottom  build divine   fine forget  guess   just   side\n  <chr>     <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n1 boat     0.0495 0.0495 0.0495 0.0495 0.0495 0.0495 0.0495 0.0495 0.0990 0.0495\n2 invitat… 0      0      0      0      0      0      0      0      0      0     \n# … with 20 more variables: tell <dbl>, try <dbl>, us <dbl>, bean <dbl>,\n#   buyer <dbl>, come <dbl>, dreamer <dbl>, er <dbl>, fire <dbl>, flax <dbl>,\n#   golden <dbl>, hope <dbl>, liar <dbl>, magic <dbl>, pray <dbl>,\n#   pretender <dbl>, sit <dbl>, spin <dbl>, tale <dbl>, wisher <dbl>\n\n\n\n\nQuestion: how would you check whether the ‘documents’ have words in common?"
  },
  {
    "objectID": "materials/slides/week5-text.html#processed-fraud-claim-data",
    "href": "materials/slides/week5-text.html#processed-fraud-claim-data",
    "title": "Text to data: basic NLP",
    "section": "Processed fraud claim data",
    "text": "Processed fraud claim data\nTF-IDF document term matrix for word tokens:\n\n\n# A tibble: 3 × 15,870\n  .id    bclass     adams afternoon  agent android    app arkansas arrest arrive\n  <chr>  <fct>      <dbl>     <dbl>  <dbl>   <dbl>  <dbl>    <dbl>  <dbl>  <dbl>\n1 url1   relevant  0.0692    0.0300 0.0365  0.0450 0.0330   0.0390 0.0140 0.0305\n2 url10  irreleva… 0         0      0       0      0        0      0      0     \n3 url100 irreleva… 0         0      0       0      0        0      0      0     \n# … with 15,860 more variables: assist <dbl>, attic <dbl>, barricade <dbl>,\n#   block <dbl>, blytheville <dbl>, burn <dbl>, captain <dbl>, catch <dbl>,\n#   check <dbl>, chemical <dbl>, copyright <dbl>, county <dbl>, custody <dbl>,\n#   dehydration <dbl>, demand <dbl>, department <dbl>, desktop <dbl>,\n#   device <dbl>, dispute <dbl>, division <dbl>, drug <dbl>, enter <dbl>,\n#   exit <dbl>, family <dbl>, federal <dbl>, fire <dbl>, force <dbl>,\n#   gas <dbl>, gray <dbl>, hartzell <dbl>, hold <dbl>, home <dbl>, …\n\n\n\n\\(n = 552\\) rows/observations (one per page)\ncolumns comprising \\(p = 15,742\\) variables and \\(1\\) class label"
  },
  {
    "objectID": "materials/slides/week5-text.html#next-time",
    "href": "materials/slides/week5-text.html#next-time",
    "title": "Text to data: basic NLP",
    "section": "Next time",
    "text": "Next time\nNext time we’ll discuss the capstone group’s analysis strategy:\n\ndimension reduction\nstatistical modeling"
  },
  {
    "objectID": "materials/slides/week6-nn.html#announcementsreminders",
    "href": "materials/slides/week6-nn.html#announcementsreminders",
    "title": "Artificial Neural Networks",
    "section": "Announcements/reminders",
    "text": "Announcements/reminders\nBefore your section meetings this week:\n\ninstall python\ncomplete pre-lab activity (to be posted with lab)\n\n\nPart of your assignment this week: fill out midquarter self-evaluation.\n\n\nAdd code request form for capstones later this week. Think now about whether you’re interested and check the schedule."
  },
  {
    "objectID": "materials/slides/week6-nn.html#discuss-with-your-table-3min",
    "href": "materials/slides/week6-nn.html#discuss-with-your-table-3min",
    "title": "Artificial Neural Networks",
    "section": "Discuss with your table (3min)",
    "text": "Discuss with your table (3min)\nWhat do you know (or have you heard) about neural networks?\n\nwhat are they\nwhat are they used for\nany other info?"
  },
  {
    "objectID": "materials/slides/week6-nn.html#graphical-model-diagram",
    "href": "materials/slides/week6-nn.html#graphical-model-diagram",
    "title": "Artificial Neural Networks",
    "section": "Graphical model diagram",
    "text": "Graphical model diagram\nConsider an arbitrary statistical model with one response \\(Y\\) and three predictors \\(x_1, x_2, x_3\\).\n\nA simple diagram of the model would look like this:\n\n\n\n\n\n\n\nG\n\n \n\ncluster_2\n\n response  \n\ncluster_0\n\n predictors   \n\nin1\n\n x1   \n\no1\n\n Y   \n\nin1->o1\n\n    \n\nin2\n\n x2   \n\nin2->o1\n\n    \n\nin3\n\n x3   \n\nin3->o1"
  },
  {
    "objectID": "materials/slides/week6-nn.html#graph-layers",
    "href": "materials/slides/week6-nn.html#graph-layers",
    "title": "Artificial Neural Networks",
    "section": "Graph layers",
    "text": "Graph layers\nA model that maps predictors directly to the response has just two “layers”:\n\nan input layer \\(X\\)\nan output layer \\(Y\\) (or more accurately \\(\\mathbb{E}Y\\))\n\n\nNeural networks add layers between the input and output."
  },
  {
    "objectID": "materials/slides/week6-nn.html#vanilla-neural-network",
    "href": "materials/slides/week6-nn.html#vanilla-neural-network",
    "title": "Artificial Neural Networks",
    "section": "‘Vanilla’ neural network",
    "text": "‘Vanilla’ neural network\n\n\n\n\n\n\n\n\n\nG\n\n \n\ncluster_1\n\n    \n\ncluster_2\n\n    \n\ncluster_0\n\n     \n\nin1\n\n     \n\nh1\n\n     \n\nin1->h1\n\n    \n\nh2\n\n     \n\nin1->h2\n\n    \n\nh3\n\n     \n\nin1->h3\n\n    \n\nh4\n\n     \n\nin1->h4\n\n    \n\nin2\n\n     \n\nin2->h1\n\n    \n\nin2->h2\n\n    \n\nin2->h3\n\n    \n\nin2->h4\n\n    \n\nin3\n\n     \n\nin3->h1\n\n    \n\nin3->h2\n\n    \n\nin3->h3\n\n    \n\nin3->h4\n\n    \n\no1\n\n     \n\nh1->o1\n\n    \n\nh2->o1\n\n    \n\nh3->o1\n\n    \n\nh4->o1\n\n   \n\n\n\n\n\n\n\none input layer\none hidden layer\none output layer\none parameter per edge"
  },
  {
    "objectID": "materials/slides/week6-nn.html#more-formally",
    "href": "materials/slides/week6-nn.html#more-formally",
    "title": "Artificial Neural Networks",
    "section": "More formally",
    "text": "More formally\nLet \\(Y\\in\\mathbb{R}^n\\) and \\(X \\in \\mathbb{R}^{n\\times p}\\) represent some data. The vanilla neural network is:\n\\[\\begin{aligned}\n\\color{#eed5b7}{\\mathbb{E}Y} &= \\sigma_z(\\color{#7ac5cd}{Z}\\color{#8b3e2f}{\\beta})\n  \\qquad &\\text{output layer}\\\\\n\n\\color{#7ac5cd}{Z} &=\n  \\left[\\sigma_x(\\color{#66cdaa}{X}\\color{#8b3e2f}{\\alpha_1})\n    \\;\\cdots\\;\n    \\sigma_x(\\color{#66cdaa}{X}\\color{#8b3e2f}{\\alpha_M})\\right]\n  \\qquad &\\text{hidden layer} \\\\\n  \n\\color{#66cdaa}{X} &= \\left[x_1 \\;\\cdots\\; x_p\\right]\n  \\qquad&\\text{input layer}\n\\end{aligned}\\]\n\n\\(\\sigma_x, \\sigma_z\\) are (known) activation functions\n\\(\\color{#8b3e2f}{\\beta}, \\color{#8b3e2f}{\\alpha}\\) are weights (model parameters)\n\n\\(p(M + 1)\\) of them as written"
  },
  {
    "objectID": "materials/slides/week6-nn.html#training-a-network",
    "href": "materials/slides/week6-nn.html#training-a-network",
    "title": "Artificial Neural Networks",
    "section": "Training a network",
    "text": "Training a network\nNotice that the output is simply a long composition:\n\\[\nY = f(X) \\quad\\text{where}\\quad f \\equiv \\sigma_z \\circ h_\\beta\\circ \\sigma_x \\circ h_\\alpha\n\\]\n\neach function is either known or linear\ncompute parameters by minimizing a loss function\nminimization by gradient descent"
  },
  {
    "objectID": "materials/slides/week6-nn.html#gradient-descent",
    "href": "materials/slides/week6-nn.html#gradient-descent",
    "title": "Artificial Neural Networks",
    "section": "Gradient descent",
    "text": "Gradient descent\nDenoting the parameter vector by \\(\\theta = \\left(\\alpha^T \\; \\beta^T\\right)\\), initialize \\(\\theta^{(0)}\\) and repeat:\n\\[\n\\theta^{(r + 1)} \\longleftarrow \\theta^{(r)} + c_r \\nabla L^{(r)}\n\\]\n\n\\(L^{(r)}\\) is a loss function evaluated at the \\(r\\)th iteration\n\nof the form \\(L^{(r)} = \\frac{1}{n}\\sum_i L_i (\\theta^{(r)}, Y)\\)\n\n\\(c_r\\) is the ‘learning rate’; can be fixed or chosen adaptively\neach cycle through all the parameters is one ‘epoch’"
  },
  {
    "objectID": "materials/slides/week6-nn.html#updates-for-the-vnn",
    "href": "materials/slides/week6-nn.html#updates-for-the-vnn",
    "title": "Artificial Neural Networks",
    "section": "Updates for the VNN",
    "text": "Updates for the VNN\nIndividual parameter updates at the \\(r\\)th iteration are given by:\n\\[\n\\beta_{m}^{(r + 1)} \\longleftarrow\n    \\beta_{m}^{(r)} + c_r \\underbrace{\\frac{1}{n}\\sum_{i = 1}^n \\frac{\\partial L_i}{\\partial \\beta_{m}}\\Big\\rvert_{\\beta_{m} = \\beta_{m}^{(r)}}}_{\\text{gradient at current iteration}} \\\\\n    \\alpha_{mp}^{(r + 1)} \\longleftarrow\n    \\alpha_{mp}^{(r)} + c_r \\underbrace{\\frac{1}{n}\\sum_{i = 1}^n \\frac{\\partial L_i}{\\partial \\alpha_{mp}}\\Big\\rvert_{\\alpha_{mp} = \\alpha_{mp}^{(r)}}}_{\\text{gradient at current iteration}}\n\\]"
  },
  {
    "objectID": "materials/slides/week6-nn.html#chain-rule",
    "href": "materials/slides/week6-nn.html#chain-rule",
    "title": "Artificial Neural Networks",
    "section": "Chain rule",
    "text": "Chain rule\nThe gradient is easy to compute. Denoting \\(t_{i} = z_{i}^T\\beta\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial L_i}{\\partial \\alpha_{mp}}\n    &= \\underbrace{\\frac{\\partial L_i}{\\partial f}\n        \\frac{\\partial f}{\\partial t_i}}_{\\delta_i}\n        \\underbrace{\\frac{\\partial t_i}{\\partial z_{im}}\n        \\frac{\\partial z_{im}}{\\partial \\alpha_{mp}}}_{s_{im}x_{ip}} \\\\\n\\frac{\\partial L_i}{\\partial \\beta_{m}}\n    &= \\underbrace{\\frac{\\partial L_i}{\\partial f}\n        \\frac{\\partial f}{\\partial t_i}}_{\\delta_i}\n        \\underbrace{\\frac{\\partial t_i}{\\partial \\beta_{m}}}_{z_{im}}\n\\end{aligned}\n\\]\n\nExplicitly computing gradients for each update gives the backpropagation algorithm of Rumelhart, Hinton, and Williams (1986) ."
  },
  {
    "objectID": "materials/slides/week6-nn.html#backpropagation",
    "href": "materials/slides/week6-nn.html#backpropagation",
    "title": "Artificial Neural Networks",
    "section": "Backpropagation",
    "text": "Backpropagation\nInitialize parameters and repeat:\n\nForward pass: compute \\(f(X), Z\\)\nBackward pass: compute \\(\\delta_i, s_{mi}\\) by ‘back-propagating’ current estimates\nUpdate the weights\n\\[\n\\hat{\\beta}_{km} \\longleftarrow\n  \\hat{\\beta}_{km} + c_r \\frac{1}{n}\\sum_i \\delta_{ki}z_{mi} \\\\\n  \\hat{\\alpha}_{mp} \\longleftarrow\n  \\hat{\\alpha}_{mp} + c_r \\frac{1}{n}\\sum_i s_{mi}x_{ip}\n\\]"
  },
  {
    "objectID": "materials/slides/week6-nn.html#gradient-estimation",
    "href": "materials/slides/week6-nn.html#gradient-estimation",
    "title": "Artificial Neural Networks",
    "section": "Gradient estimation",
    "text": "Gradient estimation\nExplicitly computing the gradient sums over all observations \\(i = 1, \\dots, n\\):\n\\[\ng = \\nabla \\frac{1}{n} \\sum_i L_i\n\\]\n\nIt’s much faster to estimate the gradient based on a “batch” of \\(m\\) observations (subsample) \\(J \\subset \\{1, \\dots, n\\}\\):\n\\[\n\\hat{g} = \\nabla \\frac{1}{m}\\sum_{i \\in J} L_i\n\\]"
  },
  {
    "objectID": "materials/slides/week6-nn.html#modern-optimization-methods",
    "href": "materials/slides/week6-nn.html#modern-optimization-methods",
    "title": "Artificial Neural Networks",
    "section": "Modern optimization methods",
    "text": "Modern optimization methods\nModern methods for training neural networks update parameters using gradient estimates and adaptive learning rates.1\n\nstochastic gradient descent (SGD): Bottou et al. (1998) replace \\(g\\) by \\(\\hat{g}\\)\nAdaGrad: Duchi, Hazan, and Singer (2011) use SGD with adaptive learning rates\nAdam: Kingma and Ba (2014) apply bias corrections to \\(\\hat{g}\\) based on moment estimates\n\nA nice overview is given in Goodfellow, Bengio, and Courville (2016) ."
  },
  {
    "objectID": "materials/slides/week6-nn.html#increasing-width",
    "href": "materials/slides/week6-nn.html#increasing-width",
    "title": "Artificial Neural Networks",
    "section": "Increasing width",
    "text": "Increasing width\n\n\n\n\n\n\n\nG\n\n \n\ncluster_0\n\n    \n\ncluster_1\n\n    \n\ncluster_2\n\n     \n\nin1\n\n     \n\nh1\n\n     \n\nin1->h1\n\n    \n\nh2\n\n     \n\nin1->h2\n\n    \n\nh3\n\n     \n\nin1->h3\n\n    \n\nh4\n\n     \n\nin1->h4\n\n    \n\nh5\n\n     \n\nin1->h5\n\n    \n\nin2\n\n     \n\nin2->h1\n\n    \n\nin2->h2\n\n    \n\nin2->h3\n\n    \n\nin2->h4\n\n    \n\nin2->h5\n\n    \n\nin3\n\n     \n\nin3->h1\n\n    \n\nin3->h2\n\n    \n\nin3->h3\n\n    \n\nin3->h4\n\n    \n\nin3->h5\n\n    \n\no1\n\n     \n\nh1->o1\n\n    \n\nh2->o1\n\n    \n\nh3->o1\n\n    \n\nh4->o1\n\n    \n\nh5->o1\n\n    \n\na\n\n     \n\no1->a\n\n    \n\nb\n\n width   \n\no1->b\n\n    \n\nc\n\n     \n\no1->c\n\n    \n\na->b\n\n    \n\nb->c\n\n   \n\n\nOne more hidden unit."
  },
  {
    "objectID": "materials/slides/week6-nn.html#increasing-depth",
    "href": "materials/slides/week6-nn.html#increasing-depth",
    "title": "Artificial Neural Networks",
    "section": "Increasing depth",
    "text": "Increasing depth\n\n\n\n\n\n\n\nG\n\n \n\ncluster_0\n\n    \n\ncluster_3\n\n    \n\ncluster_2\n\n    \n\ncluster_1\n\n     \n\nin1\n\n     \n\nh1\n\n     \n\nin1->h1\n\n    \n\nh2\n\n     \n\nin1->h2\n\n    \n\nh3\n\n     \n\nin1->h3\n\n    \n\nh4\n\n     \n\nin1->h4\n\n    \n\nh5\n\n     \n\nin1->h5\n\n    \n\nin2\n\n     \n\nin2->h1\n\n    \n\nin2->h2\n\n    \n\nin2->h3\n\n    \n\nin2->h4\n\n    \n\nin2->h5\n\n    \n\nin3\n\n     \n\nin3->h1\n\n    \n\nin3->h2\n\n    \n\nin3->h3\n\n    \n\nin3->h4\n\n    \n\nin3->h5\n\n    \n\nz1\n\n     \n\nh1->z1\n\n    \n\nz2\n\n     \n\nh1->z2\n\n    \n\nz3\n\n     \n\nh1->z3\n\n    \n\nz4\n\n     \n\nh1->z4\n\n    \n\nh2->z1\n\n    \n\nh2->z2\n\n    \n\nh2->z3\n\n    \n\nh2->z4\n\n    \n\nh3->z1\n\n    \n\nh3->z2\n\n    \n\nh3->z3\n\n    \n\nh3->z4\n\n    \n\nh4->z1\n\n    \n\nh4->z2\n\n    \n\nh4->z3\n\n    \n\nh4->z4\n\n    \n\nh5->z1\n\n    \n\nh5->z2\n\n    \n\nh5->z3\n\n    \n\nh5->z4\n\n    \n\no1\n\n     \n\nz1->o1\n\n    \n\nz2->o1\n\n    \n\nz3->o1\n\n    \n\nz4->o1\n\n    \n\na\n\n     \n\nb\n\n depth   \n\na->b\n\n    \n\nc\n\n     \n\nb->c\n\n   \n\n\nOne more hidden layer."
  },
  {
    "objectID": "materials/slides/week6-nn.html#sequential-networks",
    "href": "materials/slides/week6-nn.html#sequential-networks",
    "title": "Artificial Neural Networks",
    "section": "Sequential networks",
    "text": "Sequential networks\nNetworks of arbitrary width and depth in which the connectivity is uni-directional are known as “sequential” or “feedforward” networks/models.\n\\[\n\\begin{aligned}\n\\mathbb{E}Y &= \\sigma_1(Z_1\\beta_1) &\\text{output layer}\\\\\nZ_k &= \\sigma_k(Z_{k - 1} \\beta_k) &\\text{hidden layers } k = 2, \\dots, D - 1\\\\\nZ_D &\\equiv X &\\text{input layer}\n\\end{aligned}\n\\]\n\nchain rule calculations get longer but are otherwise the same\n“universal approximation” properties"
  },
  {
    "objectID": "materials/slides/week6-nn.html#approximation-properties",
    "href": "materials/slides/week6-nn.html#approximation-properties",
    "title": "Artificial Neural Networks",
    "section": "Approximation properties",
    "text": "Approximation properties\nSuppose:\n\n\\(\\mathbb{E}Y = f(X)\\) gives the ‘true’ relationship\n\\(\\tilde{f}(X)\\) represents the output layer of a feedforward neural network with one hidden layer of width \\(w\\)\n\n\nHornik, Stinchcombe, and White (1989) showed that, under some regularity conditions, for any \\(\\epsilon > 0\\) there exists a width \\(w\\) and parameters such that:\n\\[\n\\sup_x \\|f(x) - \\tilde{f}(x)\\| < \\epsilon\n\\]"
  },
  {
    "objectID": "materials/slides/week6-nn.html#approximation-properties-1",
    "href": "materials/slides/week6-nn.html#approximation-properties-1",
    "title": "Artificial Neural Networks",
    "section": "Approximation properties",
    "text": "Approximation properties\nSimilar results exist for deep networks with bounded width1.\n\nThese results do tell us that in most problems there exist both deep and shallow networks that approximate the true input-output relationship arbitrarily well\nThey don’t tell us how to find them.\n\nLu et al. (2017)"
  },
  {
    "objectID": "materials/slides/week6-nn.html#performance-considerations",
    "href": "materials/slides/week6-nn.html#performance-considerations",
    "title": "Artificial Neural Networks",
    "section": "Performance considerations",
    "text": "Performance considerations\nSeveral factors can affect actual performance in practice:\n\narchitecture (network structure)\nactivation function(s)\nloss function\noptimization method\nparameter initialization and training epochs\ndata quality (don’t forget this one!)"
  },
  {
    "objectID": "materials/slides/week6-nn.html#activations",
    "href": "materials/slides/week6-nn.html#activations",
    "title": "Artificial Neural Networks",
    "section": "Activations",
    "text": "Activations\nActivation functions \\(\\sigma(\\cdot)\\) determine whether a given unit ‘fires’.\n\n\n\n\n\n\n\nG\n\n \n\ncluster_0\n\n   \n\nunit1\n\n     \n\nsum\n\n Σ   \n\nunit1->sum\n\n   \n\nunit2\n\n     \n\nunit2->sum\n\n   \n\nunit3\n\n     \n\nunit3->sum\n\n   \n\nactivation\n\n σ(⋅)   \n\nsum->activation\n\n   \n\nout\n\n     \n\nactivation->out\n\n   \n\n\n\n\n\n\nFor example:\n\nif \\(Z_{k - 1}\\beta_{kj} = -28.2\\) and \\(\\sigma_k(x) = \\frac{1}{1 + e^{-x}}\\),\nthen \\(z_{kj} = \\sigma_k(Z_{k - 1}\\beta_j) \\approx 0\\)."
  },
  {
    "objectID": "materials/slides/week6-nn.html#common-activation-functions",
    "href": "materials/slides/week6-nn.html#common-activation-functions",
    "title": "Artificial Neural Networks",
    "section": "Common activation functions",
    "text": "Common activation functions\nThe most common activation functions are:\n\n(identity) \\(\\sigma(x) = x\\)\n(sigmoid) \\(\\sigma(x) = \\frac{1}{1 + \\exp\\{-x\\}}\\)\n(hyperbolic tangent) \\(\\sigma(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\)\n(rectified linear unit) \\(\\sigma(x) = \\max (0, x)\\)"
  },
  {
    "objectID": "materials/slides/week6-nn.html#loss-functions",
    "href": "materials/slides/week6-nn.html#loss-functions",
    "title": "Artificial Neural Networks",
    "section": "Loss functions",
    "text": "Loss functions\nThe most common loss function for classification is\n\\[\nL(Y, f(X)) = -\\frac{1}{n}\\sum_i \\left[y_i\\log p_i + (1 - y_i)\\log(1 - p_i)\\right]\n\\qquad\\text{(cross-entropy)}\n\\]\n\nThe most common loss function for regression is:\n\\[\nL(Y, f(X)) = \\frac{1}{n}\\sum_i (y_i - f(x_i))^2\n\\qquad\\text{(mean squared error)}\n\\]"
  },
  {
    "objectID": "materials/slides/week6-nn.html#references",
    "href": "materials/slides/week6-nn.html#references",
    "title": "Artificial Neural Networks",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nBottou, Léon et al. 1998. “Online Learning and Stochastic Approximations.” On-Line Learning in Neural Networks 17 (9): 142.\n\n\nDuchi, John, Elad Hazan, and Yoram Singer. 2011. “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.” Journal of Machine Learning Research 12 (7).\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press.\n\n\nHornik, Kurt, Maxwell Stinchcombe, and Halbert White. 1989. “Multilayer Feedforward Networks Are Universal Approximators.” Neural Networks 2 (5): 359–66.\n\n\nKingma, Diederik P, and Jimmy Ba. 2014. “Adam: A Method for Stochastic Optimization.” arXiv Preprint arXiv:1412.6980.\n\n\nLu, Zhou, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. 2017. “The Expressive Power of Neural Networks: A View from the Width.” Advances in Neural Information Processing Systems 30.\n\n\nRumelhart, David E, Geoffrey E Hinton, and Ronald J Williams. 1986. “Learning Representations by Back-Propagating Errors.” Nature 323 (6088): 533–36."
  },
  {
    "objectID": "materials/slides/week6-planning.html#todays-agenda",
    "href": "materials/slides/week6-planning.html#todays-agenda",
    "title": "",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nDiscuss Winter/Spring enrollment\nDemo assignment repository\nGroup meetings"
  },
  {
    "objectID": "materials/slides/week6-planning.html#capstone-projects",
    "href": "materials/slides/week6-planning.html#capstone-projects",
    "title": "",
    "section": "Capstone projects",
    "text": "Capstone projects\n\n10 projects, half industry, half lab\nproject abstracts will be distributed following Thanksgiving\nyou get to rank preferred projects\nassignments made over the break"
  },
  {
    "objectID": "materials/slides/week6-planning.html#expectations",
    "href": "materials/slides/week6-planning.html#expectations",
    "title": "",
    "section": "Expectations",
    "text": "Expectations\n\ncommit to two terms of work\n\ncompose and sign a team contract defining project roles and teamwork expectations\n\n4 credits per term \\(\\approx\\) 12hr/week commitment\n\none 1.25hr class meeting per week\none 1hr project meeting per week with sponsor and mentor and one 1hr+ student team meeting per week\n~2hr of meeting prep and ‘soft’ tasks per week\n5hr of project work per week\n~2hr flex time"
  },
  {
    "objectID": "materials/slides/week6-planning.html#timeline",
    "href": "materials/slides/week6-planning.html#timeline",
    "title": "",
    "section": "Timeline",
    "text": "Timeline\n\nnow: fill out enrollment request form (even if you’re not sure about commitment right now)\nend of fall: review project abstracts and specify preferences\nwinter break: receive assignments and develop motivational statement\nwinter quarter: develop team contract and begin work\n\npeer review and reflection at midquarter, start of spring, and mid-spring\n\nend of spring: poster showcase on campus"
  },
  {
    "objectID": "materials/slides/week6-planning.html#group-assignment",
    "href": "materials/slides/week6-planning.html#group-assignment",
    "title": "",
    "section": "Group assignment",
    "text": "Group assignment\nSit with your team.\nGo to the table number matching your team number mod 10."
  },
  {
    "objectID": "materials/slides/week7-curvefitting.html#word-clouds",
    "href": "materials/slides/week7-curvefitting.html#word-clouds",
    "title": "Curve fitting using basis approximations",
    "section": "Word clouds",
    "text": "Word clouds\n\nFrom self-assessments: identify a helpful feature that improved groupwork."
  },
  {
    "objectID": "materials/slides/week7-curvefitting.html#word-clouds-1",
    "href": "materials/slides/week7-curvefitting.html#word-clouds-1",
    "title": "Curve fitting using basis approximations",
    "section": "Word clouds",
    "text": "Word clouds\n\nFrom self-assessments: identify an area of improvement"
  },
  {
    "objectID": "materials/slides/week7-curvefitting.html#above",
    "href": "materials/slides/week7-curvefitting.html#above",
    "title": "Curve fitting using basis approximations",
    "section": "ABoVE",
    "text": "ABoVE\nThe Arctic-Boreal Vulnerability Experiment (ABoVE) is a NASA Terrestrial Ecology Program field campaign in Alaska and western Canada from 2016 to 2021.\n\nResearch for ABoVE will link field-based, process-level studies with geospatial data products derived from airborne and satellite sensors, providing a foundation for improving the analysis, and modeling capabilities needed to understand and predict ecosystem responses and societal implications.\n\n\nFind ABoVE data here."
  },
  {
    "objectID": "materials/slides/week7-curvefitting.html#above-soil-temperatures",
    "href": "materials/slides/week7-curvefitting.html#above-soil-temperatures",
    "title": "Curve fitting using basis approximations",
    "section": "ABoVE Soil Temperatures",
    "text": "ABoVE Soil Temperatures\nWe’ll work with soil temperatures.\n\nNicolsky, D.J., V.E. Romanovsky, A.L. Kholodov, K. Dolgikh, and N. Hasson. 2022. ABoVE: Soil Temperature Profiles, USArray Seismic Stations, 2016-2021. ORNL DAAC, Oak Ridge, Tennessee, USA. https://doi.org/10.3334/ORNLDAAC/1680\n\n\nObservations of soil temperatures (centigrade)\nMeasured at 63 locations in Alaska\nRecorded four times daily at multiple depths"
  },
  {
    "objectID": "materials/slides/week7-curvefitting.html#site-locations",
    "href": "materials/slides/week7-curvefitting.html#site-locations",
    "title": "Curve fitting using basis approximations",
    "section": "Site locations",
    "text": "Site locations\n\n\n\nWe’ll use 57 of the 63 sites for now.\n\nLocations of selected monitoring stations."
  },
  {
    "objectID": "materials/slides/week7-curvefitting.html#example-rows",
    "href": "materials/slides/week7-curvefitting.html#example-rows",
    "title": "Curve fitting using basis approximations",
    "section": "Example rows",
    "text": "Example rows\n\n\n# A tibble: 6 × 7\n  site   latitude longitude elevation date_time           depth  temp\n  <chr>     <dbl>     <dbl>     <dbl> <dttm>              <dbl> <dbl>\n1 B21K-1     69.6     -155.        96 2017-08-14 12:00:00   0    3.46\n2 B21K-1     69.6     -155.        96 2017-08-14 12:00:00   0.2  3.48\n3 B21K-1     69.6     -155.        96 2017-08-14 12:00:00   1    3.38\n4 B21K-1     69.6     -155.        96 2017-08-14 12:00:00   1.5  3.31\n5 B21K-1     69.6     -155.        96 2017-08-14 18:00:00   0    3.92\n6 B21K-1     69.6     -155.        96 2017-08-14 18:00:00   0.2  4.16"
  },
  {
    "objectID": "materials/slides/week7-curvefitting.html#temperature-profiles",
    "href": "materials/slides/week7-curvefitting.html#temperature-profiles",
    "title": "Curve fitting using basis approximations",
    "section": "Temperature profiles",
    "text": "Temperature profiles\n\nObservations for a single site at four depths (one path per depth).\nWhat is happening over time?\nWhat is happening across depth?\nAny other observations?"
  },
  {
    "objectID": "materials/slides/week7-curvefitting.html#comparing-sites",
    "href": "materials/slides/week7-curvefitting.html#comparing-sites",
    "title": "Curve fitting using basis approximations",
    "section": "Comparing sites",
    "text": "Comparing sites\n\nProfiles at two sites.\nHow do the sites differ?\nHow are they similar?"
  },
  {
    "objectID": "materials/slides/week7-curvefitting.html#comparing-sites-1",
    "href": "materials/slides/week7-curvefitting.html#comparing-sites-1",
    "title": "Curve fitting using basis approximations",
    "section": "Comparing sites",
    "text": "Comparing sites\n\n\n\n\n\n\n\n\nHere are the locations of the sites just compared.\n\nwhat factors might account for some of the differences in temperature profiles between the sites?\nare any of them recorded in our data?"
  },
  {
    "objectID": "materials/slides/week7-curvefitting.html#goals",
    "href": "materials/slides/week7-curvefitting.html#goals",
    "title": "Curve fitting using basis approximations",
    "section": "Goals",
    "text": "Goals\nOur overall goal this week is to build a forecasting model.\n\nStrategy:\n\nTo start, approximate the seasonal trend.\nDe-trend the data and model the correlation structure of deviations from seasonal trend.\nForecast as: \\(\\text{trend} + \\mathbb{E}(\\text{future}| \\text{present})\\)"
  },
  {
    "objectID": "materials/slides/week7-curvefitting.html#annual-cycles",
    "href": "materials/slides/week7-curvefitting.html#annual-cycles",
    "title": "Curve fitting using basis approximations",
    "section": "Annual cycles",
    "text": "Annual cycles\nThe seasonality is annual – let’s examine the annual cycle instead of the usual time course plot.\n\n\n\n\n\n\n\nAnnual cycle for site H17K-1 at 0.2m depth.\n\n\n\n\n\n\nCan you see the start and stop dates in the plot?\nAny other observations?"
  },
  {
    "objectID": "materials/slides/week7-curvefitting.html#pooling-sites",
    "href": "materials/slides/week7-curvefitting.html#pooling-sites",
    "title": "Curve fitting using basis approximations",
    "section": "Pooling sites",
    "text": "Pooling sites\nHow would you estimate the annual cycle based on data at each site?\n\nDaily average temperatures at 0.2m depth for 37 sites, 2017-2019."
  },
  {
    "objectID": "materials/slides/week7-curvefitting.html#as-an-estimation-problem",
    "href": "materials/slides/week7-curvefitting.html#as-an-estimation-problem",
    "title": "Curve fitting using basis approximations",
    "section": "As an estimation problem",
    "text": "As an estimation problem\nModeling the trend can be formulated as estimating the model:\n\\[\nY_{i, t} = f(t) + \\epsilon_{i, t}\n\\]\n\nWhere:\n\n\\(Y_{i, t}\\) is the temperature at site \\(i\\) and time \\(t\\)\n\\(f(t)\\) is the mean temperature at time \\(t\\)\n\\(\\epsilon_{i, t}\\) is a random error\n\n\n\nBut how do you estimate an arbitrary function?"
  },
  {
    "objectID": "materials/slides/week7-curvefitting.html#basis-functions",
    "href": "materials/slides/week7-curvefitting.html#basis-functions",
    "title": "Curve fitting using basis approximations",
    "section": "Basis functions",
    "text": "Basis functions\nA basis function is an element of a basis for a function space.\n\nIf \\(\\{f_j\\}\\) form a basis for a function space \\(C\\) then\n\\[\nf \\in C \\quad\\Longleftrightarrow f = \\sum_j c_j f_j\n\\]\n\n\nA finite subset of basis functions can be used to approximate functions in the space:\n\\[\nf \\approx \\sum_{j = 1}^J c_j f_j\n\\]"
  },
  {
    "objectID": "materials/slides/week7-curvefitting.html#basis-approximation",
    "href": "materials/slides/week7-curvefitting.html#basis-approximation",
    "title": "Curve fitting using basis approximations",
    "section": "Basis approximation",
    "text": "Basis approximation\nA nifty trick is to estimate \\(f\\) using a suitable basis approximation:\n\\[\nY_{i, t} = \\beta_0 + \\color{maroon}{\\underbrace{\\sum_{j = 1}^J \\beta_j f_j(t)}_{\\tilde{f}(t) \\approx f(t)}} + \\epsilon_{i, t}\n\\]\n\nThis model can be fit using standard linear regression. (Think of the \\(f_j(t)\\)’s as \\(J\\) ‘new’ predictors.)"
  },
  {
    "objectID": "materials/slides/week7-curvefitting.html#spline-basis",
    "href": "materials/slides/week7-curvefitting.html#spline-basis",
    "title": "Curve fitting using basis approximations",
    "section": "Spline basis",
    "text": "Spline basis\nThe spline basis is a basis for piecewise polynomials of a specified order.\n\nBases for piecewise polynomials of order 1 through 4 joined at evenly-spaced knot points.\nGenerated recursively based on ‘knots’ – joining locations"
  },
  {
    "objectID": "materials/slides/week7-curvefitting.html#knot-spacing",
    "href": "materials/slides/week7-curvefitting.html#knot-spacing",
    "title": "Curve fitting using basis approximations",
    "section": "Knot spacing",
    "text": "Knot spacing\nKnot spacing will affect how densely basis functions are concentrated around particular regions of data.\n\nHere are bases generated on some unevenly-spaced knots:\n\n\n\n\n\n\n\n\n\n\n\nCheck your understanding: where would this spline basis have the most flexible approximation capability?"
  },
  {
    "objectID": "materials/slides/week7-curvefitting.html#knot-placement",
    "href": "materials/slides/week7-curvefitting.html#knot-placement",
    "title": "Curve fitting using basis approximations",
    "section": "Knot placement",
    "text": "Knot placement\nAppropriate placement of knots is essential for quality function approximation.\n\ndefault: place at data quantiles\nbetter: concentrated in regions with irregular trend\n\n\n\n\n\n\n\n\n\n\nWhere would you put them for our data?"
  },
  {
    "objectID": "materials/slides/week7-curvefitting.html#a-first-attempt-spline-basis",
    "href": "materials/slides/week7-curvefitting.html#a-first-attempt-spline-basis",
    "title": "Curve fitting using basis approximations",
    "section": "A first attempt: spline basis",
    "text": "A first attempt: spline basis\nModel: \\(Y_{i,t} = \\beta_0 + \\beta_1\\cdot\\text{elev}_i + \\sum_{j = 1}^7 \\gamma_j \\cdot f_j(t) + \\epsilon_{i, t}\\)\n\nKnot placementFitted curve\n\n\n\n\n\n\n\nKnots placed at vertical lines.\n\n\n\n\n\n\n\n\n\n\n\nEstimated mean with 95% prediction interval at median site elevation."
  },
  {
    "objectID": "materials/slides/week7-curvefitting.html#a-problem",
    "href": "materials/slides/week7-curvefitting.html#a-problem",
    "title": "Curve fitting using basis approximations",
    "section": "A problem",
    "text": "A problem\n\n\n\n\n\n\n\nSpline bases produce discontinuities\n\n\n\n\n\nThe choice of basis must match problem context.\n\nhere, need boundaries to meet\nin other words, need a harmonic function"
  },
  {
    "objectID": "materials/slides/week7-curvefitting.html#fourier-basis",
    "href": "materials/slides/week7-curvefitting.html#fourier-basis",
    "title": "Curve fitting using basis approximations",
    "section": "Fourier basis",
    "text": "Fourier basis\nThe Fourier basis is a basis for square-integrable functions on closed intervals consisting of sine-cosine pairs.\n\n\n\n\n\n\n4 Fourier basis functions on the interval [1, 365]."
  },
  {
    "objectID": "materials/slides/week7-curvefitting.html#second-try",
    "href": "materials/slides/week7-curvefitting.html#second-try",
    "title": "Curve fitting using basis approximations",
    "section": "Second try",
    "text": "Second try\n\nSeasonal meanTime plot\n\n\n\n\n\n\n\nSeasonal mean approximation using 4 Fourier basis functions.\n\n\n\n\n\n\n\n\n\n\n\nSeasonal mean approxiamtion using 4 Fourier basis functions."
  },
  {
    "objectID": "materials/slides/week7-curvefitting.html#forecasting",
    "href": "materials/slides/week7-curvefitting.html#forecasting",
    "title": "Curve fitting using basis approximations",
    "section": "Forecasting",
    "text": "Forecasting\nDoes this forecast make sense? Why or why not?"
  },
  {
    "objectID": "materials/slides/week7-curvefitting.html#next-time",
    "href": "materials/slides/week7-curvefitting.html#next-time",
    "title": "Curve fitting using basis approximations",
    "section": "Next time",
    "text": "Next time\n\nFit a time series model to the residuals\n\\[\ne_{i, t} = Y_{i, t} - \\underbrace{\\left(\\hat{\\beta_0} + \\hat{\\beta_1}\\text{elev}_i + \\hat{f}(t)\\right)}_{\\text{mean function } \\hat{\\mu}(i, t)}\n\\]\nForecast \\(\\hat{e}_{i, t} = \\mathbb{E}\\left(e_{i, t}|e_{i, t - 1}\\right)\\) using the residual model\n“Feed forward” residual forecasts to obtain temperature forecasts\n\\[\n\\hat{Y}_{i, t} = \\hat{\\mu}(i, t) + \\hat{e}_{i, t}\n\\]"
  },
  {
    "objectID": "materials/slides/week7-forecasting.html#from-last-time",
    "href": "materials/slides/week7-forecasting.html#from-last-time",
    "title": "Building a forecasting model",
    "section": "From last time",
    "text": "From last time\n\nPooled data from all sites with at least a year of continuous observation between 2017 and 2020 at 0.2m depth\nModeled seasonal trend based on elevation and day of the year using curve fitting techniques"
  },
  {
    "objectID": "materials/slides/week7-forecasting.html#seasonal-trend-model",
    "href": "materials/slides/week7-forecasting.html#seasonal-trend-model",
    "title": "Building a forecasting model",
    "section": "Seasonal trend model",
    "text": "Seasonal trend model\n\n\n\nFrom last time: using 4 Fourier bases and elevation.\n\n\\(Y_{i, t} = \\beta_0 + \\beta_1 \\text{elev}_i + \\sum_{j = 1}^4 \\gamma_j \\phi_j(t) + \\epsilon_{i, t}\\)"
  },
  {
    "objectID": "materials/slides/week7-forecasting.html#a-small-adjustment",
    "href": "materials/slides/week7-forecasting.html#a-small-adjustment",
    "title": "Building a forecasting model",
    "section": "A small adjustment",
    "text": "A small adjustment\nAdding elevation x seasonality interaction terms.\n\n\\(Y_{i, t} = \\beta_0 + \\beta_1 \\text{elev}_i + \\sum_{j = 1}^4 \\left(\\gamma_j \\phi_j(t) + \\color{maroon}{\\delta_j \\text{elev}\\times\\phi_j(t)}\\right) + \\epsilon_{i, t}\\)"
  },
  {
    "objectID": "materials/slides/week7-forecasting.html#seasonal-forecast",
    "href": "materials/slides/week7-forecasting.html#seasonal-forecast",
    "title": "Building a forecasting model",
    "section": "Seasonal forecast",
    "text": "Seasonal forecast\nSeasonal forecasts ignore recent data.\n\n10-day forecast based on seasonal mean only."
  },
  {
    "objectID": "materials/slides/week7-forecasting.html#forecasting-error",
    "href": "materials/slides/week7-forecasting.html#forecasting-error",
    "title": "Building a forecasting model",
    "section": "Forecasting error",
    "text": "Forecasting error\nThis leads to greater error.\n\nSeasonal forecast vs. observed temperature."
  },
  {
    "objectID": "materials/slides/week7-forecasting.html#a-better-forecast",
    "href": "materials/slides/week7-forecasting.html#a-better-forecast",
    "title": "Building a forecasting model",
    "section": "A better forecast",
    "text": "A better forecast\nIdea: the present temperature at time \\(t\\) contains useful information about the expected temperature at time \\(t + 1\\).\n\nOur model for site \\(i\\) at time \\(t\\) is:\n\\[\n\\underbrace{Y_{i, t}}_{\\text{temperature}} = \\underbrace{\\mu(i, t)}_{\\text{seasonal mean}} + \\underbrace{\\epsilon_{i, t}}_{\\text{random deviation}}\n\\]\n\n\nThe conditional mean forecast \\(\\mathbb{E}(Y_{i, t + 1}| Y_{i, t})\\) should be better than the seasonal forecast \\(\\mathbb{E}Y_{i, t + 1} = \\mu(i, t + 1)\\)…\n\n\n… because it incorporates information about the recent past."
  },
  {
    "objectID": "materials/slides/week7-forecasting.html#conditional-mean-forecast",
    "href": "materials/slides/week7-forecasting.html#conditional-mean-forecast",
    "title": "Building a forecasting model",
    "section": "Conditional mean forecast",
    "text": "Conditional mean forecast\nThe conditional mean of the next temp given the present is:\n\\[\n\\begin{aligned}\n\\mathbb{E}(Y_{i, t + 1}|Y_{i, t} = y_{i, t})\n&= \\mathbb{E}[\\underbrace{\\mu(i, t + 1)}_{\\text{nonrandom}}|Y_{i, t} = y_{i, t}] + \\mathbb{E}(\\epsilon_{i, t + 1}|Y_{i, t} = y_{i, t}) \\\\\\\\\n&= \\mu(i, t + 1) + \\mathbb{E}(\\epsilon_{i, t + 1}|\\epsilon_{i, t} = \\underbrace{y_{i, t} - \\mu(i, t)}_{\\text{residual}}) \\\\\\\\\n&= \\underbrace{\\mu(i, t + 1)}_{\\text{seasonal forecast}} + \\underbrace{\\mathbb{E}(\\epsilon_{i, t + 1}|\\epsilon_{i, t} = e_{i, t})}_{\\text{forecasted deviation}}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "materials/slides/week7-forecasting.html#modeling-the-residuals",
    "href": "materials/slides/week7-forecasting.html#modeling-the-residuals",
    "title": "Building a forecasting model",
    "section": "Modeling the residuals",
    "text": "Modeling the residuals\nTo forecast the deviation from the seasonal mean, we should model the residuals.\n\\[\ne_{i, t} = Y_{i, t} - \\hat{\\mu}(i, t)\n\\qquad\\text{(residual)}\n\\]"
  },
  {
    "objectID": "materials/slides/week7-forecasting.html#residual-autocorrelation",
    "href": "materials/slides/week7-forecasting.html#residual-autocorrelation",
    "title": "Building a forecasting model",
    "section": "Residual autocorrelation",
    "text": "Residual autocorrelation\n\n\n\n\n\n\n\nResidual vs. lagged residual.\n\n\n\n\n\nResiduals are strongly correlated with their immediately previous value.\nThis is called autocorrelation (think: self-correlation)."
  },
  {
    "objectID": "materials/slides/week7-forecasting.html#an-intuitive-approach-slr",
    "href": "materials/slides/week7-forecasting.html#an-intuitive-approach-slr",
    "title": "Building a forecasting model",
    "section": "An intuitive approach: SLR",
    "text": "An intuitive approach: SLR\n(1) Lag the residuals: \\(\\texttt{resid} = e_{i, t}\\) and \\(\\texttt{resid.lag} = e_{i, t - 1}\\)\n\n\n# A tibble: 6 × 5\n# Groups:   site [1]\n  site   date        temp resid resid.lag\n  <chr>  <date>     <dbl> <dbl>     <dbl>\n1 B21K-1 2017-08-15  3.72 -6.18     -6.14\n2 B21K-1 2017-08-16  3.31 -6.52     -6.18\n3 B21K-1 2017-08-17  4.95 -4.81     -6.52\n4 B21K-1 2017-08-18  5.46 -4.24     -4.81\n5 B21K-1 2017-08-19  5.80 -3.82     -4.24\n6 B21K-1 2017-08-20  5.68 -3.87     -3.82\n\n\n\n(2) Fit SLR at one time lag: \\(e_{i, t} = \\beta_0 + \\beta_1 e_{i, t - 1} + \\xi_{i, t}\\).\n\nfit_resid <- lm(resid ~ resid.lag - 1, data = resid_df)"
  },
  {
    "objectID": "materials/slides/week7-forecasting.html#computing-one-step-forecasts",
    "href": "materials/slides/week7-forecasting.html#computing-one-step-forecasts",
    "title": "Building a forecasting model",
    "section": "Computing one-step forecasts",
    "text": "Computing one-step forecasts\n\n\n\n\nSeasonal forecastsResidual forecastsFinal forecasts\n\n\n\\(\\texttt{pred.mean} = \\hat{\\mu}(i, t)\\)\n\n\n# A tibble: 6 × 4\n  date        elev  temp pred.mean\n  <date>     <dbl> <dbl>     <dbl>\n1 2019-03-12   396 -2.85     -6.83\n2 2019-03-13   396 -2.86     -6.76\n3 2019-03-14   396 -2.92     -6.69\n4 2019-03-15   396 -3.16     -6.62\n5 2019-03-16   396 -3.34     -6.55\n6 2019-03-17   396 -3.28     -6.47\n\n\n\n\n\\(\\texttt{pred.resid} = \\hat{e}_{i, t} = \\mathbb{E}(e_{i, t}|e_{i, t - 1})\\)\n\n\n# A tibble: 6 × 6\n  date        elev  temp pred.mean resid pred.resid\n  <date>     <dbl> <dbl>     <dbl> <dbl>      <dbl>\n1 2019-03-12   396 -2.85     -6.83  3.98      NA   \n2 2019-03-13   396 -2.86     -6.76  3.90       3.90\n3 2019-03-14   396 -2.92     -6.69  3.77       3.83\n4 2019-03-15   396 -3.16     -6.62  3.46       3.69\n5 2019-03-16   396 -3.34     -6.55  3.20       3.39\n6 2019-03-17   396 -3.28     -6.47  3.18       3.14\n\n\n\n\n\\(\\texttt{pred} = \\texttt{pred.mean} + \\texttt{pred.resid} = \\hat{\\mu}(i, t) + \\hat{e}_{i, t}\\)\n\n\n# A tibble: 6 × 7\n  date        elev  temp pred.mean resid pred.resid  pred\n  <date>     <dbl> <dbl>     <dbl> <dbl>      <dbl> <dbl>\n1 2019-03-12   396 -2.85     -6.83  3.98      NA    NA   \n2 2019-03-13   396 -2.86     -6.76  3.90       3.90 -2.86\n3 2019-03-14   396 -2.92     -6.69  3.77       3.83 -2.86\n4 2019-03-15   396 -3.16     -6.62  3.46       3.69 -2.93\n5 2019-03-16   396 -3.34     -6.55  3.20       3.39 -3.15\n6 2019-03-17   396 -3.28     -6.47  3.18       3.14 -3.33"
  },
  {
    "objectID": "materials/slides/week7-forecasting.html#one-step-forecasts",
    "href": "materials/slides/week7-forecasting.html#one-step-forecasts",
    "title": "Building a forecasting model",
    "section": "One-step forecasts",
    "text": "One-step forecasts\n\nSeasonal forecast (blue curve), residual forecasts (red lines), one-step forecasts (dotted line), and observed temperatures (solid black line)."
  },
  {
    "objectID": "materials/slides/week7-forecasting.html#forecasting-error-1",
    "href": "materials/slides/week7-forecasting.html#forecasting-error-1",
    "title": "Building a forecasting model",
    "section": "Forecasting error",
    "text": "Forecasting error\nOne the site we’ve been examining, the one-step forecasting error is:\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nccc\nstandard\n0.9981049\n\n\nmsd\nstandard\n0.0536997\n\n\nrmse\nstandard\n0.3494716\n\n\n\n\n\n\nccc is the concordance correlation coefficient\nmsd is the mean signed deviation\nrmse is the root mean squared error"
  },
  {
    "objectID": "materials/slides/week7-forecasting.html#average-forecasting-error-across-sites",
    "href": "materials/slides/week7-forecasting.html#average-forecasting-error-across-sites",
    "title": "Building a forecasting model",
    "section": "Average forecasting error across sites",
    "text": "Average forecasting error across sites\nMeans and standard deviations across all 29 sites of error metrics for one-step forecasts computed across entire observation window:\n\n\n\n\n\n.metric\naverage\nsd\nmin\nmax\nn\n\n\n\n\nccc\n0.9949698\n0.0032392\n0.9869467\n0.9993684\n29\n\n\nmsd\n-0.0062058\n0.0510766\n0.0005584\n0.1186212\n29\n\n\nrmse\n0.6460647\n0.3593531\n0.1801040\n1.4875005\n29"
  },
  {
    "objectID": "materials/slides/week7-forecasting.html#one-step-forecasts-mathematically",
    "href": "materials/slides/week7-forecasting.html#one-step-forecasts-mathematically",
    "title": "Building a forecasting model",
    "section": "One-step forecasts (mathematically)",
    "text": "One-step forecasts (mathematically)\nThe one-step forecasts are the predicted conditional means at the next time step given the present :\n\\[\n\\hat{Y}_{i, t} = \\mathbb{E}(Y_{i, t + 1}|Y_{i, t} = \\color{blue}{y_{i, t}})\n\\]\n\nConditional expectation gives optimal prediction under squared error loss (assuming the model is correct).\n\n\nAccording to our model:\n\\[\n\\hat{Y}_{i, t + 1}\n=  \\hat{\\mu}(i, t + 1) + \\hat{\\alpha}_0 + \\hat{\\alpha}_1 \\left(\\color{blue}{y_{i, t}} - \\hat{\\mu}(i, t)\\right)\n\\]"
  },
  {
    "objectID": "materials/slides/week7-forecasting.html#multi-step-forecasts",
    "href": "materials/slides/week7-forecasting.html#multi-step-forecasts",
    "title": "Building a forecasting model",
    "section": "Multi-step forecasts",
    "text": "Multi-step forecasts\nMultistep forecasts must be computed recursively:\n\\[\n\\begin{aligned}\n\\color{maroon}{\\hat{Y}_{i, t + 1}}\n&= \\mathbb{E}(Y_{i, t + 1}|Y_{i, t} = y_{i, t}) \\\\\n\\color{teal}{\\hat{Y}_{i, t + 2}}\n&= \\mathbb{E}(Y_{i, t + 2}|Y_{i, t + 1} = \\color{maroon}{\\hat{Y}_{i, t + 1}}) \\\\\n\\hat{Y}_{i, t + 3}\n&= \\mathbb{E}(Y_{i, t + 3}|Y_{i, t + 2} = \\color{teal}{\\hat{Y}_{i, t + 2}}) \\\\\n&\\vdots\n\\end{aligned}\n\\]\n\nWhat do you think will happen the farther out we forecast??"
  },
  {
    "objectID": "materials/slides/week7-forecasting.html#multistep-forecasts-on-one-site",
    "href": "materials/slides/week7-forecasting.html#multistep-forecasts-on-one-site",
    "title": "Building a forecasting model",
    "section": "Multistep forecasts on one site",
    "text": "Multistep forecasts on one site"
  },
  {
    "objectID": "materials/slides/week7-forecasting.html#comments",
    "href": "materials/slides/week7-forecasting.html#comments",
    "title": "Building a forecasting model",
    "section": "Comments",
    "text": "Comments\nThis approach pooled data across sites to estimate model quantities.\n\nseasonal mean (using Fourier basis approximation)\nresidual autocorrelation at one lag (using SLR)\n\n\nWorks pretty well for one-step forecasts; not very well for longer-term forecasts."
  },
  {
    "objectID": "materials/slides/week7-forecasting.html#site-specific-approach",
    "href": "materials/slides/week7-forecasting.html#site-specific-approach",
    "title": "Building a forecasting model",
    "section": "Site-specific approach",
    "text": "Site-specific approach\nAn alternative to pooling data together to estimate the seasonal trend and residual autocorrelation is to do so individually for every site.\n\nupshot: more flexibility on approaches; can use time series techniques\ndownside: many models \\(\\longrightarrow\\) more total uncertainty\n\n\nFor now we’ll leave the seasonality alone and revise the residual autocorrelation approach."
  },
  {
    "objectID": "materials/slides/week7-forecasting.html#autoregression",
    "href": "materials/slides/week7-forecasting.html#autoregression",
    "title": "Building a forecasting model",
    "section": "Autoregression",
    "text": "Autoregression\nAn autoregressive model of order \\(D\\) is\n\\[\nX_t = \\nu + \\alpha_1 X_{t - 1} + \\cdots + \\alpha_D X_{t - D} + \\epsilon_t\n\\]\n\n‘innovations’ \\(\\epsilon_t\\) are \\(iid\\) with mean zero\nprocess mean linear in \\(D\\) lags\n\\(\\mathbb{E}X_t\\) and \\(\\text{var}X_t\\) are constant in time (‘weak stationarity’)"
  },
  {
    "objectID": "materials/slides/week7-forecasting.html#technical-asides",
    "href": "materials/slides/week7-forecasting.html#technical-asides",
    "title": "Building a forecasting model",
    "section": "Technical asides",
    "text": "Technical asides\nAbout the AR parameters:\n\nconstraints on \\(\\alpha_j\\)’s needed for a well-defined process in infinite time\nestimates \\(\\hat{\\alpha}_j\\) found by:\n\n(moment estimator) solving a recursive system of equations (known as the Yule-Walker equations)\n(mle) maximum likelihood assuming \\(\\epsilon_{t} \\sim N(0, \\sigma^2)\\)"
  },
  {
    "objectID": "materials/slides/week7-forecasting.html#site-specific-model",
    "href": "materials/slides/week7-forecasting.html#site-specific-model",
    "title": "Building a forecasting model",
    "section": "Site-specific model",
    "text": "Site-specific model\nSo let’s revise:\n\\[\n\\begin{aligned}\nY_{i, t} &= f_i (t) + \\epsilon_{i, t} \\quad\\text{(nonlinear regression)} \\\\\n\\epsilon_{i, t} &= \\sum_{d = 1}^D \\alpha_{i,d}\\epsilon_{i, t - d} + \\xi_{i, t} \\quad\\text{(AR(D) errors)}\n\\end{aligned}\n\\]\n\nNote:\n\nseasonal mean \\(f_i(t)\\) is site-dependent (hence subscript)\nAR process is site-dependent (hence \\(\\alpha_{i, d}\\) subscript)\nno elevation included, since it is constant for each site"
  },
  {
    "objectID": "materials/slides/week7-forecasting.html#fit-comparison",
    "href": "materials/slides/week7-forecasting.html#fit-comparison",
    "title": "Building a forecasting model",
    "section": "Fit comparison",
    "text": "Fit comparison\n\n\n\n\nAR fitComparision of estimates\n\n\n\nfit_ar2 <- arima(y_train, \n      order = c(2, 0, 0), \n      xreg = x_train, \n      include.mean = T, \n      method = 'ML')\n\ntidy(fit_ar2) %>% knitr::kable()\n\n\n\n\nterm\nestimate\nstd.error\n\n\n\n\nar1\n1.5389488\n0.0310331\n\n\nar2\n-0.5872016\n0.0310424\n\n\nintercept\n1.7120661\n0.2661139\n\n\nsin1\n-57.4478541\n4.9296869\n\n\ncos1\n-97.6093126\n5.1741646\n\n\nsin2\n9.5264551\n4.9135875\n\n\ncos2\n15.0945042\n5.0136827\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate.ar\nestimate.pooled\n\n\n\n\nar1\n1.5389488\nNA\n\n\nar2\n-0.5872016\nNA\n\n\nintercept\n1.7120661\n1.7428057\n\n\nsin1\n-57.4478541\n-60.4797059\n\n\ncos1\n-97.6093126\n-84.9711125\n\n\nsin2\n9.5264551\n7.0360375\n\n\ncos2\n15.0945042\n15.0271089\n\n\nelev\nNA\n-0.0038422\n\n\nelevsin1\nNA\n0.0094721\n\n\nelevsin2\nNA\n0.0063869\n\n\nelevcos1\nNA\n-0.0627032\n\n\nelevcos2\nNA\n-0.0032467\n\n\nresidlag\nNA\n0.9804500"
  },
  {
    "objectID": "materials/slides/week7-forecasting.html#forecast-comparison",
    "href": "materials/slides/week7-forecasting.html#forecast-comparison",
    "title": "Building a forecasting model",
    "section": "Forecast comparison",
    "text": "Forecast comparison"
  },
  {
    "objectID": "materials/slides/week7-forecasting.html#next-time",
    "href": "materials/slides/week7-forecasting.html#next-time",
    "title": "Building a forecasting model",
    "section": "Next time",
    "text": "Next time\nSpatial prediction…\n\nbased on observations\nbased on forecasts"
  },
  {
    "objectID": "materials/slides/week8-spatial.html#announcementsreminders",
    "href": "materials/slides/week8-spatial.html#announcementsreminders",
    "title": "Spatial prediction",
    "section": "Announcements/reminders",
    "text": "Announcements/reminders\n\nI am away Thursday; no class meeting due to strike\nStudents in the 4pm section should attend Josh or Erika’s section this week\nNext week (Thanksgiving):\n\nwe are meeting Tuesday\nbut there are no Wednesday section meetings on 11/23\n\nYou should start working on your last group assignment before Thanksgiving"
  },
  {
    "objectID": "materials/slides/week8-spatial.html#final-group-assignment",
    "href": "materials/slides/week8-spatial.html#final-group-assignment",
    "title": "Spatial prediction",
    "section": "Final group assignment",
    "text": "Final group assignment\n\ngroups posted [here]\ntask: create a method vignette on a data science topic or theme\n\ngoal: create a reference that you or someone else might use as a starting point next term\ndeliverable: public repository in the pstat197 workspace"
  },
  {
    "objectID": "materials/slides/week8-spatial.html#possible-vignette-topics",
    "href": "materials/slides/week8-spatial.html#possible-vignette-topics",
    "title": "Spatial prediction",
    "section": "Possible vignette topics",
    "text": "Possible vignette topics\n\nclustering methods\nneural net architecture(s) for … [images, text, time series, spatial data]\nconfiguring a database and writing queries in R\nanalysis of network data\nnumerical optimization\nbootstrapping\ngeospatial data structures\nanomaly detection\nfunctional regression"
  },
  {
    "objectID": "materials/slides/week8-spatial.html#outputs",
    "href": "materials/slides/week8-spatial.html#outputs",
    "title": "Spatial prediction",
    "section": "Outputs",
    "text": "Outputs\nYour repository should contain:\n\nA brief .README summarizing repo content and listing the best references on your topic for a user to consult after reviewing your vignette if they wish to learn more\nA primary vignette document that explains methods and walks through implementation line-by-line (similar to an in-class or lab activity)\nAt least one example dataset\nA script containing commented codes appearing in the vignette"
  },
  {
    "objectID": "materials/slides/week8-spatial.html#timeline",
    "href": "materials/slides/week8-spatial.html#timeline",
    "title": "Spatial prediction",
    "section": "Timeline",
    "text": "Timeline\n\nlet me know your topic by end of day Thursday 11/17\nI will confirm by end of day Friday 11/18\nmake a start before Thanksgiving\npresent a draft in class Thursday 12/1\nfinalize repository by Thursday 12/8"
  },
  {
    "objectID": "materials/slides/week8-spatial.html#expectations",
    "href": "materials/slides/week8-spatial.html#expectations",
    "title": "Spatial prediction",
    "section": "Expectations",
    "text": "Expectations\nYou’ll need to yourself learn about the topic and implementation by finding reference materials and code examples.\n\nIt is okay to borrow closely from other vignettes in creating your own, but you should:\n\ncite them\nuse different data\ndo something new\n\n\n\nIt is not okay to make a collage of reference materials by copying verbatim, or simply rewrite an existing vignette.\n\nthe best safeguard against this is to find your own data so you’re forced to translate codes/steps to apply in your particular case\nwe’ll do a brief search and skim your references to ensure sufficient originality"
  },
  {
    "objectID": "materials/slides/week8-spatial.html#from-last-time",
    "href": "materials/slides/week8-spatial.html#from-last-time",
    "title": "Spatial prediction",
    "section": "From last time",
    "text": "From last time\n\n\n\nWe had fit the site-specific model:\n\\[\n\\begin{aligned}\nY_{i, t} &= f_i (t) + \\epsilon_{i, t} \\quad\\text{(nonlinear regression)} \\\\\n\\epsilon_{i, t} &= \\sum_{d = 1}^D \\alpha_{i,d}\\epsilon_{i, t - d} + \\xi_{i, t} \\quad\\text{(AR(D) errors)}\n\\end{aligned}\n\\]\n\nAnd computed forecasts \\(\\hat{Y}_{i, t+ 1} = \\mathbb{E}(Y_{i, t + 1}|Y_{i, t})\\)"
  },
  {
    "objectID": "materials/slides/week8-spatial.html#fitting-and-forecasts-for-one-site",
    "href": "materials/slides/week8-spatial.html#fitting-and-forecasts-for-one-site",
    "title": "Spatial prediction",
    "section": "Fitting and forecasts for one site",
    "text": "Fitting and forecasts for one site\n\nPartitionsFittingForecastingVisualization\n\n\n\n# data partitioning\nsite15 <- soil %>% \n  dplyr::select(-year, -elev) %>%\n  filter(site == soil$site[15]) %>%\n  arrange(date)\n\ntrain <- site15 %>%\n  filter(date < ymd('2018-06-01'))\n\ntest <- site15 %>%\n  filter(date >= ymd('2018-06-01'))\n\ntrain %>% head()\n\n# A tibble: 6 × 7\n  site     day date        temp longitude latitude elevation\n  <chr>  <dbl> <date>     <dbl>     <dbl>    <dbl>     <dbl>\n1 B21K-1   226 2017-08-14  3.82     -155.     69.6        96\n2 B21K-1   227 2017-08-15  3.72     -155.     69.6        96\n3 B21K-1   228 2017-08-16  3.31     -155.     69.6        96\n4 B21K-1   229 2017-08-17  4.95     -155.     69.6        96\n5 B21K-1   230 2017-08-18  5.46     -155.     69.6        96\n6 B21K-1   231 2017-08-19  5.80     -155.     69.6        96\n\n\n\n\n\nx_train <- pull(train, day) %>% \n  fourier(nbasis = 4, period = 365)\ny_train <- pull(train, temp)\n\nfit <- Arima(y_train, \n      order = c(2, 0, 0), \n      xreg = x_train, \n      include.mean = F,\n      method = 'ML')\n\nfit\n\nSeries: y_train \nRegression with ARIMA(2,0,0) errors \n\nCoefficients:\n         ar1      ar2     const      sin1       cos1     sin2     cos2\n      1.4200  -0.5192  -97.0613  -88.1952  -113.2035  -8.7344   5.5794\ns.e.  0.0497   0.0497   11.5069    9.0469    13.2328   9.5149  12.0896\n\nsigma^2 = 0.7837:  log likelihood = -375.26\nAIC=766.52   AICc=767.03   BIC=795.9\n\n\n\n\n\nx_test <- pull(test, day) %>% \n  fourier(nbasis = 4, period = 365)\n\npreds <- forecast(fit, h = nrow(x_test), xreg = x_test)\n\nhead(preds$mean)\n\nTime Series:\nStart = 292 \nEnd = 297 \nFrequency = 1 \n[1] -0.2837326 -0.1614655 -0.0189233  0.1355583  0.2962263  0.4592358"
  },
  {
    "objectID": "materials/slides/week8-spatial.html#now-for-many-sites",
    "href": "materials/slides/week8-spatial.html#now-for-many-sites",
    "title": "Spatial prediction",
    "section": "Now for many sites",
    "text": "Now for many sites\nRemember the functional programming iteration strategy?\n\nFittingFitPredictions\n\n\n\n\n# A tibble: 26 × 5\n   site   train              test               fit        pred      \n   <chr>  <list>             <list>             <list>     <list>    \n 1 C27K-1 <tibble [485 × 3]> <tibble [78 × 3]>  <fr_ARIMA> <forecast>\n 2 F24K-1 <tibble [485 × 3]> <tibble [54 × 3]>  <fr_ARIMA> <forecast>\n 3 G25K-2 <tibble [485 × 3]> <tibble [24 × 3]>  <fr_ARIMA> <forecast>\n 4 G26K-2 <tibble [485 × 3]> <tibble [69 × 3]>  <fr_ARIMA> <forecast>\n 5 G27K-3 <tibble [485 × 3]> <tibble [76 × 3]>  <fr_ARIMA> <forecast>\n 6 M17K-2 <tibble [358 × 3]> <tibble [338 × 3]> <fr_ARIMA> <forecast>\n 7 M18K-5 <tibble [357 × 3]> <tibble [382 × 3]> <fr_ARIMA> <forecast>\n 8 M16K-2 <tibble [354 × 3]> <tibble [323 × 3]> <fr_ARIMA> <forecast>\n 9 N17K-3 <tibble [353 × 3]> <tibble [357 × 3]> <fr_ARIMA> <forecast>\n10 L16K-1 <tibble [352 × 3]> <tibble [386 × 3]> <fr_ARIMA> <forecast>\n# … with 16 more rows"
  },
  {
    "objectID": "materials/slides/week8-spatial.html#spatial-prediction",
    "href": "materials/slides/week8-spatial.html#spatial-prediction",
    "title": "Spatial prediction",
    "section": "Spatial prediction",
    "text": "Spatial prediction\nWe could consider our data to be more explicitly spatial:\n\\[\nY_{i, t} = Y_t(s_i)\n\\qquad\\text{where}\\qquad\ns_i = \\text{location of site }i\n\\]\n\nIn other words, our data at a given time are a realization of a spatial process \\(Y(s)\\) observed at locations \\(s_1, \\dots, s_n\\).\n\n\nCan we predict \\(Y(s_{n + 1})\\) based on \\(Y(s_1), \\dots, Y(s_n)\\)?"
  },
  {
    "objectID": "materials/slides/week8-spatial.html#intuition",
    "href": "materials/slides/week8-spatial.html#intuition",
    "title": "Spatial prediction",
    "section": "Intuition",
    "text": "Intuition\nTobler’s first law of geography:\n\n“everything is related to everything else, but near things are more related than distant things”\n\n\nSo a weighted average of some kind makes sense for spatial prediction\n\\[\n\\hat{Y}(s) = \\sum_i w_i Y(s_i)\n\\]\nwhere the weights \\(w_i\\) are larger for \\(s_i\\) closer to \\(s\\)."
  },
  {
    "objectID": "materials/slides/week8-spatial.html#inverse-distance-weighting",
    "href": "materials/slides/week8-spatial.html#inverse-distance-weighting",
    "title": "Spatial prediction",
    "section": "Inverse distance weighting",
    "text": "Inverse distance weighting\nA simple and fully nonparametric method of spatial prediction is to set \\(w_i \\propto 1/d(s, s_i)\\) where \\(d\\) is a distance measure.\n\nInverse distance weighting does just that, for powers of distance:\n\\[\n\\hat{Y}(s) = \\sum_i c \\times d(s, s_i)^{-p} \\times Y(s_i)\n\\]\nWhere \\(c\\) is the normalizing constant \\(1/\\sum_i d(s, s_i)^{-p}\\)."
  },
  {
    "objectID": "materials/slides/week8-spatial.html#power-parameter",
    "href": "materials/slides/week8-spatial.html#power-parameter",
    "title": "Spatial prediction",
    "section": "Power parameter",
    "text": "Power parameter\n\n\nThe power parameter \\(p\\) controls the rate of weight decay with distance:\n\\[\nw_i \\propto \\frac{1}{d(s, s_i)^p}\n\\]"
  },
  {
    "objectID": "materials/slides/week8-spatial.html#interpolation",
    "href": "materials/slides/week8-spatial.html#interpolation",
    "title": "Spatial prediction",
    "section": "Interpolation",
    "text": "Interpolation\nSpatial interpolation refers to ‘filling in’ values between observed locations.\n\nGenerate a spatial mesh of with centers \\(g_1, g_2, \\dots, g_m\\)\nPredict \\(\\hat{Y}(g_j)\\) for every center \\(g_j\\)\nMake a raster plot\n\n\n\n\n\n\n\n\nMesh\n\n\nFor spatial problems, a mesh is a mutually exclusive partitioning of an area into subregions. Subregions could be regular (e.g., squares, polygons) or irregular (try googling ‘Voronoi tesselation’)."
  },
  {
    "objectID": "materials/slides/week8-spatial.html#map-of-locations",
    "href": "materials/slides/week8-spatial.html#map-of-locations",
    "title": "Spatial prediction",
    "section": "Map of locations",
    "text": "Map of locations\nEarlier, I fit models and generated forecasts for 26 sites chosen largely based on having overlapping observation windows."
  },
  {
    "objectID": "materials/slides/week8-spatial.html#forecasts",
    "href": "materials/slides/week8-spatial.html#forecasts",
    "title": "Spatial prediction",
    "section": "Forecasts",
    "text": "Forecasts\nI also truncated the training data to stop on the same date (April 30, 2018). So we can plot point forecasts for May 1."
  },
  {
    "objectID": "materials/slides/week8-spatial.html#interpolations-using-idw",
    "href": "materials/slides/week8-spatial.html#interpolations-using-idw",
    "title": "Spatial prediction",
    "section": "Interpolations using IDW",
    "text": "Interpolations using IDW\nSo interpolating between forecasts yields spatial forecasts."
  },
  {
    "objectID": "materials/slides/week8-spatial.html#effect-of-idw-parameter-p",
    "href": "materials/slides/week8-spatial.html#effect-of-idw-parameter-p",
    "title": "Spatial prediction",
    "section": "Effect of IDW parameter \\(p\\)",
    "text": "Effect of IDW parameter \\(p\\)\nThe power parameter \\(p\\) controls the rate of decay of interpolation weight \\(w_i\\) with distance."
  },
  {
    "objectID": "materials/slides/week8-spatial.html#considerations",
    "href": "materials/slides/week8-spatial.html#considerations",
    "title": "Spatial prediction",
    "section": "Considerations",
    "text": "Considerations\n\nChoosing \\(p\\) can be done based on optimizing predictions or by hand.\nUncertainty quantification?\n\nusually, could use variance of weighted average\nbut also tricky in this case because we are interpolating forecasts, which themselves have some associated uncertainty"
  },
  {
    "objectID": "materials/slides/week9-claimsresults.html#todays-agenda",
    "href": "materials/slides/week9-claimsresults.html#todays-agenda",
    "title": "Results from claims assignment",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nReview results of claims assignment\nDiscuss vignette guidelines\nRemaining time to organize in groups"
  },
  {
    "objectID": "materials/slides/week9-claimsresults.html#preface",
    "href": "materials/slides/week9-claimsresults.html#preface",
    "title": "Results from claims assignment",
    "section": "Preface",
    "text": "Preface\nMy goal here is to use the results of your work on the last assignment to learn as a group what worked well and what didn’t.\n\nAs a disclaimer, many groups did not successfully generate predictions according to instructions. This is okay.\n\n\nMy goal is not to judge anyone based on results or having completed the assignment."
  },
  {
    "objectID": "materials/slides/week9-claimsresults.html#accessing-claims-predictions",
    "href": "materials/slides/week9-claimsresults.html#accessing-claims-predictions",
    "title": "Results from claims assignment",
    "section": "Accessing claims predictions",
    "text": "Accessing claims predictions\nResults from the last assignment are now available online.\n\nlibrary(tidyverse)\n\ngithub_url <- \"https://github.com/pstat197/pstat197a/raw/main/materials/slides/data/f22-claims-evals.RData\"\n\nload(url(github_url))\n\nevals %>% head()\n\n# A tibble: 6 × 2\n  group eval            \n  <int> <list>          \n1     1 <smplErrr>      \n2     2 <tibble [6 × 5]>\n3     3 <tibble [6 × 5]>\n4     4 <rlng_rrr>      \n5     5 <tibble [6 × 5]>\n6     6 <rlng_rrr>"
  },
  {
    "objectID": "materials/slides/week9-claimsresults.html#checking-errors-example",
    "href": "materials/slides/week9-claimsresults.html#checking-errors-example",
    "title": "Results from claims assignment",
    "section": "Checking errors: example",
    "text": "Checking errors: example\n\nevals %>%\n  filter(group == 4) %>%\n  pull(eval)\n\n[[1]]\n<error/rlang_error>\nError in `metric_set()`:\n! Failed to compute `sensitivity()`.\nCaused by error in `dplyr::summarise()`:\n! Problem while computing `.estimate = metric_fn(...)`.\n---\nBacktrace:\n  1. tibble(group = 1:15) %>% ...\n 15. yardstick (local) panel(., truth = mclass, estimate = mclass.pred, estimator = \"macro\")\n 16. base::mapply(...)\n 17. yardstick (local) `<fn>`(dots[[1L]][[1L]], dots[[2L]][[1L]])\nCaused by error in `multiclass_checks()`:\n! `truth` and `estimate` levels must be equivalent.\n`truth`: N/A: No relevant content., Physical Activity, Possible Fatality, Potentially unlawful activity, Other claim content\n`estimate`: N/A: No relevant content., Other claim content, Physical Activity, Possible Fatality, Potentially unlawful activity\n---\nBacktrace:\n  1. tibble(group = 1:15) %>% ...\n 34. yardstick (local) metric_fn(...)\n 35. yardstick::metric_vec_template(...)\n 36. yardstick:::validate_truth_estimate_checks(...)\n 38. yardstick:::validate_truth_estimate_types.factor(...)\n 40. yardstick:::multiclass_checks.factor(truth, estimate)"
  },
  {
    "objectID": "materials/slides/week9-claimsresults.html#accuracies",
    "href": "materials/slides/week9-claimsresults.html#accuracies",
    "title": "Results from claims assignment",
    "section": "Accuracies",
    "text": "Accuracies\n\naccuracies <- evals %>%\n  rowwise() %>%\n  filter(is_tibble(eval)) %>%\n  ungroup() %>%\n  unnest(eval)\n\naccuracies %>% head(8) %>% knitr::kable()\n\n\n\n\ngroup\n.metric\n.estimator\n.estimate\nclass\nn\n\n\n\n\n2\nsensitivity\nmacro\n0.8081203\nmulticlass\n99\n\n\n2\nspecificity\nmacro\n0.9370310\nmulticlass\n99\n\n\n2\naccuracy\nmulticlass\n0.8181818\nmulticlass\n99\n\n\n2\nsensitivity\nbinary\n0.8431373\nbinary\n99\n\n\n2\nspecificity\nbinary\n0.8750000\nbinary\n99\n\n\n2\naccuracy\nbinary\n0.8585859\nbinary\n99\n\n\n3\nsensitivity\nmacro\n0.2058975\nmulticlass\n828\n\n\n3\nspecificity\nmacro\n0.8035735\nmulticlass\n828"
  },
  {
    "objectID": "materials/slides/week9-claimsresults.html#comparisons",
    "href": "materials/slides/week9-claimsresults.html#comparisons",
    "title": "Results from claims assignment",
    "section": "Comparisons",
    "text": "Comparisons"
  },
  {
    "objectID": "materials/slides/week9-claimsresults.html#vignette-guidelines",
    "href": "materials/slides/week9-claimsresults.html#vignette-guidelines",
    "title": "Results from claims assignment",
    "section": "Vignette guidelines",
    "text": "Vignette guidelines\nLet’s review as a group."
  },
  {
    "objectID": "materials/vignette-guidelines.html",
    "href": "materials/vignette-guidelines.html",
    "title": "Vignette Guidelines",
    "section": "",
    "text": "A vignette is a simple example intended to help learn a method or tool.\nThe overarching goal of creating vignettes is to provide starting points for learning about specialized topics in data science that students in the class can later consult to familiarize themselves with an unfamiliar topic during their project work.\nThis document sets expectations for the organization and content of vignette repositories."
  },
  {
    "objectID": "materials/vignette-guidelines.html#configuring-your-repository",
    "href": "materials/vignette-guidelines.html#configuring-your-repository",
    "title": "Vignette Guidelines",
    "section": "Configuring your repository",
    "text": "Configuring your repository\nCreate a public repository in the PSTAT197 workspace and add your teammates as collaborators.\nSelect options at the creation step to initialize the repository with:\n\na .README file\na .gitignore file\na license\n\nGive the repository a descriptive name. Use the naming convention\n\nvignette-[keyword]\n\nfor example, “vignette-lstm”, “vignette-kriging”, “vignette-cnn”, and the like. A single keyword is best if possible, but consider using two or three if needed to make your repo name sufficiently specific, e.g., “vignette-database-configuration” or “vignette-distribution-based-clustering”. Do not use more than three keywords in your repository name.\nSupply an optional description that contains a long title for your vignette topic, for instance:\n\nDistribution-based clustering in R and application to unsupervised cell type classification\n\nLastly, once the repository is created, add a few topics to the “About” section on the far right of your repository homepage."
  },
  {
    "objectID": "materials/vignette-guidelines.html#directory-organization",
    "href": "materials/vignette-guidelines.html#directory-organization",
    "title": "Vignette Guidelines",
    "section": "Directory organization",
    "text": "Directory organization\nAs a general guideline, all files except the README should be placed in appropriately-named subdirectories so that your repository homepage is free from file clutter.\nIf your project has a single main file – in this case the vignette document – it is reasonable to place that in the root directory with the README. Everything else should go in a subfolder.\nThe high-level directories should clearly differentiate the main project contents, and overall there should not be too many levels of subdirectory, especially for a simple project like a code vignette. Your directory structure might look something like this in the end:\nroot directory\n|-- data\n    |-- raw\n    |-- processed\n|-- scripts\n    |-- drafts\n    |-- vignette-script.R\n|-- img\n    |-- fig1.png\n    |-- fig2.png\n|-- vignette.qmd\n|-- vignette.html\n|-- README.md\nAs a guiding principle, each subdirectory should contain either\n\n(a)a few primary files and one or more subdirectories\nscripts\n|-- functions\n|-- drafts\n|-- exploratory-analysis.R\n|-- model-fitting.R\n|-- visualizations.R\n(b) a single file type with an obvious naming convention\nimg\n|-- fig-autocorrelation.png\n|-- fig-forecasts.png\n|-- fig-rawseries.png\n|-- logo-ucsb.png\n\nTry to organize your repository so that it is easy to navigate for the general coding public (and for your future self)."
  },
  {
    "objectID": "materials/vignette-guidelines.html#readme-contents",
    "href": "materials/vignette-guidelines.html#readme-contents",
    "title": "Vignette Guidelines",
    "section": "README contents",
    "text": "README contents\nYour README file should contain five main pieces of information in the following order:\n\nA one-sentence description at the very top before any (sub)headers:\n\nVignette on implementing distribution-based clustering using cell type data; created as a class project for PSTAT197A in Fall 2022.\n\nContributors\nVignette abstract: a brief description in a few sentences of your vignette topic, example data, and outcomes.\nRepository contents: an explanation of the directory structure of the repository\nReference list: 2 or more references to learn more about your topic.\n\nA typical README file would also contain instructions on use and instructions on contributing to the repository."
  },
  {
    "objectID": "materials/vignette-guidelines.html#repository-contents",
    "href": "materials/vignette-guidelines.html#repository-contents",
    "title": "Vignette Guidelines",
    "section": "Repository contents",
    "text": "Repository contents\nYour repository should contain at minimum the following:\n\nan example dataset with which you illustrate the use of the method(s) or tool(s) of your topic\na primary vignette document – either a notebook or rendered markdown file – that teaches your method(s) and/or tool(s). this document should integrate codes with step-by-step explanation and read much like a lab activity\na script with line annoations that replicates all results shown in the primary vignette document end-to-end"
  },
  {
    "objectID": "materials/vignette-guidelines.html#evaluation",
    "href": "materials/vignette-guidelines.html#evaluation",
    "title": "Vignette Guidelines",
    "section": "Evaluation",
    "text": "Evaluation\nYour work will be evaluated on:\n\nhow well the repository and contents conform to the expectations outlined above\nthe clarity of the vignette, from the perspective of another student in the class\nthe correctness of the data analysis and any other technical aspects of the vignette"
  },
  {
    "objectID": "materials/vignette-guidelines.html#deadlines",
    "href": "materials/vignette-guidelines.html#deadlines",
    "title": "Vignette Guidelines",
    "section": "Deadlines",
    "text": "Deadlines\nThere are two deadlines associated with this project, a draft deadline and a final deadline.\n\ndraft deadline Thursday, December 1, 2pm PST (in class) – be prepared to share a draft of your primary vignette document and explain it to a small group of other students\nfinal deadline Thursday, December 8, 11:59pm PST – the repository and all contained files should be in final form ready for evaluation by course staff"
  },
  {
    "objectID": "materials/slides/week10-projects.html#announcementsreminders",
    "href": "materials/slides/week10-projects.html#announcementsreminders",
    "title": "Capstone projects",
    "section": "Announcements/reminders",
    "text": "Announcements/reminders\n\ncome to class next time prepared to show a draft of your vignette\nafter class today look for abstracts on the course site\noffice hours in place of section meetings this Wednesday"
  },
  {
    "objectID": "materials/slides/week10-projects.html#amgen",
    "href": "materials/slides/week10-projects.html#amgen",
    "title": "Capstone projects",
    "section": "Amgen",
    "text": "Amgen\nAmgen is an international biotechnology company.\n\nProject: evaluation of natural language processing algorithms used for knowledge graph generation\n\nNLP algorithms are used to recognize and link entities and identify relations based on text data\noutputs can be used to extract networks (‘knowledge graphs’) from text corpora\n\n\n\nGoals: generate a benchmarking dataset from PubMed database and evaluate performance of NLP-based methods for knowledge graph construction\n\n\nSee this example of related work."
  },
  {
    "objectID": "materials/slides/week10-projects.html#appfolio",
    "href": "materials/slides/week10-projects.html#appfolio",
    "title": "Capstone projects",
    "section": "Appfolio",
    "text": "Appfolio\nAppfolio is a local property management software company. Among other things, clients use their software for accounting purposes.\n\nProject: anomaly detection from property management transaction histories\n\nclients would like to flag smaller/higher transactions than typical without having to inspect full transaction records each month\n\n\n\nGoals: determine applicable anomaly/outlier detection methods, evaluate performance, and develop dashboard based on best method(s)"
  },
  {
    "objectID": "materials/slides/week10-projects.html#calcofi",
    "href": "materials/slides/week10-projects.html#calcofi",
    "title": "Capstone projects",
    "section": "CalCOFI",
    "text": "CalCOFI\nCalCOFI stands for California Cooperative Oceanic Fisheries Investigations. They run a long-term monitoring program of the California current ecosystem.\n\nProject: an eDNA window into larval fish habitat, ecosystem structure, and function\n\nCalCOFI collects physical data and environmental DNA (eDNA) across depth in the water column at multiple monitoring sites longitudinally\na major interest is on impacts of environmental conditions on fisheries\n\n\n\nGoals: develop dashboard for exploration of eDNA data and develop a model for prediction of fish larvae based on eDNA and physical data (or derived variables)\n\n\nSee last year’s project."
  },
  {
    "objectID": "materials/slides/week10-projects.html#carpe-data",
    "href": "materials/slides/week10-projects.html#carpe-data",
    "title": "Capstone projects",
    "section": "Carpe Data",
    "text": "Carpe Data\nCarpe Data is a local company focusing on data-driven solutions for insurance carriers.\n\nProject: business characteristics classification models\n\ninsurance carriers use risk categorization as a factor in determining premiums for property loss and general liability insurance for businesses\n\n\n\nGoals: classify businesses according to characteristics and/or risk levels based on basic business information (name, description, hours, images, etc.) and develop software pipelines for preprocessing and prediction"
  },
  {
    "objectID": "materials/slides/week10-projects.html#caves-visual-ecology-lab",
    "href": "materials/slides/week10-projects.html#caves-visual-ecology-lab",
    "title": "Capstone projects",
    "section": "Caves visual ecology lab",
    "text": "Caves visual ecology lab\n\n\n\n\n\nExample closeup of bee eye.\n\n\n\nThe Caves lab studies visual acuity and its evolutionary and ecological drivers in animals. Bees are great model organisms for studying the relationship between ecology and acuity due to wide variation in lifestyles and ecologies.\n\nProject: measuring visual acuity in bees from high-resolution images\n\nacuity quantification is derived from physical measurements that could potentially be inferred from photographs rather than measured directly\nspecifically, radius of curvature of the eye and width of ommatidia (compound eye facets)\n\nGoals: utilize computer vision techniques to infer acuity measurements from photographs and merge with ecological data to explore correlates"
  },
  {
    "objectID": "materials/slides/week10-projects.html#ccber",
    "href": "materials/slides/week10-projects.html#ccber",
    "title": "Capstone projects",
    "section": "CCBER",
    "text": "CCBER\nThe Cheadle Center for Biodiversity and Ecological Restoration (CCBER) contributes to the Big Bee Project, aimed at creating >1M 2D and 3D high-resolution images of bees for the study of anatomical variation.\n\nProject: constructing three-dimensional bee models from high-resolution images\n\nseveral software tools for constructing 3D models from 2D images are available, but performance with bees specifically is not well understood\nseveral image sets of ~100 photographs each are available for construction of models\n\n\n\nGoals: after receiving training on 3D modeling tools, students will experiment with parameter tuning for optimal rendering of bee models; students will then derive physical measurements from the models."
  },
  {
    "objectID": "materials/slides/week10-projects.html#climate-hazards-center",
    "href": "materials/slides/week10-projects.html#climate-hazards-center",
    "title": "Capstone projects",
    "section": "Climate Hazards Center",
    "text": "Climate Hazards Center\nThe Climate Hazards Center housed in the geography department is a multidisciplinary research center focusing on climate risk analysis and response.\n\nProject 1: identifying the drivers of food insecurity in the developing world\n\nprecipitation and potential evapotranspiration together can help identify deficits in available water and risk of food shortages\nexplore the relationship between precipitation and potential evapotranspiration globally over time and identify drivers after accounting for relationship\n\n\n\nProject 2: evaluating and validating station- and satellite-based daily precipitation datasets\n\nCHC supports precipitation datasets based on interpolating measurements from satellites and ground stations\nnew data releases go through an evaluation/validation process with benchmarking data prior to release\nstudents will carry out evaluation/validation of a new release based on prior strategies"
  },
  {
    "objectID": "materials/slides/week10-projects.html#eembpatrick-green",
    "href": "materials/slides/week10-projects.html#eembpatrick-green",
    "title": "Capstone projects",
    "section": "EEMB/Patrick Green",
    "text": "EEMB/Patrick Green\n\n\n\n\nPatrick Green is a research scientist in EEMB studying animal behavior and competition, and is developing a pilot project studying contests among mantis shrimp.\n\nProject: how do mantis shrimp fight in a community of competitors?\n\nfeasible to carry out continuous video monitoring of small populations in tanks to capture interactions and other behavior\ntime-consuming to review footage\nnot obvious how to define/code interactions\n\nGoal: develop heuristic methodology for detecting time intervals in which interactions occur; generate tracking data summarizing movements of each individual."
  },
  {
    "objectID": "materials/slides/week10-projects.html#evidation-health",
    "href": "materials/slides/week10-projects.html#evidation-health",
    "title": "Capstone projects",
    "section": "Evidation Health",
    "text": "Evidation Health\nEvidation is a California-based company focusing on health data analytics for individuals and for researchers.\n\nProject: Impact of case definition on early detection systems for COVID-19\n\nmodel-based early detection systems for infectious disease use inconsistent criteria for case onset\nthe definition of when a case starts may impact the efficacy and other features of early detection systems\n\n\n\nGoals: assess the impact of case onset definition on existing early detection models and find optimal case onset points."
  },
  {
    "objectID": "materials/slides/week10-projects.html#inogen",
    "href": "materials/slides/week10-projects.html#inogen",
    "title": "Capstone projects",
    "section": "Inogen",
    "text": "Inogen\nInogen is a medical device company that builds portable oxygen concentrators.\n\nProject: analysis of portable oxygen concentrator patient use data\n\nInogen collects a variety of data on patient use of their POC devices and is interested in identifying areas of potential improvement\n\n\n\nGoals: identify patterns of device use from patient data with particular focus on adherence."
  },
  {
    "objectID": "materials/slides/week10-projects.html#move-lab",
    "href": "materials/slides/week10-projects.html#move-lab",
    "title": "Capstone projects",
    "section": "MOVE lab",
    "text": "MOVE lab\nThe MOVE lab at UCSB focuses on movement data science in general and in particular human mobility in response to disruptions.\n\nProject: detecting changes in human mobility and movement patterns associated with wildfires in California\n\nmovement and mobility are often markers of behavior and changes in movement patterns may capture information about behavioral responses to events\nnatural disaster in general and wildfire in particular are likely to produce shifts in movement patterns\n\n\n\nGoals: assess suitability of several candidate datasets for studying behavioral responses to wildfires; identify movement patterns and explore the hypothesis that change points occur in connection with wildfire events; potentially explore demographic covariates."
  },
  {
    "objectID": "materials/slides/week10-projects.html#peak-performance-project-p3",
    "href": "materials/slides/week10-projects.html#peak-performance-project-p3",
    "title": "Capstone projects",
    "section": "Peak Performance Project (P3)",
    "text": "Peak Performance Project (P3)\nP3 is a local company focusing on applied sports science and technology for biomechanical analysis of athletic performance.\n\nProject: understanding links between biomechanical data and on-court NBA production\n\nP3 collects biomechanical data – force plate and motion capture – on professional athletes and maintains a large proprietary database on NBA athletes\nhistorically, have focused on injury risk, but interested in finding biomechanical correlates of real performance\n\n\n\nGoals: scrape publicly available on-court data, merge with biomechanical data, and identify correlates of on-court production; develop visualization tools."
  },
  {
    "objectID": "materials/slides/week10-projects.html#slac-national-accelerator-lab",
    "href": "materials/slides/week10-projects.html#slac-national-accelerator-lab",
    "title": "Capstone projects",
    "section": "SLAC National Accelerator Lab",
    "text": "SLAC National Accelerator Lab\nStanford Synchrotron Ratiation Lightsource (SSRL) is a DOE facility at Stanford supporting a wide range of fundamental research involving bright X-rays.\n\nProject: diffraction image selector\n\nX-ray diffraction data provides insight into the atomic and molecular structure of crystals; serial crystallography involves merging diffraction patterns from several crystals in order to analyze the material structure\nexperimenters tend to select diffraction images manually from serial experiments; this selection has an impact on experimental outputs\n\n\n\nGoals: build a regression model to label images during data collection in real time; develop model from simulated data and validate on real data."
  },
  {
    "objectID": "materials/slides/week10-projects.html#project-preferences",
    "href": "materials/slides/week10-projects.html#project-preferences",
    "title": "Capstone projects",
    "section": "Project Preferences",
    "text": "Project Preferences\nPlease read abstracts first and then fill out the preference form by Friday 12/2.\n\n3 top lab choices\n3 top industry choices\n\n\nWe’ll try to accommodate preferences, but we can’t guarantee you’ll get your top choices.\n\n\nTarget date for assignments: Friday 12/9."
  },
  {
    "objectID": "about/links.html#sec-class-forms",
    "href": "about/links.html#sec-class-forms",
    "title": "Links",
    "section": "",
    "text": "Lecture Attendance reporting form; fill out once per class meeting.\nSection Attendance reporting form\nCapstone project intake form; fill out by October 6."
  },
  {
    "objectID": "materials/course-materials.html#introductory-module",
    "href": "materials/course-materials.html#introductory-module",
    "title": "Course materials",
    "section": "",
    "text": "Objectives: set expectations; explore data science raison d’etre; introduce systems and design thinking; introduce software tools and collaborative coding; conduct exploratory/descriptive analysis of class background and interests.\n\n\n\nWednesday meeting: Course orientation [slides]\nAssignments due by next class meeting:\n\ninstall course software and create github account;\nfill out intake form\nread Peng and Parker (2022);\nprepare a reading reading response\n\n\n\n\n\n\nMonday meeting: On projects in(volving) data science [slides]\nSection meeting: software and technology overview [activity] Teams spreadsheet\nWednesday meeting: basic GitHub actions [activity] [slides]\nAssignments due by next class meeting:\n\nread MDSR 9.1 and 9.2\nprepare a reading response\n\n\n\n\n\n\nMonday meeting: Introducing class intake survey data [slides]\nSection meeting: tidyverse basics [activity]\nWednesday meeting: planning group work for analysis of survey data [slides]\nAssignments:\n\nfirst team assignment due Wednesday, October 26, 11:59 PM PST [accept via GH classroom here]Teams spreadsheet"
  },
  {
    "objectID": "about/technology.html#sec-software",
    "href": "about/technology.html#sec-software",
    "title": "Technology",
    "section": "",
    "text": "Computing in PSTAT197A will be shown in R, and codes and other materials will be shared via GitHub. The following software will be required to access course materials:\n\nR\nRStudio\nGit\nGitHub Desktop (or another visual GitHub client)\n\nInstallations and basic functionality will be covered in the first section meeting.\nWhile PSTAT197A is not language-agnostic and some instruction in R is provided, it is also not a course especially emphasizing programming technique in R. Students are free to use or experiment with other software at their discretion provided it does not interfere with their participation in the class, but are expected to submit work and collaborate using RStudio-supported files."
  },
  {
    "objectID": "materials/slides/week2-classdata.html#courses",
    "href": "materials/slides/week2-classdata.html#courses",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Courses",
    "text": "Courses"
  }
]