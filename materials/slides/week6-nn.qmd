---
title: "Artificial Neural Networks"
subtitle: "PSTAT197A/CMPSC190DD Fall 2023"
institute: 'UCSB'
bibliography: refs.bib
format: 
  revealjs:
    incremental: true
    # footer: 'PSTAT197A/CMPSC190DD Fall 2023'
    # logo: 'img/ucsbds_hex.png'
    fig-width: 6
    fig-height: 4
    fig-align: 'left'
    slide-number: true
execute:
  message: false
  warning: false
  echo: false
  cache: true
---

## Announcements/reminders

*Before your section meetings this week:*

-   [install python](https://www.python.org/downloads/)


. . .

*Part of your assignment this week:* fill out midquarter [self-evaluation](https://forms.gle/HXsZSKhbcC52EKRe6).

. . .



# Neural Networks

## Discuss with your table (3min)

What do you know (or have you heard) about neural networks?

-   what are they

-   what are they used for

-   any other info?

## Graphical model diagram

Consider an arbitrary statistical model with one response $Y$ and three predictors $x_1, x_2, x_3$.

. . .

A simple diagram of the model would look like this:

```{dot}
//| fig-width: 6
//| fig-height: 4
digraph G {
    layout = dot
    rankdir = LR
    splines = false
    edge [arrowsize = 0.5, color = coral4]
    
  subgraph cluster_0 {
   color = white
    node [color=white, style = filled, fillcolor = aquamarine3, shape = circle];
    
    in1 [label = "x1"]
    in2 [label = "x2"]
    in3 [label = "x3"]
    
    label = "predictors";
  }
  
  
  subgraph cluster_2 {
   color = white
    node [color=white, style = filled, fillcolor = bisque1, shape = circle];
    
    o1 [label = "Y"]

    label = "response";
  }
  
  {in1 in2 in3} -> o1

  
}
```

## Graph layers

A model that maps predictors directly to the response has just two ***"layers"***:

-   an input layer $X$

-   an output layer $Y$ (or more accurately $\mathbb{E}Y$)

. . .

Neural networks add layers between the input and output.

## 'Vanilla' neural network

::: columns
::: {.column width="60%"}
```{dot}
//| fig-width: 8
//| fig-height: 5
digraph G {
    layout = dot
    rankdir = LR
    constraint = false
    splines = false
    edge [arrowsize = 0.5, color = coral4]
    
  subgraph cluster_0 {
   color = white
    node [color=white, style = filled, fillcolor = aquamarine3, shape = circle];
    
    in1 [label = " "]
    in2 [label = " "]
    in3 [label = " "]
    
    label = " ";
  }
  
  subgraph cluster_1 {
   color = white
    node [color=white, style = filled, fillcolor = cadetblue2, shape = circle];
    
    h1 [label = " "]
    h2 [label = " "]
    h3 [label = " "]
    h4 [label = " "]
   
    label = " ";
  }
  
  subgraph cluster_2 {
   color = white
    node [color=white, style = filled, fillcolor = bisque1, shape = circle];
    
    o1 [label = " "]

    label = " ";
  }
  
  {in1 in2 in3} -> {h1 h2 h3 h4} -> o1

  
}
```
:::

::: {.column width="40%"}
-   one input layer

-   one hidden layer

-   one output layer

-   one parameter per edge
:::
:::

## More formally

Let $Y\in\mathbb{R}^n$ and $X \in \mathbb{R}^{n\times p}$ represent some data. The **vanilla neural network** is:

```{=tex}
\begin{aligned}
\color{#eed5b7}{\mathbb{E}Y} &= \sigma_z(\color{#7ac5cd}{Z}\color{#8b3e2f}{\beta}) 
  \qquad &\text{output layer}\\

\color{#7ac5cd}{Z} &=
  \left[\sigma_x(\color{#66cdaa}{X}\color{#8b3e2f}{\alpha_1}) 
    \;\cdots\; 
    \sigma_x(\color{#66cdaa}{X}\color{#8b3e2f}{\alpha_M})\right] 
  \qquad &\text{hidden layer} \\
  
\color{#66cdaa}{X} &= \left[x_1 \;\cdots\; x_p\right] 
  \qquad&\text{input layer}
\end{aligned}
```
-   $\sigma_x, \sigma_z$ are (known) *activation* functions
-   $\color{#8b3e2f}{\beta}, \color{#8b3e2f}{\alpha}$ are *weights* (model parameters)
    -   $p(M + 1)$ of them as written

## Training a network

Notice that the output is simply a long composition:

$$
Y = f(X) \quad\text{where}\quad f \equiv \sigma_z \circ h_\beta\circ \sigma_x \circ h_\alpha
$$

-   each function is either known or linear

-   compute parameters by minimizing a loss function

-   minimization by gradient descent

## Gradient descent

Denoting the parameter vector by $\theta = \left(\alpha^T \; \beta^T\right)$, initialize $\theta^{(0)}$ and repeat:

$$
\theta^{(r + 1)} \longleftarrow \theta^{(r)} + c_r \nabla L^{(r)}
$$

-   $L^{(r)}$ is a loss function evaluated at the $r$th iteration

    -   of the form $L^{(r)} = \frac{1}{n}\sum_i L_i (\theta^{(r)}, Y)$

-   $c_r$ is the 'learning rate'; can be fixed or chosen adaptively

-   each cycle through all the parameters is one 'epoch'

## Updates for the VNN

Individual parameter updates at the $r$th iteration are given by:

$$
\beta_{m}^{(r + 1)} \longleftarrow 
    \beta_{m}^{(r)} + c_r \underbrace{\frac{1}{n}\sum_{i = 1}^n \frac{\partial L_i}{\partial \beta_{m}}\Big\rvert_{\beta_{m} = \beta_{m}^{(r)}}}_{\text{gradient at current iteration}} \\
    \alpha_{mp}^{(r + 1)} \longleftarrow 
    \alpha_{mp}^{(r)} + c_r \underbrace{\frac{1}{n}\sum_{i = 1}^n \frac{\partial L_i}{\partial \alpha_{mp}}\Big\rvert_{\alpha_{mp} = \alpha_{mp}^{(r)}}}_{\text{gradient at current iteration}}
$$

## Chain rule

The gradient is easy to compute. Denoting $t_{i} = z_{i}^T\beta$:

$$
\begin{aligned}
\frac{\partial L_i}{\partial \alpha_{mp}}
    &= \underbrace{\frac{\partial L_i}{\partial f}
        \frac{\partial f}{\partial t_i}}_{\delta_i} 
        \underbrace{\frac{\partial t_i}{\partial z_{im}}
        \frac{\partial z_{im}}{\partial \alpha_{mp}}}_{s_{im}x_{ip}} \\
\frac{\partial L_i}{\partial \beta_{m}}
    &= \underbrace{\frac{\partial L_i}{\partial f} 
        \frac{\partial f}{\partial t_i}}_{\delta_i} 
        \underbrace{\frac{\partial t_i}{\partial \beta_{m}}}_{z_{im}}
\end{aligned}
$$

. . .

Explicitly computing gradients for each update gives the *backpropagation* algorithm of @rumelhart1986learning .

## Backpropagation

Initialize parameters and repeat:

1.  **Forward pass**: compute $f(X), Z$

2.  **Backward pass**: compute $\delta_i, s_{mi}$ by 'back-propagating' current estimates

3.  Update the weights\
    $$
    \hat{\beta}_{km} \longleftarrow 
      \hat{\beta}_{km} + c_r \frac{1}{n}\sum_i \delta_{ki}z_{mi} \\
      \hat{\alpha}_{mp} \longleftarrow 
      \hat{\alpha}_{mp} + c_r \frac{1}{n}\sum_i s_{mi}x_{ip}
    $$

## Gradient estimation

Explicitly computing the gradient sums over all observations $i = 1, \dots, n$:

$$
g = \nabla \frac{1}{n} \sum_i L_i
$$

. . .

It's much faster to *estimate* the gradient based on a "batch" of $m$ observations (subsample) $J \subset \{1, \dots, n\}$:

$$
\hat{g} = \nabla \frac{1}{m}\sum_{i \in J} L_i
$$

## Modern optimization methods

Modern methods for training neural networks update parameters using gradient estimates and adaptive learning rates.[^1]

[^1]: A nice overview is given in @Goodfellow-et-al-2016 .

-   stochastic gradient descent (SGD): @bottou1998online replace $g$ by $\hat{g}$
-   AdaGrad: @duchi2011adaptive use SGD with adaptive learning rates
-   Adam: @kingma2014adam apply bias corrections to $\hat{g}$ based on moment estimates

## Increasing width

```{dot}
//| fig-width: 8
//| fig-height: 6
//| fig-cap: One more hidden unit.
digraph G {
    layout = dot
    rankdir = LR
    splines = false
    edge [arrowsize = 0.5, color = coral4]
    
  subgraph cluster_0 {
   color = white
    node [color=white, style = filled, fillcolor = aquamarine3, shape = circle];
    
    in1 [label = " "]
    in2 [label = " "]
    in3 [label = " "]
    
    
    label = " ";
  }
  
  subgraph cluster_1 {
   color = white
    node [color=white, style = filled, fillcolor = cadetblue2, shape = circle];
    
    h1 [label = " "]
    h2 [label = " "]
    h3 [label = " "]
    h4 [label = " "]
    h5 [label = " "]   

    label = " ";
  }
  
  subgraph cluster_2 {
   color = white
    node [color=white, style = filled, fillcolor = bisque1, shape = circle];
    
    o1 [label = " "]
    
    label = " ";
  }
  
  
  {in1 in2 in3} -> {h1 h2 h3 h4 h5} -> o1
  
  subgraph axv {
      rank = "max"
      node [color = white]
      edge [color = black]
      
      a [label = " "]
      b [label = "width"]
      c [label = " "]
      
      a -> b [dir = "back"] 
      b -> c
  }
  
  o1 -> {a b c} [color = white]
  

}
```

## Increasing depth

```{dot}
//| fig-width: 8
//| fig-height: 6
//| fig-cap: One more hidden layer.
digraph G {
    layout = dot
    rankdir = LR
    splines = false
    constraint = false
    edge [arrowsize = 0.5, color = coral4]
    
  subgraph cluster_0 {
   color = white
    node [color=white, style = filled, fillcolor = aquamarine3, shape = circle];
    
    in1 [label = " "]
    in2 [label = " "]
    in3 [label = " "]
    
    
    label = " ";
  }
  
  subgraph cluster_1 {
   color = white
    node [color=white, style = filled, fillcolor = cadetblue2, shape = circle];
    
    h1 [label = " "]
    h2 [label = " "]
    h3 [label = " "]
    h4 [label = " "]
    h5 [label = " "]
   
    label = " ";
  }
  
  subgraph cluster_2 {
   color = white
    node [color=white, style = filled, fillcolor = cadetblue3, shape = circle];
    
    z1 [label = " "]
    z2 [label = " "]
    z3 [label = " "]
    z4 [label = " "]
   
    label = " ";
  }
  
  subgraph cluster_3 {
   color = white
    node [color=white, style = filled, fillcolor = bisque1, shape = circle];
    
    o1 [label = " "]
    
    label = " ";
  }
  
  
  {in1 in2 in3} -> {h1 h2 h3 h4 h5} -> {z1 z2 z3 z4} -> o1
  
  subgraph axh {
      node [color = white]
      edge [color = black]
      
      a [label = " "]
      b [label = "depth"]
      c [label = " "]
      
      a -> b [dir = "back"] 
      b -> c
  }
  


}
```

## Sequential networks

Networks of arbitrary width and depth in which the connectivity is uni-directional are known as "sequential" or "feedforward" networks/models.

$$
\begin{aligned}
\mathbb{E}Y &= \sigma_1(Z_1\beta_1) &\text{output layer}\\
Z_k &= \sigma_k(Z_{k - 1} \beta_k) &\text{hidden layers } k = 2, \dots, D - 1\\
Z_D &\equiv X &\text{input layer}
\end{aligned}
$$

-   chain rule calculations get longer but are otherwise the same
-   "universal approximation" properties

## Approximation properties

Suppose:

-   $\mathbb{E}Y = f(X)$ gives the 'true' relationship

-   $\tilde{f}(X)$ represents the output layer of a feedforward neural network with one hidden layer of width $w$

. . .

@hornik1989multilayer showed that, under some regularity conditions, for any $\epsilon > 0$ there exists a width $w$ and parameters such that:

$$
\sup_x \|f(x) - \tilde{f}(x)\| < \epsilon
$$

## Approximation properties

Similar results exist for deep networks with bounded width[^2].

[^2]: @lu2017expressive

-   These results ***do*** tell us that in most problems there exist both deep and shallow networks that approximate the true input-output relationship arbitrarily well

-   They ***don't*** tell us how to find them.

## Performance considerations

Several factors can affect actual performance in practice:

1.  *architecture* (network structure)
2.  activation function(s)
3.  loss function
4.  optimization method
5.  parameter initialization and training epochs
6.  data quality (don't forget this one!)

## Activations

[Activation functions](https://en.wikipedia.org/wiki/Activation_function) $\sigma(\cdot)$ determine whether a given unit 'fires'.

```{dot}
//| fig-width: 8
//| fig-height: 3
digraph G {
    layout = dot
    rankdir = LR
    
    subgraph cluster_0 {
        color = white
    node [label = " ", 
        shape = circle, 
        fillcolor = aquamarine1, 
        style = filled, 
        color = white]
        
    unit1
    unit2
    unit3
        
    }
    
    sum [label = "&Sigma;", shape = rectangle]
    
    activation [label = "&sigma;(&sdot;)", shape = oval]
    
    out [label = " ", 
        shape = circle, 
        fillcolor = cadetblue2, 
        style = filled, 
        color = white]
        
    {unit1 unit2 unit3} -> sum [color = coral4, arrowhead = none]
    sum -> activation [arrowhead = none]
    activation -> out [arrowsize = 0.5]
    
}
```

. . .

For example:

-   if $Z_{k - 1}\beta_{kj} = -28.2$ and $\sigma_k(x) = \frac{1}{1 + e^{-x}}$,

-   then $z_{kj} = \sigma_k(Z_{k - 1}\beta_j) \approx 0$.

## Common activation functions

The most common activation functions are:

-   (identity) $\sigma(x) = x$

-   (sigmoid) $\sigma(x) = \frac{1}{1 + \exp\{-x\}}$

-   (hyperbolic tangent) $\sigma(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$

-   (rectified linear unit) $\sigma(x) = \max (0, x)$

## Loss functions

The most common loss function for classification is

$$
L(Y, f(X)) = -\frac{1}{n}\sum_i \left[y_i\log p_i + (1 - y_i)\log(1 - p_i)\right]
\qquad\text{(cross-entropy)}
$$

. . .

The most common loss function for regression is:

$$
L(Y, f(X)) = \frac{1}{n}\sum_i (y_i - f(x_i))^2
\qquad\text{(mean squared error)}
$$

## References
