---
title: "Text to data: basic NLP"
subtitle: "PSTAT197A/CMPSC190DD Fall 2023"
institute: 'UCSB'
bibliography: refs.bib
format: 
  revealjs:
    incremental: true
    # footer: 'PSTAT197A/CMPSC190DD Fall 2023'
    # logo: 'img/ucsbds_hex.png'
    fig-width: 4
    fig-height: 2
    fig-align: 'left'
    slide-number: true
execute:
  message: false
  warning: false
  echo: false
  cache: true
---


# Data introduction

## Objectives

The data we'll use for this module comes from a 2021-2022 capstone project.

. . .

***Goal:*** use a predictive model to flag webpages that may contain evidence related to fraud claims.

-   data are a sample of pages

-   classification problem

    -   given a webpage, want to predict whether contents include potential evidence

## About the data

Data available to develop a model are a collection of labeled webpages.

-   \~ 3K webpages

-   manually assigned labels specify type of content

    -   multiple classes

-   sampling method unclear/unknown

    -   predictive model fit to this data may not work well in general for an arbitrary webpage

## Example rows

We will work with a random subsample of 618 observations (pages).

```{r}
#| echo: true
library(tidyverse)
load('data/carpe-raw-subsample.RData')
rawdata %>% head()
```

## Data semantics

At face value:

-   The *observational units* are webpages

    -   one observation per page sampled

-   The *variables* are claim labels and ... ???

. . .

How do we obtain useable data from HTML?

## Labels {.scrollable}

It'll be hard to classify labels ocurring \<1% of the time.

```{r}
# count observations in each category
rawdata %>%  
  count(internal_feedback) %>%
  mutate(prop = n/sum(n)) %>%
  arrange(desc(n)) %>%
  knitr::kable()
```

## Lumping {.scrollable}

We can ***lump*** infrequent labels together, by collapsing them into “other”. (see `forcats` \[[docs](https://forcats.tidyverse.org/)\]).

```{r}
# combine factor levels
rawdata %>%  
  mutate(class = fct_lump(internal_feedback, prop = 0.05),
         class = fct_infreq(class)) %>%
  count(class) %>% 
  mutate(proportion = n/sum(n)) %>%
  knitr::kable()
```

-   half of pages contain no relevant information

## Binary classification

This is a *multi-class classification* problem.

. . .

***BUT*** if we can't do well with binary classification, there's not much hope for the multi-class setting. So let's start there.

```{r}
# lump and relabel
rawdata_relabeled <- rawdata %>%
  mutate(bclass = fct_lump(internal_feedback, 
                           prop = 0.5, 
                           other_level = 'relevant'),
         bclass = fct_recode(bclass, 
                             irrelevant = 'N/A: No relevant content.'),
         bclass = fct_infreq(bclass))  %>%
  mutate(.id = paste('url', row_number(), sep = '')) %>%
  select(.id, bclass, text_tmp)

rawdata_relabeled %>% head()
```

. . .

***First task:*** HTML ➜ data.

# Scraping and preprocessing

## Raw HTML

Here's what a page looks like.

```{r}
page <- rawdata_relabeled %>% slice(1) %>% pull(text_tmp)
page
```

. . .

We'll try to extract the words from the page.

## Scraping tools {.scrollable}

***First step***: strip HTML and extract text (see `rvest` \[[docs](https://rvest.tidyverse.org/index.html)\]).

```{r}
#| echo: true
library(rvest)

# parse
page_text <- read_html(page) %>%
  # extract paragraph elements
  html_elements('p') %>%
  # strip html and extract text
  html_text2()

# print result
page_text
```

## One long string

We can collapse the list into one long character string containing all the paragraph text.

```{r}
#| echo: true
string <- page_text %>% str_c(collapse = ' ')

string
```

## Words $\neq$ strings

`nathan` , `Nathan`, and `Nathan!` are **identical words but distinct strings**.

```{r}
#| echo: true

'nathan' == 'Nathan'
```

. . .

But text analysis requires that strings ⟺ words.

. . .

***Question.*** What are the steps to get from `[1]` to `[2]` ?

```{r}
c('For more information, call @Alfred | (201) 744 5050',
  'for more information call alfred')
```

## String manipulation

See `stringr` \[[docs](https://stringr.tidyverse.org/index.html)\] for string manipulation via pattern matching.

```{r}
#| echo: true

library(stringr)

c('example-string') %>% str_replace('[[:punct:]]', ' ')
```

. . .

See `qdapRegex` \[[docs](https://github.com/trinker/qdapRegex)\] for shorthand wrappers for removing common but complex patterns.

```{r}
#| echo: true
library(qdapRegex)

c('email Mildred mildred@mildred.info') %>% rm_email() 
```

## Page text processing

Our strategy will be:

1.  Remove URLs and email addresses
2.  Remove non-letters:
    -   line breaks `\n` and `&nbsp`

    -   punctuation, numbers, and special characters
3.  Add spaces before capital letters then remove extra whitespace
4.  Replace all capital letters with lower case letters

## Example {.scrollable}

Here's what that looks like for one page.

```{r}
#| echo: true
remove <- c('\n', 
            '[[:punct:]]', 
            'nbsp', 
            '[[:digit:]]', 
            '[[:symbol:]]') %>%
  paste(collapse = '|')

string %>%
  rm_url() %>%
  rm_email() %>%
  str_remove_all('\'') %>%
  str_replace_all(remove, ' ') %>%
  str_replace_all("([a-z])([A-Z])", "\\1 \\2") %>%
  tolower() %>%
  str_replace_all("\\s+", " ")
```

## Output quality

***Comments:***

-   consistent input format (*i.e.* sampling and collection) is really important for consistent scraping and text processing

    -   tricky with HTML because webpages may use different elements to display content

-   thorough quality tests are recommended: inspect random subsamples for errors in processing

## Quality checks {.scrollable}

```{r}
parse_fn <- function(.html){
  read_html(.html) %>%
    html_elements('p') %>%
    html_text2() %>%
    str_c(collapse = ' ') %>%
    rm_url() %>%
    rm_email() %>%
    str_remove_all('\'') %>%
    str_replace_all(remove, ' ') %>%
    str_replace_all("([a-z])([A-Z])", "\\1 \\2") %>%
    tolower() %>%
    str_replace_all("\\s+", " ")
}

set.seed(102422)
rawdata_relabeled %>% 
  slice_sample(n = 3) %>% 
  pull(text_tmp) %>%
  lapply(parse_fn)
```

## Processed data {.scrollable}

```{r}
#| echo: true

clean <- rawdata_relabeled %>%
  filter(str_detect(text_tmp, '<!')) %>%
  rowwise() %>%
  mutate(text_clean = parse_fn(text_tmp)) %>%
  select(-text_tmp) %>%
  unnest(text_clean)

clean %>% head()
```

# Natural language processing

## About NLP

[*Natural language processing*](https://en.wikipedia.org/wiki/Natural_language_processing)(NLP) refers to techniques for processing and analyzing speech and text. Although a specialized subfield, it comprises a broad range of problems and methods, including:

-   text and speech processing and representation

-   automated summarization

-   speech recognition

-   machine translation

-   sentiment analysis

## Text processing techniques

We will focus here on NLP techniques for processing text, *i.e.*, converting text into data.

-   *tokenization:* breaking a string of text into smaller units

-   *lemmatization:* converting tokens into common forms

-   calculating frequency measures

## Tokenization

Breaking a string of text into subunits is called ***tokenization.***

. . .

Consider this string:

```{r}
invitation_text <- c('if you are a dreamer come in if you are a dreamer a wisher a liar a hope er a pray er a magic bean buyer if youre a pretender come sit by my fire for we have some flax golden tales to spin come in come in')

invitation <- tibble(text = invitation_text)

invitation_text
```

## Word tokenization

The `tokenizers` package \[[docs](https://github.com/ropensci/tokenizers)\] contains various tokenization functions. The most elementary method is to treat each word as a token.

```{r}
#| echo: true
library(tokenizers)

invitation_text %>% tokenize_words()
```

## N-gram tokens {.scrollable}

Alternatively, one could tokenize by ***n-grams***: unique combinations of $n$ adjacent words.

```{r}
#| echo: true
invitation_text %>% tokenize_ngrams(n = 2)
```

## Stopwords

Some tokens are thought to contain little semantic information, such as logical connectives, pronouns, and the like.

. . .

In NLP these are treated as ***stopwords***: words that are stopped in text processing.

```{r}
#| echo: true
library(stopwords)

# display 10 random stopwords from the 'snowball' dictionary
set.seed(102422)
stopwords(language = 'en', source = 'snowball') %>% 
  sample(size = 10)
```

## Stopword removal

In the default stopword list, some stopwords include punctuation. Since this was removed from our string, it should also be removed from the stopword list for effective string matching.

```{r}
#| echo: true
stopwords_nopunct <- stopwords() %>% 
  str_remove_all('[[:punct:]]')

invitation_text %>%
  tokenize_words(stopwords = stopwords_nopunct)
```

## Lemmatization

***Lemmatization*** refers to grouping word inflections into a single form. See `textstem` \[[docs](https://github.com/trinker/textstem)\].

```{r}
#| echo: true

library(textstem)

locomotion <- c('run', 'running', 'ran', 
                'boating', 'boat', 
                'swim', 'swam', 'swimming', 'swum') 

locomotion %>% lemmatize_words()
```

. . .

Also works by pattern matching and replacement using a source dictionary.

## `tidytext::unnest_tokens()` {.scrollable}

```{r}
boat_text <- 'this boat that we just built is just fine and dont try to tell us its not the sides and the back are divine its the bottom i guess we forgot'
```

The `tidytext` package \[[docs](https://github.com/juliasilge/tidytext)\] contains wrappers around tokenizers and other functions for use in tidyverse-style programming.

```{r}
library(tidytext)

token_df <- tibble(doc = c('invitation', 'boat'),
                   text = c(invitation_text, boat_text)) %>%
  unnest_tokens(output = token, 
                input = text, 
                token = 'words', 
                stopwords = stopwords_nopunct) %>%
  mutate(token.lem = lemmatize_words(token))

token_df
```

## Quality check

Text processing is error-prone. The functions shown here are handy but imperfect. ***You should always perform quality checks to identify bugs!***



## Corpora {.scrollable}

We can summarize a text corpus (collection of texts) as:

-   $T = \{t_1, \dots, t_p\}$ set of $p$ unique tokens

-   $D = \{d_1, \dots, d_n\}$ set of $n$ documents comprising some corpus

    -   $d_i = \{t_{i1}, \dots, t_{iL_i}: t_{ij} \in T\}$ is the $i$th document

## Frequency measures

Then we can define the following:

-   documentwise token counts $n_{ti} = \sum_j \mathbf{1}\{t_{ij} = t\}$

    -   number of times token $t$ appears in document $i$

-   corpuswise token counts $n_t = \sum_i \mathbf{1}\{t \in d_i\}$

    -   number of documents containing token $t$

-   **term frequency:** $\text{tf}(t, i) = n_{ti}/n_i$, $\quad n_i$: Document length.

-   **document frequency:** $\text{df}(t, i) = n_t/n$, $\quad n$ : Number of documents.

-   **inverse document frequency:** $\text{idf}(t, i)=-\log\left(\text{df}(t, i)\right)$

## Computing TF-IDF

Continuing with our toy example:

```{r}
token_df %>%
  count(doc, token.lem, name = 'n_ti') %>%
  bind_tf_idf(term = token.lem, document = doc, n = n_ti) %>%
  arrange(desc(tf)) %>%
  head(4)
```

. . .

**Question:** what does it mean that IDF is the same for all terms?

## TF-IDF

The last column was the product of term frequency and inverse document frequency, known as ***TF-IDF:***

$$
\text{tf-idf}(t, i) = \text{tf}(t, i) \times \text{idf}(t, i)
$$

. . .

Interpretation:

-   higher values indicate rare words used often in a document

-   lower values indicate common words used infrequently in a document

## Document term matrix {.scrollable}

Finally, we can pivot a selected frequency measure into a data frame in which:

-   each row is a document

-   each column is a token

-   each value is a frequency measure

. . .

```{r}
toy_data <- token_df %>%
  count(doc, token.lem, name = 'n_ti') %>%
  bind_tf_idf(term = token.lem, document = doc, n = n_ti) %>%
  pivot_wider(id_cols = doc, 
              names_from = 'token.lem', 
              values_from = 'tf_idf',
              values_fill = 0)

toy_data
```

. . .

***Question:*** how would you check whether the 'documents' have words in common?

## Processed fraud claim data

TF-IDF document term matrix for word tokens:

```{r}
claims <- clean %>% 
  unnest_tokens(output = token, 
                input = text_clean, 
                token = 'words',
                stopwords = str_remove_all(stop_words$word, 
                                           '[[:punct:]]')) %>%
  mutate(token.lem = lemmatize_words(token)) %>%
  filter(str_length(token.lem) > 2) %>%
  count(.id, bclass, token.lem, name = 'n') %>%
  bind_tf_idf(term = token.lem, 
              document = .id,
              n = n) %>%
  pivot_wider(id_cols = c('.id', 'bclass'),
              names_from = 'token.lem',
              values_from = 'tf_idf',
              values_fill = 0)

claims %>% head(3)
```

-   $n = 552$ rows/observations (one per page)

-   columns comprising $p = 15,742$ variables and $1$ class label

## Next time

Next time we'll discuss the capstone group's analysis strategy:

-   dimension reduction

-   statistical modeling
